{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10_fold_densenet_0723+KFold_night_.ipynb","provenance":[{"file_id":"1QCidMJPE2bhPEL8LbAPwzarAip9raFML","timestamp":1595413529150},{"file_id":"1t4zVBLjKhc06XtFI5mVuMEyKFq5nZrmm","timestamp":1595413347466},{"file_id":"1SpH-4P3Y0PU7QVsvCS2vmN2-yZhyOpNr","timestamp":1595319285589}],"collapsed_sections":["5NGhVy-8qbOh","0Xk_7j9eeP1t","iyDgJm29zDBd","HBwrwWa1DNvC"],"authorship_tag":"ABX9TyNZwr/a+ZflJofJKW0Tfs+F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2024534dac26486097ab59bb6d801433":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_96739ac799324f6190d6195d7f446bc6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b8c0509bfd1b41c4b2925becb3ed2905","IPY_MODEL_e0a1ba4288184816bc4af4432edec60a"]}},"96739ac799324f6190d6195d7f446bc6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b8c0509bfd1b41c4b2925becb3ed2905":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b0a5f0498b234011a2999b3f96232af8","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":57365526,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":57365526,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6ff062ae063c476ba18f6342741cd5a1"}},"e0a1ba4288184816bc4af4432edec60a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_dcb37d6c6ba3411dbe031ace9c86fd52","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 54.7M/54.7M [00:14&lt;00:00, 4.07MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_209aa03580f349dfa7e0537ca5534bcb"}},"b0a5f0498b234011a2999b3f96232af8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6ff062ae063c476ba18f6342741cd5a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dcb37d6c6ba3411dbe031ace9c86fd52":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"209aa03580f349dfa7e0537ca5534bcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"5NGhVy-8qbOh","colab_type":"text"},"source":["## 修改记录"]},{"cell_type":"markdown","metadata":{"id":"0gzL-9mFcn4K","colab_type":"text"},"source":["#### 截止7.19 \n","- 利用预训练模型实现迁移学习\n","- 问题：\n","```\n","  1. 训练集准确率小于测试集\n","  2. 没有实现随机划分\n","```\n","- 解决方案：\n","```\n","  1. 交叉验证，看下是否还是训练精度低于测试精度；\n","  2. 随机选择数据作为测试集；\n","  3. 去掉0-30度的数据看下实验效果；\n","  4. 查找一下是否有解决dropout局限性的方法；\n","  5. 整理算法的框架图及伪代码\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"LSvx5OQIIaey","colab_type":"text"},"source":["#### 7.20\n","- 尝试实现训练集和测试集的随机划分，以及K折交叉验证\n","- 问题：\n","```\n","  1. 一开始采用pytoch的包完成数据读取，预处理和分块；\n","  2. 在利用ImageFolder生成dataset后 没有现有函数对其进行transform（因为训练集和测试集采用\n","  不同的方法transform）\n","  3. pytorch没有现成API实现交叉验证\n","```\n","- 解决方案：\n","```\n","  1. 考虑采用sklearn完成数据的处理，但还没找到合适的transform 方法\n","  2. 随机划分可以使用pytorch的  \n","  torch.utils.data.random_split 或者  \n","  sklearn.model_selection.train_test_split\n","  3. 使用 KFold 进行K折划分\n","  4. 重写训练函数，K折交叉验证参考：  \n","  https://blog.csdn.net/foneone/article/details/104445320  ；  \n","  https://blog.csdn.net/Pl_Sun/article/details/106975414  \n","```"]},{"cell_type":"markdown","metadata":{"id":"PZGi0Q8qIccw","colab_type":"text"},"source":["#### 7.21\n","\n","- 添加随机划分和K折交叉验证（都已完成）\n","- 关于交叉验证的一些思考 https://www.jianshu.com/p/f14826061612\n","- 还没测试去掉30度以下数据的效果\n","- 问题：\n","```\n","  1. 标准化参数的选择，transforms.Normalize，目前灰度图采用的是Mnist的参数\n","  2. batch_size的选择，一般可能偏大时效果变好（不一定），此数据集较小，目前认为 4 比较好\n","  3. 依然存在val_acc > train_acc (根据目前训练过程，猜测和数据集划分方式有关，随机划分时\n","  验证集每一类都是平均的，K折交叉验证时出现概率更小。数据集较小，容易出现巧合)\n","  4. 以下是关于准确率的（7.21使用的都是night_data，7.20使用的是morning_data）  \n","  K折（10折）交叉验证很多个分组的效果明显较差（大多在40%以下）\n","  未训练完，未测试morning data\n","  对于随机划分数据集的准确率在 9:1划分是55%，8:2划分是76%\n","  利用night_data训练的模型去测试所有morning data，准确率很低（百分之几）\n","  morning_data和night_data存在较大偏差？\n","```\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dfsLspQHfafT","colab_type":"text"},"source":["## 依赖包\n","<!-- **Author**: `zwy` -->\n"]},{"cell_type":"code","metadata":{"id":"pBncOk5bgwCo","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595474692364,"user_tz":-480,"elapsed":4905,"user":{"displayName":"周尉阳","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTKLMpcfIq9vdPt1QQyeIS64dyhcfjcYMqI2RN=s64","userId":"12403260934929188499"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","from sklearn.model_selection import KFold, StratifiedShuffleSplit\n","\n","import random\n","import shutil\n","\n","plt.ion()   # 交互模式可以动态显示图像"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o_E5YRwsN4dR","colab_type":"text"},"source":["### 硬件选择 GPU"]},{"cell_type":"code","metadata":{"id":"Na2VrBlNNcJS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595474692372,"user_tz":-480,"elapsed":4897,"user":{"displayName":"周尉阳","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTKLMpcfIq9vdPt1QQyeIS64dyhcfjcYMqI2RN=s64","userId":"12403260934929188499"}},"outputId":"0ac61673-9700-4092-a2dc-45695f2a3dca"},"source":["# GPU\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1TMlepIMhu6X","colab_type":"text"},"source":["## 数据加载及处理\n"]},{"cell_type":"markdown","metadata":{"id":"uw9RCO1eiI3E","colab_type":"text"},"source":["### 下载数据集\n","可以从Google Drive / Github / 其他数据地址下载数据"]},{"cell_type":"code","metadata":{"id":"D8DmF5QPiE4l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":151},"executionInfo":{"status":"ok","timestamp":1595474704173,"user_tz":-480,"elapsed":16677,"user":{"displayName":"周尉阳","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTKLMpcfIq9vdPt1QQyeIS64dyhcfjcYMqI2RN=s64","userId":"12403260934929188499"}},"outputId":"94cd0672-26e4-4d91-b54a-0bf73a866966"},"source":["!rm -rf CNN\n","!git clone https://github.com/Zwysun/CNN.git\n","!ls"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Cloning into 'CNN'...\n","remote: Enumerating objects: 693, done.\u001b[K\n","remote: Counting objects: 100% (693/693), done.\u001b[K\n","remote: Compressing objects: 100% (551/551), done.\u001b[K\n","remote: Total 693 (delta 160), reused 670 (delta 140), pack-reused 0\u001b[K\n","Receiving objects: 100% (693/693), 31.28 MiB | 10.09 MiB/s, done.\n","Resolving deltas: 100% (160/160), done.\n","CNN  sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"joEpP9JyEqJV","colab_type":"code","colab":{}},"source":["# night bu\n","!rm -rf CNN/dataset/morning/val\n","!rm -rf CNN/dataset/morning/train"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T6dJNWuRiRPz","colab_type":"text"},"source":["### 数据预处理\n","利用transforms对图片进行预处理，可以分别针对训练集和验证集采取不同的处理方法\n"]},{"cell_type":"markdown","metadata":{"id":"0Xk_7j9eeP1t","colab_type":"text"},"source":["#### 比例随机划分 数据集和验证集"]},{"cell_type":"code","metadata":{"id":"GqEz2QkBsVdW","colab_type":"code","colab":{}},"source":["data_dir = 'CNN/dataset/night'    # 数据目录\n","ratio_train = 0.7                 # 训练集比例\n"," \n","def data_random_split(current_dir,ratio_train):\n","    '''\n","    将当前文件夹中文件按一定比例分成train和test连个列表，列表存放文件名\n","    '''\n","    data_listdir = os.listdir(current_dir)\n","    random.shuffle(data_listdir)\n","    train_len = int(len(data_listdir)*ratio_train)\n","    train_listdir = data_listdir[:train_len]\n","    train_listdir = data_listdir[:train_len]\n","    test_listdir  = data_listdir[train_len:]\n","    return train_listdir,test_listdir\n"," \n","def data_generator(root,data_dir,ratio_train=0.8):\n","    '''\n","    root：输入你的data目录\n","    datadir：你要创建好的的文件夹，将会生成train、和val\n","    ratio_train:train_data占总数据的比例\n","    '''\n","    listdir = os.listdir(root)\n","    train_data_dir = os.path.join(data_dir, \"train\")\n","    test_data_dir = os.path.join(data_dir, \"val\")\n","    os.makedirs(train_data_dir)\n","    os.makedirs(test_data_dir)\n","    for name in listdir:\n","        print(name)\n","        current_dir = os.path.join(root, name)\n","        print(current_dir)\n","        train_dir_a,test_dir_a = data_random_split(current_dir,ratio_train)\n","        train_listdir_c = os.path.join(train_data_dir,name)\n","        test_listdir_c  = os.path.join(test_data_dir,name)\n","        a=1 if os.path.exists(train_listdir_c) else os.makedirs(train_listdir_c)\n","        a=1 if os.path.exists(test_listdir_c) else os.makedirs(test_listdir_c)\n","        for img in train_dir_a:\n","            train_listdir_b = os.path.join(current_dir, img) \n","            train_listdir_d = os.path.join(train_listdir_c, img)\n","            train_dir_b = shutil.copy(train_listdir_b,train_listdir_d)\n","        for img in test_dir_a:\n","            test_listdir_b  = os.path.join(current_dir, img)\n","            test_listdir_d  = os.path.join(test_listdir_c, img)\n","            test_dir_b = shutil.copy(test_listdir_b,test_listdir_d)\n","                \n","    print('ok')\n","\n","# 随机划分为训练集和验证集\n","data_generator(data_dir, data_dir, ratio_train)\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tb9XnU5XurB4","colab_type":"code","colab":{}},"source":["# !cd train_data\n","# !ls -lR |grep -v ^d|awk '{print $9}'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZYfsgAwoinGr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":439},"executionInfo":{"status":"ok","timestamp":1595318851190,"user_tz":-480,"elapsed":3193,"user":{"displayName":"周尉阳","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTKLMpcfIq9vdPt1QQyeIS64dyhcfjcYMqI2RN=s64","userId":"12403260934929188499"}},"outputId":"a77cba6b-15d6-449b-810a-0c74f83e6ddb"},"source":["# 训练集 扩充及正则化\n","# 验证集 仅正则化\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.Grayscale(1),      # 转换为灰度图\n","        transforms.CenterCrop(1080),\n","        transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),    # 增广\n","        transforms.ToTensor(),\n","        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])    # RGB标准化参数(Imagenet Dateset)\n","        transforms.Normalize([0.1307], [0.3081])      # 灰度图标准化参数(Mnist)\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Grayscale(1),\n","        transforms.CenterCrop(1080),\n","        transforms.Resize(224),\n","        transforms.ToTensor(),\n","        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        transforms.Normalize([0.1307], [0.3081])      # 灰度图标准化参数\n","    ]),\n","}\n","\n","# full_datasets = datasets.ImageFolder(data_dir)\n","# train_size = int(0.8 * len(full_datasets))   # 训练集划分比例  还有点问题\n","# test_size = len(full_datasets) - train_size\n","# train_dataset, test_dataset = torch.utils.data.random_split(full_datasets, [train_size, test_size])\n","\n","image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n","                                          data_transforms[x])\n","                  for x in ['train', 'val']}\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n","                                             shuffle=True, num_workers=4)\n","              for x in ['train', 'val']}\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n","class_names = image_datasets['train'].classes\n","print (dataset_sizes)\n","print (image_datasets)\n","print (class_names)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'train': 133, 'val': 57}\n","{'train': Dataset ImageFolder\n","    Number of datapoints: 133\n","    Root location: CNN/dataset/night/train\n","    StandardTransform\n","Transform: Compose(\n","               Grayscale(num_output_channels=1)\n","               CenterCrop(size=(1080, 1080))\n","               RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)\n","               RandomHorizontalFlip(p=0.5)\n","               ToTensor()\n","               Normalize(mean=[0.1307], std=[0.3081])\n","           ), 'val': Dataset ImageFolder\n","    Number of datapoints: 57\n","    Root location: CNN/dataset/night/val\n","    StandardTransform\n","Transform: Compose(\n","               Grayscale(num_output_channels=1)\n","               CenterCrop(size=(1080, 1080))\n","               Resize(size=224, interpolation=PIL.Image.BILINEAR)\n","               ToTensor()\n","               Normalize(mean=[0.1307], std=[0.3081])\n","           )}\n","['0', '10', '15', '20', '25', '30', '35', '40', '45', '5', '50', '55', '60', '65', '70', '75', '80', '85', '90']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oGJg0syEecEP","colab_type":"text"},"source":["#### K折交叉验证(的数据处理)"]},{"cell_type":"code","metadata":{"id":"fK2yS8R9e8sq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1595474704177,"user_tz":-480,"elapsed":6132,"user":{"displayName":"周尉阳","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTKLMpcfIq9vdPt1QQyeIS64dyhcfjcYMqI2RN=s64","userId":"12403260934929188499"}},"outputId":"e174b759-b2d5-4dff-ee34-d099b8f52011"},"source":["data_dir = 'CNN/dataset/night'    # 数据目录\n","\n","data_transforms = transforms.Compose([\n","        transforms.Grayscale(1),      # 转换为灰度图\n","        transforms.CenterCrop(1080),\n","        transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),    # 增广\n","        transforms.ToTensor(),\n","        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])    # RGB标准化参数(Imagenet Dateset)\n","        transforms.Normalize([0.1307], [0.3081])      # 灰度图标准化参数(Mnist)\n","    ])\n","\n","image_datasets = datasets.ImageFolder(data_dir, data_transforms)\n","\n","class_names = image_datasets.classes\n","dataset_sizes = len(image_datasets)\n","print ('Total:', dataset_sizes)\n","print (class_names)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Total: 190\n","['0', '10', '15', '20', '25', '30', '35', '40', '45', '5', '50', '55', '60', '65', '70', '75', '80', '85', '90']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ay1QqY-Ok_pc","colab_type":"code","colab":{}},"source":["# 不要运行\n","X=np.arange(380).reshape(190,2)\n","\n","for train_index,test_index in kf.split(X):\n","    print('train_index %s, test_index %s'%(train_index, test_index))\n","\n","train_sampler = torch.utils.data.SubsetRandomSampler(train_index)\n","valid_sampler = torch.utils.data.SubsetRandomSampler(test_index)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iyDgJm29zDBd","colab_type":"text"},"source":["### 显示部分图片\n"]},{"cell_type":"code","metadata":{"id":"8pLq8lIszFCe","colab_type":"code","colab":{}},"source":["# 交叉验证这 用不了\n","def imshow(inp, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    # mean = np.array([0.485, 0.456, 0.406])\n","    # std = np.array([0.229, 0.224, 0.225])   # RGB\n","    mean = np.array([0.1307])\n","    std = np.array([0.3081])   # 灰度图\n","    \n","    inp = std * inp + mean\n","    inp = np.clip(inp, 0, 1)\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","\n","# Get a batch of training data\n","inputs, classes = next(iter(dataloaders))\n","\n","# Make a grid from batch\n","out = torchvision.utils.make_grid(inputs)\n","\n","imshow(out, title=[class_names[x] for x in classes])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7jdsXjB6zYk-","colab_type":"text"},"source":["## 模型训练"]},{"cell_type":"code","metadata":{"id":"-RG19n3jzYML","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595474708731,"user_tz":-480,"elapsed":1092,"user":{"displayName":"周尉阳","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTKLMpcfIq9vdPt1QQyeIS64dyhcfjcYMqI2RN=s64","userId":"12403260934929188499"}}},"source":["def train_model(model, dataloaders, criterion, optimizer, scheduler, datasize, num_epochs=25):\n","    since = time.time()\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / datasize[phase]\n","            epoch_acc = running_corrects.double() / datasize[phase]\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","                phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","            if phase == 'train':\n","              scheduler.step()\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, best_acc"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"KSv2W7-mv47o","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595474715571,"user_tz":-480,"elapsed":1027,"user":{"displayName":"周尉阳","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTKLMpcfIq9vdPt1QQyeIS64dyhcfjcYMqI2RN=s64","userId":"12403260934929188499"}}},"source":["# K折交叉验证\n","\n","def k_fold_train(k, dataset, model, batch_size, criterion, optimizer, scheduler, num_epochs=25):\n","    # kf = KFold(k, shuffle=True)\n","    # 分层划分，避免出现某一层全在训练集/验证集的情况\n","    kf = StratifiedShuffleSplit(n_splits=k,train_size=1-1/k,test_size=1/k,random_state=True)\n","    \n","    i = 1\n","    valid_acc_sum = 0    \n","    Kfold_best_acc = 0.0\n","    kfold_model_wts = copy.deepcopy(model.state_dict())\n","    best_model_dict = kfold_model_wts\n","\n","    X=np.arange(len(dataset)*2).reshape(len(dataset),2)\n","    y=np.arange(190)\n","    for j in range(19):\n","      y[j*10:j*10+10]=j\n","\n","    for train_index,val_index in kf.split(X,y):\n","      # print('train_index %s, test_index %s'%(train_index, test_index))\n","      print('*'*25,'第',i,'折','*'*25)\n","      datasize = {'train':len(train_index), 'val':len(val_index)}\n","      print('train:',datasize['train'],'val:',datasize['val'])\n","\n","      i += 1\n","\n","      train_sampler = torch.utils.data.SubsetRandomSampler(train_index)\n","      valid_sampler = torch.utils.data.SubsetRandomSampler(val_index)\n","      dataloaders = {'train': torch.utils.data.DataLoader(image_datasets, \n","                  batch_size=batch_size, num_workers=4, sampler=train_sampler),\n","              'val': torch.utils.data.DataLoader(image_datasets,\n","                  batch_size=batch_size, num_workers=4, sampler=valid_sampler)\n","              }\n","      model.load_state_dict(kfold_model_wts) #每一折训练前 都重置为初始状态\n","\n","      model, k_acc = train_model(model, dataloaders, criterion, optimizer, scheduler, datasize, num_epochs)\n","      valid_acc_sum += float(k_acc)\n","      if Kfold_best_acc < k_acc:\n","        Kfold_best_acc = k_acc\n","        best_model_dict = copy.deepcopy(model.state_dict()) # 保存最佳的训练参数\n","    \n","    valid_acc = valid_acc_sum/(i-1)\n","    print('*'*23,'第',i,'折****END','*'*23)\n","    print('the best acc in kfold', Kfold_best_acc)\n","    print('teh average val acc', valid_acc)\n","    model.load_state_dict(best_model_dict)\n","    return model"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I4J3qwx4zcs0","colab_type":"text"},"source":["## 搭建网络结构"]},{"cell_type":"code","metadata":{"id":"D76hfWxfzhFv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":82,"referenced_widgets":["2024534dac26486097ab59bb6d801433","96739ac799324f6190d6195d7f446bc6","b8c0509bfd1b41c4b2925becb3ed2905","e0a1ba4288184816bc4af4432edec60a","b0a5f0498b234011a2999b3f96232af8","6ff062ae063c476ba18f6342741cd5a1","dcb37d6c6ba3411dbe031ace9c86fd52","209aa03580f349dfa7e0537ca5534bcb"]},"executionInfo":{"status":"ok","timestamp":1595474746751,"user_tz":-480,"elapsed":15192,"user":{"displayName":"周尉阳","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTKLMpcfIq9vdPt1QQyeIS64dyhcfjcYMqI2RN=s64","userId":"12403260934929188499"}},"outputId":"036e3797-8547-4f1e-81ed-9a77e3beee2d"},"source":["model_ft = models.densenet169(pretrained=True)\n","num_ftrs = model_ft.classifier.in_features\n","model_ft.classifier = nn.Linear(num_ftrs, 19, bias=True)  # 输出参数修改\n","\n","num_ochannels = model_ft.features.conv0.out_channels\n","model_ft.features.conv0 = nn.Conv2d(1, num_ochannels, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) # 输入参数修改\n","\n","model_ft = model_ft.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","# Observe that all parameters are being optimized\n","optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n","\n","# Decay LR by a factor of 0.1 every 7 epochs\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n","# print (model_ft)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/densenet169-b2777c0a.pth\" to /root/.cache/torch/checkpoints/densenet169-b2777c0a.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2024534dac26486097ab59bb6d801433","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=57365526.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oS3-HhvfU8kf","colab_type":"code","colab":{}},"source":["# 可视化网络\n","!pip install tensorwatch\n","import tensorwatch as tw\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VnyeYPv8Y6bJ","colab_type":"code","colab":{}},"source":["tw.draw_model(model_ft, [1, 1, 224, 224])  # 有问题"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mOom8R1QVxUo","colab_type":"code","colab":{}},"source":["# 可视化工具\n","https://lutzroeder.github.io/netron/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7icvKjpWzlHP","colab_type":"text"},"source":["## 训练"]},{"cell_type":"markdown","metadata":{"id":"A5zjbTmZ2Da2","colab_type":"text"},"source":["K折交叉验证"]},{"cell_type":"code","metadata":{"id":"wb_qKOyT2BC_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1595487118779,"user_tz":-480,"elapsed":9393263,"user":{"displayName":"周尉阳","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTKLMpcfIq9vdPt1QQyeIS64dyhcfjcYMqI2RN=s64","userId":"12403260934929188499"}},"outputId":"ae2524e5-20f4-4b00-afbb-0bf664925ef2"},"source":["model_ft = k_fold_train(10, image_datasets, model_ft, 4, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=100)\n","# os.chdir(\"/content/drive/My Drive\")\n","!ls\n","torch.save(model_ft,'light_10fold.pth')\n","!ls\n","# 针对最优模型再次训练, 可选\n","# model_ft, _ = train_model(model_ft, dataloaders, 8, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=100)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n","\n","Epoch 9/99\n","----------\n","train Loss: 1.9322 Acc: 0.4503\n","val Loss: 1.6014 Acc: 0.5263\n","\n","Epoch 10/99\n","----------\n","train Loss: 1.8281 Acc: 0.4035\n","val Loss: 1.4149 Acc: 0.6316\n","\n","Epoch 11/99\n","----------\n","train Loss: 1.8632 Acc: 0.3918\n","val Loss: 1.1913 Acc: 0.6842\n","\n","Epoch 12/99\n","----------\n","train Loss: 1.6629 Acc: 0.5322\n","val Loss: 1.3310 Acc: 0.7368\n","\n","Epoch 13/99\n","----------\n","train Loss: 1.7597 Acc: 0.4971\n","val Loss: 1.4154 Acc: 0.6316\n","\n","Epoch 14/99\n","----------\n","train Loss: 1.6673 Acc: 0.5263\n","val Loss: 1.1371 Acc: 0.7368\n","\n","Epoch 15/99\n","----------\n","train Loss: 1.5746 Acc: 0.6082\n","val Loss: 1.1063 Acc: 0.6316\n","\n","Epoch 16/99\n","----------\n","train Loss: 1.5780 Acc: 0.5556\n","val Loss: 1.2588 Acc: 0.7368\n","\n","Epoch 17/99\n","----------\n","train Loss: 1.6398 Acc: 0.5439\n","val Loss: 1.1861 Acc: 0.5789\n","\n","Epoch 18/99\n","----------\n","train Loss: 1.6520 Acc: 0.5673\n","val Loss: 1.1553 Acc: 0.6842\n","\n","Epoch 19/99\n","----------\n","train Loss: 1.6742 Acc: 0.5439\n","val Loss: 1.0019 Acc: 0.6842\n","\n","Epoch 20/99\n","----------\n","train Loss: 1.5743 Acc: 0.6199\n","val Loss: 1.1384 Acc: 0.5789\n","\n","Epoch 21/99\n","----------\n","train Loss: 1.6922 Acc: 0.5263\n","val Loss: 0.6917 Acc: 0.8947\n","\n","Epoch 22/99\n","----------\n","train Loss: 1.5348 Acc: 0.5614\n","val Loss: 0.9075 Acc: 0.5789\n","\n","Epoch 23/99\n","----------\n","train Loss: 1.5864 Acc: 0.6082\n","val Loss: 1.0104 Acc: 0.6842\n","\n","Epoch 24/99\n","----------\n","train Loss: 1.5604 Acc: 0.5848\n","val Loss: 1.0355 Acc: 0.7895\n","\n","Epoch 25/99\n","----------\n","train Loss: 1.6634 Acc: 0.5263\n","val Loss: 0.7870 Acc: 0.7895\n","\n","Epoch 26/99\n","----------\n","train Loss: 1.6343 Acc: 0.5789\n","val Loss: 0.9034 Acc: 0.7895\n","\n","Epoch 27/99\n","----------\n","train Loss: 1.6780 Acc: 0.5673\n","val Loss: 1.2124 Acc: 0.6316\n","\n","Epoch 28/99\n","----------\n","train Loss: 1.5608 Acc: 0.5848\n","val Loss: 1.0569 Acc: 0.5789\n","\n","Epoch 29/99\n","----------\n","train Loss: 1.4672 Acc: 0.6140\n","val Loss: 0.8551 Acc: 0.7895\n","\n","Epoch 30/99\n","----------\n","train Loss: 1.6486 Acc: 0.5439\n","val Loss: 1.0256 Acc: 0.6316\n","\n","Epoch 31/99\n","----------\n","train Loss: 1.5596 Acc: 0.6023\n","val Loss: 0.8799 Acc: 0.8947\n","\n","Epoch 32/99\n","----------\n","train Loss: 1.5775 Acc: 0.5848\n","val Loss: 1.3361 Acc: 0.4737\n","\n","Epoch 33/99\n","----------\n","train Loss: 1.6407 Acc: 0.5439\n","val Loss: 1.3034 Acc: 0.6316\n","\n","Epoch 34/99\n","----------\n","train Loss: 1.6579 Acc: 0.5614\n","val Loss: 1.0222 Acc: 0.7895\n","\n","Epoch 35/99\n","----------\n","train Loss: 1.6530 Acc: 0.5439\n","val Loss: 1.2495 Acc: 0.5263\n","\n","Epoch 36/99\n","----------\n","train Loss: 1.6777 Acc: 0.5146\n","val Loss: 1.2938 Acc: 0.5263\n","\n","Epoch 37/99\n","----------\n","train Loss: 1.6701 Acc: 0.5263\n","val Loss: 1.0486 Acc: 0.6316\n","\n","Epoch 38/99\n","----------\n","train Loss: 1.6774 Acc: 0.4971\n","val Loss: 1.0915 Acc: 0.5263\n","\n","Epoch 39/99\n","----------\n","train Loss: 1.5884 Acc: 0.5556\n","val Loss: 1.0626 Acc: 0.7895\n","\n","Epoch 40/99\n","----------\n","train Loss: 1.7117 Acc: 0.5029\n","val Loss: 0.8172 Acc: 0.7368\n","\n","Epoch 41/99\n","----------\n","train Loss: 1.6035 Acc: 0.5556\n","val Loss: 0.7525 Acc: 0.6842\n","\n","Epoch 42/99\n","----------\n","train Loss: 1.6909 Acc: 0.5322\n","val Loss: 1.1892 Acc: 0.5789\n","\n","Epoch 43/99\n","----------\n","train Loss: 1.6086 Acc: 0.5614\n","val Loss: 1.3421 Acc: 0.6316\n","\n","Epoch 44/99\n","----------\n","train Loss: 1.6967 Acc: 0.5556\n","val Loss: 1.1308 Acc: 0.6316\n","\n","Epoch 45/99\n","----------\n","train Loss: 1.6929 Acc: 0.5497\n","val Loss: 1.3511 Acc: 0.5263\n","\n","Epoch 46/99\n","----------\n","train Loss: 1.5879 Acc: 0.5848\n","val Loss: 1.0984 Acc: 0.8421\n","\n","Epoch 47/99\n","----------\n","train Loss: 1.5864 Acc: 0.6082\n","val Loss: 1.3295 Acc: 0.6316\n","\n","Epoch 48/99\n","----------\n","train Loss: 1.5779 Acc: 0.5848\n","val Loss: 1.2590 Acc: 0.6842\n","\n","Epoch 49/99\n","----------\n","train Loss: 1.7263 Acc: 0.4737\n","val Loss: 1.0456 Acc: 0.5789\n","\n","Epoch 50/99\n","----------\n","train Loss: 1.6966 Acc: 0.5673\n","val Loss: 1.0497 Acc: 0.6842\n","\n","Epoch 51/99\n","----------\n","train Loss: 1.6073 Acc: 0.5731\n","val Loss: 1.0443 Acc: 0.7368\n","\n","Epoch 52/99\n","----------\n","train Loss: 1.5982 Acc: 0.5789\n","val Loss: 0.9518 Acc: 0.7368\n","\n","Epoch 53/99\n","----------\n","train Loss: 1.6292 Acc: 0.5673\n","val Loss: 1.1203 Acc: 0.6316\n","\n","Epoch 54/99\n","----------\n","train Loss: 1.6424 Acc: 0.5614\n","val Loss: 1.0614 Acc: 0.6316\n","\n","Epoch 55/99\n","----------\n","train Loss: 1.6114 Acc: 0.5380\n","val Loss: 1.2501 Acc: 0.6842\n","\n","Epoch 56/99\n","----------\n","train Loss: 1.6397 Acc: 0.5789\n","val Loss: 1.0748 Acc: 0.6842\n","\n","Epoch 57/99\n","----------\n","train Loss: 1.7349 Acc: 0.4678\n","val Loss: 0.9448 Acc: 0.6316\n","\n","Epoch 58/99\n","----------\n","train Loss: 1.5941 Acc: 0.5439\n","val Loss: 1.0209 Acc: 0.6316\n","\n","Epoch 59/99\n","----------\n","train Loss: 1.5756 Acc: 0.5614\n","val Loss: 1.2074 Acc: 0.6842\n","\n","Epoch 60/99\n","----------\n","train Loss: 1.5619 Acc: 0.6140\n","val Loss: 1.1422 Acc: 0.5263\n","\n","Epoch 61/99\n","----------\n","train Loss: 1.6185 Acc: 0.5556\n","val Loss: 1.0279 Acc: 0.6842\n","\n","Epoch 62/99\n","----------\n","train Loss: 1.5716 Acc: 0.5731\n","val Loss: 0.9580 Acc: 0.7895\n","\n","Epoch 63/99\n","----------\n","train Loss: 1.6521 Acc: 0.5439\n","val Loss: 1.1700 Acc: 0.7368\n","\n","Epoch 64/99\n","----------\n","train Loss: 1.6246 Acc: 0.5556\n","val Loss: 1.2444 Acc: 0.6316\n","\n","Epoch 65/99\n","----------\n","train Loss: 1.5121 Acc: 0.6140\n","val Loss: 1.4247 Acc: 0.6316\n","\n","Epoch 66/99\n","----------\n","train Loss: 1.5036 Acc: 0.6374\n","val Loss: 1.3204 Acc: 0.5789\n","\n","Epoch 67/99\n","----------\n","train Loss: 1.5477 Acc: 0.5380\n","val Loss: 1.2936 Acc: 0.5789\n","\n","Epoch 68/99\n","----------\n","train Loss: 1.6031 Acc: 0.5556\n","val Loss: 1.1161 Acc: 0.7895\n","\n","Epoch 69/99\n","----------\n","train Loss: 1.6981 Acc: 0.5029\n","val Loss: 0.9248 Acc: 0.8421\n","\n","Epoch 70/99\n","----------\n","train Loss: 1.6095 Acc: 0.5556\n","val Loss: 0.8069 Acc: 0.8947\n","\n","Epoch 71/99\n","----------\n","train Loss: 1.7864 Acc: 0.5439\n","val Loss: 0.9580 Acc: 0.8421\n","\n","Epoch 72/99\n","----------\n","train Loss: 1.6195 Acc: 0.5497\n","val Loss: 1.4716 Acc: 0.5263\n","\n","Epoch 73/99\n","----------\n","train Loss: 1.6880 Acc: 0.5322\n","val Loss: 1.5252 Acc: 0.5789\n","\n","Epoch 74/99\n","----------\n","train Loss: 1.6391 Acc: 0.5556\n","val Loss: 1.2570 Acc: 0.5789\n","\n","Epoch 75/99\n","----------\n","train Loss: 1.5132 Acc: 0.5789\n","val Loss: 1.5621 Acc: 0.4211\n","\n","Epoch 76/99\n","----------\n","train Loss: 1.6337 Acc: 0.5614\n","val Loss: 1.0245 Acc: 0.6842\n","\n","Epoch 77/99\n","----------\n","train Loss: 1.5806 Acc: 0.6082\n","val Loss: 1.0505 Acc: 0.7368\n","\n","Epoch 78/99\n","----------\n","train Loss: 1.6656 Acc: 0.5146\n","val Loss: 0.7228 Acc: 0.8421\n","\n","Epoch 79/99\n","----------\n","train Loss: 1.6282 Acc: 0.5263\n","val Loss: 1.3946 Acc: 0.5789\n","\n","Epoch 80/99\n","----------\n","train Loss: 1.6727 Acc: 0.5497\n","val Loss: 1.0823 Acc: 0.6842\n","\n","Epoch 81/99\n","----------\n","train Loss: 1.5837 Acc: 0.5731\n","val Loss: 1.4643 Acc: 0.6316\n","\n","Epoch 82/99\n","----------\n","train Loss: 1.5869 Acc: 0.6140\n","val Loss: 0.9642 Acc: 0.8421\n","\n","Epoch 83/99\n","----------\n","train Loss: 1.5265 Acc: 0.5848\n","val Loss: 1.1468 Acc: 0.6842\n","\n","Epoch 84/99\n","----------\n","train Loss: 1.7257 Acc: 0.4854\n","val Loss: 1.2686 Acc: 0.6316\n","\n","Epoch 85/99\n","----------\n","train Loss: 1.6089 Acc: 0.5789\n","val Loss: 1.0065 Acc: 0.7368\n","\n","Epoch 86/99\n","----------\n","train Loss: 1.6360 Acc: 0.5205\n","val Loss: 0.9598 Acc: 0.6842\n","\n","Epoch 87/99\n","----------\n","train Loss: 1.6989 Acc: 0.5205\n","val Loss: 0.9875 Acc: 0.6842\n","\n","Epoch 88/99\n","----------\n","train Loss: 1.6518 Acc: 0.5380\n","val Loss: 1.1801 Acc: 0.6316\n","\n","Epoch 89/99\n","----------\n","train Loss: 1.6138 Acc: 0.5614\n","val Loss: 1.0775 Acc: 0.7368\n","\n","Epoch 90/99\n","----------\n","train Loss: 1.8020 Acc: 0.4503\n","val Loss: 0.7948 Acc: 0.7895\n","\n","Epoch 91/99\n","----------\n","train Loss: 1.6180 Acc: 0.5497\n","val Loss: 1.2001 Acc: 0.5789\n","\n","Epoch 92/99\n","----------\n","train Loss: 1.5691 Acc: 0.5789\n","val Loss: 1.2044 Acc: 0.6316\n","\n","Epoch 93/99\n","----------\n","train Loss: 1.7040 Acc: 0.5029\n","val Loss: 0.7471 Acc: 0.8421\n","\n","Epoch 94/99\n","----------\n","train Loss: 1.6293 Acc: 0.5088\n","val Loss: 1.2103 Acc: 0.7368\n","\n","Epoch 95/99\n","----------\n","train Loss: 1.7126 Acc: 0.5263\n","val Loss: 1.0384 Acc: 0.6316\n","\n","Epoch 96/99\n","----------\n","train Loss: 1.6220 Acc: 0.5614\n","val Loss: 1.1355 Acc: 0.6316\n","\n","Epoch 97/99\n","----------\n","train Loss: 1.6286 Acc: 0.5380\n","val Loss: 1.0163 Acc: 0.6316\n","\n","Epoch 98/99\n","----------\n","train Loss: 1.6781 Acc: 0.5497\n","val Loss: 1.2344 Acc: 0.5789\n","\n","Epoch 99/99\n","----------\n","train Loss: 1.5796 Acc: 0.5673\n","val Loss: 1.0970 Acc: 0.7368\n","\n","Training complete in 20m 39s\n","Best val Acc: 0.894737\n","************************* 第 2 折 *************************\n","train: 171 val: 19\n","Epoch 0/99\n","----------\n","train Loss: 3.0031 Acc: 0.0351\n","val Loss: 2.9800 Acc: 0.1053\n","\n","Epoch 1/99\n","----------\n","train Loss: 3.0083 Acc: 0.0526\n","val Loss: 2.9729 Acc: 0.1053\n","\n","Epoch 2/99\n","----------\n","train Loss: 2.9955 Acc: 0.0468\n","val Loss: 3.0657 Acc: 0.0000\n","\n","Epoch 3/99\n","----------\n","train Loss: 3.0130 Acc: 0.0585\n","val Loss: 3.0298 Acc: 0.0526\n","\n","Epoch 4/99\n","----------\n","train Loss: 3.0191 Acc: 0.0526\n","val Loss: 3.0067 Acc: 0.0526\n","\n","Epoch 5/99\n","----------\n","train Loss: 3.0203 Acc: 0.0234\n","val Loss: 3.0661 Acc: 0.0526\n","\n","Epoch 6/99\n","----------\n","train Loss: 3.0210 Acc: 0.0585\n","val Loss: 2.9933 Acc: 0.1053\n","\n","Epoch 7/99\n","----------\n","train Loss: 3.0105 Acc: 0.0468\n","val Loss: 3.0370 Acc: 0.0526\n","\n","Epoch 8/99\n","----------\n","train Loss: 3.0117 Acc: 0.0409\n","val Loss: 3.0170 Acc: 0.0526\n","\n","Epoch 9/99\n","----------\n","train Loss: 3.0156 Acc: 0.0409\n","val Loss: 3.0136 Acc: 0.0526\n","\n","Epoch 10/99\n","----------\n","train Loss: 3.0068 Acc: 0.0643\n","val Loss: 3.0407 Acc: 0.0526\n","\n","Epoch 11/99\n","----------\n","train Loss: 3.0222 Acc: 0.0468\n","val Loss: 2.9599 Acc: 0.1053\n","\n","Epoch 12/99\n","----------\n","train Loss: 3.0030 Acc: 0.0468\n","val Loss: 3.1158 Acc: 0.0000\n","\n","Epoch 13/99\n","----------\n","train Loss: 3.0205 Acc: 0.0409\n","val Loss: 3.0169 Acc: 0.0526\n","\n","Epoch 14/99\n","----------\n","train Loss: 3.0056 Acc: 0.0468\n","val Loss: 3.0377 Acc: 0.1053\n","\n","Epoch 15/99\n","----------\n","train Loss: 3.0472 Acc: 0.0468\n","val Loss: 3.0797 Acc: 0.0526\n","\n","Epoch 16/99\n","----------\n","train Loss: 3.0464 Acc: 0.0585\n","val Loss: 3.0710 Acc: 0.0526\n","\n","Epoch 17/99\n","----------\n","train Loss: 3.0184 Acc: 0.0585\n","val Loss: 3.0209 Acc: 0.0526\n","\n","Epoch 18/99\n","----------\n","train Loss: 3.0109 Acc: 0.0409\n","val Loss: 3.0189 Acc: 0.0526\n","\n","Epoch 19/99\n","----------\n","train Loss: 3.0111 Acc: 0.0409\n","val Loss: 3.0759 Acc: 0.0000\n","\n","Epoch 20/99\n","----------\n","train Loss: 3.0257 Acc: 0.0409\n","val Loss: 3.0225 Acc: 0.0000\n","\n","Epoch 21/99\n","----------\n","train Loss: 3.0276 Acc: 0.0585\n","val Loss: 3.0309 Acc: 0.0000\n","\n","Epoch 22/99\n","----------\n","train Loss: 2.9941 Acc: 0.0468\n","val Loss: 3.0414 Acc: 0.0000\n","\n","Epoch 23/99\n","----------\n","train Loss: 3.0159 Acc: 0.0526\n","val Loss: 2.9910 Acc: 0.1053\n","\n","Epoch 24/99\n","----------\n","train Loss: 3.0215 Acc: 0.0351\n","val Loss: 3.0127 Acc: 0.1053\n","\n","Epoch 25/99\n","----------\n","train Loss: 3.0110 Acc: 0.0468\n","val Loss: 3.0121 Acc: 0.0526\n","\n","Epoch 26/99\n","----------\n","train Loss: 3.0075 Acc: 0.0585\n","val Loss: 3.0035 Acc: 0.1053\n","\n","Epoch 27/99\n","----------\n","train Loss: 3.0063 Acc: 0.0526\n","val Loss: 2.9900 Acc: 0.0526\n","\n","Epoch 28/99\n","----------\n","train Loss: 3.0053 Acc: 0.0468\n","val Loss: 3.0357 Acc: 0.0526\n","\n","Epoch 29/99\n","----------\n","train Loss: 3.0114 Acc: 0.0819\n","val Loss: 3.1197 Acc: 0.0000\n","\n","Epoch 30/99\n","----------\n","train Loss: 2.9985 Acc: 0.0526\n","val Loss: 3.0155 Acc: 0.1579\n","\n","Epoch 31/99\n","----------\n","train Loss: 2.9871 Acc: 0.0643\n","val Loss: 3.0067 Acc: 0.0526\n","\n","Epoch 32/99\n","----------\n","train Loss: 3.0199 Acc: 0.0585\n","val Loss: 3.0328 Acc: 0.1053\n","\n","Epoch 33/99\n","----------\n","train Loss: 3.0191 Acc: 0.0409\n","val Loss: 2.9941 Acc: 0.1579\n","\n","Epoch 34/99\n","----------\n","train Loss: 3.0356 Acc: 0.0643\n","val Loss: 3.0796 Acc: 0.0526\n","\n","Epoch 35/99\n","----------\n","train Loss: 3.0193 Acc: 0.0409\n","val Loss: 2.9823 Acc: 0.0526\n","\n","Epoch 36/99\n","----------\n","train Loss: 2.9911 Acc: 0.0409\n","val Loss: 3.0615 Acc: 0.1053\n","\n","Epoch 37/99\n","----------\n","train Loss: 3.0399 Acc: 0.0526\n","val Loss: 3.0751 Acc: 0.1053\n","\n","Epoch 38/99\n","----------\n","train Loss: 3.0106 Acc: 0.0585\n","val Loss: 3.0350 Acc: 0.1053\n","\n","Epoch 39/99\n","----------\n","train Loss: 2.9930 Acc: 0.0409\n","val Loss: 2.9806 Acc: 0.0000\n","\n","Epoch 40/99\n","----------\n","train Loss: 2.9963 Acc: 0.0526\n","val Loss: 3.0435 Acc: 0.0000\n","\n","Epoch 41/99\n","----------\n","train Loss: 3.0141 Acc: 0.0526\n","val Loss: 3.0053 Acc: 0.0526\n","\n","Epoch 42/99\n","----------\n","train Loss: 3.0031 Acc: 0.0526\n","val Loss: 3.0666 Acc: 0.0526\n","\n","Epoch 43/99\n","----------\n","train Loss: 3.0086 Acc: 0.0468\n","val Loss: 3.0262 Acc: 0.1053\n","\n","Epoch 44/99\n","----------\n","train Loss: 3.0096 Acc: 0.0702\n","val Loss: 3.0235 Acc: 0.1053\n","\n","Epoch 45/99\n","----------\n","train Loss: 3.0423 Acc: 0.0409\n","val Loss: 2.9487 Acc: 0.0526\n","\n","Epoch 46/99\n","----------\n","train Loss: 3.0080 Acc: 0.0702\n","val Loss: 3.0365 Acc: 0.1053\n","\n","Epoch 47/99\n","----------\n","train Loss: 3.0121 Acc: 0.0292\n","val Loss: 3.0207 Acc: 0.0526\n","\n","Epoch 48/99\n","----------\n","train Loss: 3.0126 Acc: 0.0585\n","val Loss: 3.0516 Acc: 0.0526\n","\n","Epoch 49/99\n","----------\n","train Loss: 3.0081 Acc: 0.0409\n","val Loss: 3.0404 Acc: 0.0526\n","\n","Epoch 50/99\n","----------\n","train Loss: 3.0089 Acc: 0.0585\n","val Loss: 2.9803 Acc: 0.0526\n","\n","Epoch 51/99\n","----------\n","train Loss: 3.0404 Acc: 0.0468\n","val Loss: 3.0103 Acc: 0.0526\n","\n","Epoch 52/99\n","----------\n","train Loss: 2.9961 Acc: 0.0409\n","val Loss: 3.0176 Acc: 0.0526\n","\n","Epoch 53/99\n","----------\n","train Loss: 3.0122 Acc: 0.0468\n","val Loss: 2.9745 Acc: 0.0526\n","\n","Epoch 54/99\n","----------\n","train Loss: 3.0049 Acc: 0.0526\n","val Loss: 2.9785 Acc: 0.1053\n","\n","Epoch 55/99\n","----------\n","train Loss: 3.0180 Acc: 0.0585\n","val Loss: 2.8837 Acc: 0.0526\n","\n","Epoch 56/99\n","----------\n","train Loss: 2.9939 Acc: 0.0643\n","val Loss: 2.9954 Acc: 0.0000\n","\n","Epoch 57/99\n","----------\n","train Loss: 3.0278 Acc: 0.0409\n","val Loss: 2.9992 Acc: 0.0526\n","\n","Epoch 58/99\n","----------\n","train Loss: 3.0209 Acc: 0.0526\n","val Loss: 2.9208 Acc: 0.1053\n","\n","Epoch 59/99\n","----------\n","train Loss: 3.0082 Acc: 0.0526\n","val Loss: 2.9848 Acc: 0.0000\n","\n","Epoch 60/99\n","----------\n","train Loss: 3.0033 Acc: 0.0409\n","val Loss: 3.0382 Acc: 0.0526\n","\n","Epoch 61/99\n","----------\n","train Loss: 3.0084 Acc: 0.0468\n","val Loss: 2.9325 Acc: 0.0526\n","\n","Epoch 62/99\n","----------\n","train Loss: 3.0148 Acc: 0.0468\n","val Loss: 3.0512 Acc: 0.0000\n","\n","Epoch 63/99\n","----------\n","train Loss: 3.0393 Acc: 0.0351\n","val Loss: 3.0110 Acc: 0.0000\n","\n","Epoch 64/99\n","----------\n","train Loss: 3.0031 Acc: 0.0702\n","val Loss: 2.9926 Acc: 0.1053\n","\n","Epoch 65/99\n","----------\n","train Loss: 3.0330 Acc: 0.0351\n","val Loss: 3.0370 Acc: 0.1053\n","\n","Epoch 66/99\n","----------\n","train Loss: 3.0076 Acc: 0.0468\n","val Loss: 3.0131 Acc: 0.0000\n","\n","Epoch 67/99\n","----------\n","train Loss: 3.0202 Acc: 0.0468\n","val Loss: 2.9534 Acc: 0.0526\n","\n","Epoch 68/99\n","----------\n","train Loss: 3.0167 Acc: 0.0585\n","val Loss: 3.0551 Acc: 0.1053\n","\n","Epoch 69/99\n","----------\n","train Loss: 3.0262 Acc: 0.0585\n","val Loss: 3.0597 Acc: 0.0000\n","\n","Epoch 70/99\n","----------\n","train Loss: 2.9974 Acc: 0.0526\n","val Loss: 3.0687 Acc: 0.0526\n","\n","Epoch 71/99\n","----------\n","train Loss: 3.0136 Acc: 0.0409\n","val Loss: 2.9669 Acc: 0.1053\n","\n","Epoch 72/99\n","----------\n","train Loss: 3.0014 Acc: 0.0643\n","val Loss: 3.0686 Acc: 0.0000\n","\n","Epoch 73/99\n","----------\n","train Loss: 2.9798 Acc: 0.0585\n","val Loss: 3.0739 Acc: 0.0526\n","\n","Epoch 74/99\n","----------\n","train Loss: 3.0055 Acc: 0.0526\n","val Loss: 2.9721 Acc: 0.1053\n","\n","Epoch 75/99\n","----------\n","train Loss: 3.0149 Acc: 0.0643\n","val Loss: 3.0128 Acc: 0.0526\n","\n","Epoch 76/99\n","----------\n","train Loss: 3.0121 Acc: 0.0468\n","val Loss: 3.0112 Acc: 0.0000\n","\n","Epoch 77/99\n","----------\n","train Loss: 3.0240 Acc: 0.0526\n","val Loss: 2.9822 Acc: 0.0526\n","\n","Epoch 78/99\n","----------\n","train Loss: 3.0124 Acc: 0.0526\n","val Loss: 2.8926 Acc: 0.1053\n","\n","Epoch 79/99\n","----------\n","train Loss: 3.0243 Acc: 0.0468\n","val Loss: 3.0386 Acc: 0.0526\n","\n","Epoch 80/99\n","----------\n","train Loss: 3.0215 Acc: 0.0468\n","val Loss: 2.9956 Acc: 0.0000\n","\n","Epoch 81/99\n","----------\n","train Loss: 3.0043 Acc: 0.0526\n","val Loss: 3.0177 Acc: 0.0526\n","\n","Epoch 82/99\n","----------\n","train Loss: 3.0198 Acc: 0.0468\n","val Loss: 2.9741 Acc: 0.1579\n","\n","Epoch 83/99\n","----------\n","train Loss: 3.0085 Acc: 0.0409\n","val Loss: 2.9411 Acc: 0.1053\n","\n","Epoch 84/99\n","----------\n","train Loss: 3.0172 Acc: 0.0468\n","val Loss: 2.9742 Acc: 0.0526\n","\n","Epoch 85/99\n","----------\n","train Loss: 3.0261 Acc: 0.0585\n","val Loss: 3.0073 Acc: 0.0526\n","\n","Epoch 86/99\n","----------\n","train Loss: 3.0065 Acc: 0.0702\n","val Loss: 3.0754 Acc: 0.0526\n","\n","Epoch 87/99\n","----------\n","train Loss: 3.0180 Acc: 0.0643\n","val Loss: 3.0550 Acc: 0.0526\n","\n","Epoch 88/99\n","----------\n","train Loss: 2.9942 Acc: 0.0526\n","val Loss: 2.9850 Acc: 0.0526\n","\n","Epoch 89/99\n","----------\n","train Loss: 3.0067 Acc: 0.0234\n","val Loss: 3.0454 Acc: 0.0000\n","\n","Epoch 90/99\n","----------\n","train Loss: 3.0174 Acc: 0.0643\n","val Loss: 3.0361 Acc: 0.0526\n","\n","Epoch 91/99\n","----------\n","train Loss: 3.0206 Acc: 0.0409\n","val Loss: 2.9471 Acc: 0.1053\n","\n","Epoch 92/99\n","----------\n","train Loss: 3.0230 Acc: 0.0468\n","val Loss: 3.0171 Acc: 0.0526\n","\n","Epoch 93/99\n","----------\n","train Loss: 2.9997 Acc: 0.0585\n","val Loss: 3.0336 Acc: 0.0526\n","\n","Epoch 94/99\n","----------\n","train Loss: 3.0375 Acc: 0.0526\n","val Loss: 2.9875 Acc: 0.0526\n","\n","Epoch 95/99\n","----------\n","train Loss: 2.9882 Acc: 0.0468\n","val Loss: 3.0805 Acc: 0.0000\n","\n","Epoch 96/99\n","----------\n","train Loss: 3.0139 Acc: 0.0468\n","val Loss: 3.0907 Acc: 0.0526\n","\n","Epoch 97/99\n","----------\n","train Loss: 3.0305 Acc: 0.0175\n","val Loss: 3.0496 Acc: 0.0526\n","\n","Epoch 98/99\n","----------\n","train Loss: 3.0039 Acc: 0.0351\n","val Loss: 3.0577 Acc: 0.1579\n","\n","Epoch 99/99\n","----------\n","train Loss: 2.9956 Acc: 0.0585\n","val Loss: 2.9969 Acc: 0.1053\n","\n","Training complete in 20m 37s\n","Best val Acc: 0.157895\n","************************* 第 3 折 *************************\n","train: 171 val: 19\n","Epoch 0/99\n","----------\n","train Loss: 3.0398 Acc: 0.0351\n","val Loss: 3.0319 Acc: 0.0526\n","\n","Epoch 1/99\n","----------\n","train Loss: 3.0106 Acc: 0.0409\n","val Loss: 3.0340 Acc: 0.0526\n","\n","Epoch 2/99\n","----------\n","train Loss: 3.0372 Acc: 0.0292\n","val Loss: 3.0393 Acc: 0.0526\n","\n","Epoch 3/99\n","----------\n","train Loss: 3.0170 Acc: 0.0409\n","val Loss: 3.0599 Acc: 0.0000\n","\n","Epoch 4/99\n","----------\n","train Loss: 3.0278 Acc: 0.0468\n","val Loss: 3.0276 Acc: 0.0526\n","\n","Epoch 5/99\n","----------\n","train Loss: 3.0012 Acc: 0.0468\n","val Loss: 2.9805 Acc: 0.1053\n","\n","Epoch 6/99\n","----------\n","train Loss: 3.0089 Acc: 0.0526\n","val Loss: 3.0915 Acc: 0.1053\n","\n","Epoch 7/99\n","----------\n","train Loss: 3.0412 Acc: 0.0351\n","val Loss: 2.9874 Acc: 0.0526\n","\n","Epoch 8/99\n","----------\n","train Loss: 3.0192 Acc: 0.0526\n","val Loss: 3.0335 Acc: 0.1053\n","\n","Epoch 9/99\n","----------\n","train Loss: 3.0238 Acc: 0.0468\n","val Loss: 3.0350 Acc: 0.0526\n","\n","Epoch 10/99\n","----------\n","train Loss: 3.0002 Acc: 0.0468\n","val Loss: 3.0297 Acc: 0.0000\n","\n","Epoch 11/99\n","----------\n","train Loss: 3.0126 Acc: 0.0585\n","val Loss: 3.0370 Acc: 0.0526\n","\n","Epoch 12/99\n","----------\n","train Loss: 2.9888 Acc: 0.0468\n","val Loss: 2.9482 Acc: 0.0526\n","\n","Epoch 13/99\n","----------\n","train Loss: 3.0097 Acc: 0.0526\n","val Loss: 2.9644 Acc: 0.0000\n","\n","Epoch 14/99\n","----------\n","train Loss: 3.0230 Acc: 0.0643\n","val Loss: 3.0037 Acc: 0.1579\n","\n","Epoch 15/99\n","----------\n","train Loss: 3.0229 Acc: 0.0468\n","val Loss: 3.0155 Acc: 0.1053\n","\n","Epoch 16/99\n","----------\n","train Loss: 3.0132 Acc: 0.0409\n","val Loss: 3.0166 Acc: 0.0526\n","\n","Epoch 17/99\n","----------\n","train Loss: 3.0121 Acc: 0.0468\n","val Loss: 2.9999 Acc: 0.0526\n","\n","Epoch 18/99\n","----------\n","train Loss: 3.0118 Acc: 0.0526\n","val Loss: 2.9996 Acc: 0.0526\n","\n","Epoch 19/99\n","----------\n","train Loss: 2.9995 Acc: 0.0468\n","val Loss: 2.9725 Acc: 0.0526\n","\n","Epoch 20/99\n","----------\n","train Loss: 3.0135 Acc: 0.0468\n","val Loss: 3.0037 Acc: 0.0526\n","\n","Epoch 21/99\n","----------\n","train Loss: 3.0172 Acc: 0.0585\n","val Loss: 3.0375 Acc: 0.0526\n","\n","Epoch 22/99\n","----------\n","train Loss: 3.0036 Acc: 0.0585\n","val Loss: 3.0544 Acc: 0.0526\n","\n","Epoch 23/99\n","----------\n","train Loss: 3.0194 Acc: 0.0585\n","val Loss: 3.0074 Acc: 0.0526\n","\n","Epoch 24/99\n","----------\n","train Loss: 3.0195 Acc: 0.0409\n","val Loss: 2.9493 Acc: 0.0526\n","\n","Epoch 25/99\n","----------\n","train Loss: 3.0267 Acc: 0.0351\n","val Loss: 2.9806 Acc: 0.1053\n","\n","Epoch 26/99\n","----------\n","train Loss: 2.9931 Acc: 0.0702\n","val Loss: 3.0335 Acc: 0.1053\n","\n","Epoch 27/99\n","----------\n","train Loss: 3.0343 Acc: 0.0526\n","val Loss: 2.9560 Acc: 0.0526\n","\n","Epoch 28/99\n","----------\n","train Loss: 3.0293 Acc: 0.0409\n","val Loss: 3.0286 Acc: 0.0526\n","\n","Epoch 29/99\n","----------\n","train Loss: 3.0049 Acc: 0.0526\n","val Loss: 3.0202 Acc: 0.0526\n","\n","Epoch 30/99\n","----------\n","train Loss: 3.0120 Acc: 0.0409\n","val Loss: 3.0262 Acc: 0.0526\n","\n","Epoch 31/99\n","----------\n","train Loss: 3.0304 Acc: 0.0468\n","val Loss: 3.0701 Acc: 0.0000\n","\n","Epoch 32/99\n","----------\n","train Loss: 3.0256 Acc: 0.0526\n","val Loss: 3.0200 Acc: 0.0526\n","\n","Epoch 33/99\n","----------\n","train Loss: 3.0341 Acc: 0.0526\n","val Loss: 3.0629 Acc: 0.0526\n","\n","Epoch 34/99\n","----------\n","train Loss: 3.0157 Acc: 0.0585\n","val Loss: 2.9662 Acc: 0.0526\n","\n","Epoch 35/99\n","----------\n","train Loss: 2.9943 Acc: 0.0585\n","val Loss: 3.0553 Acc: 0.0526\n","\n","Epoch 36/99\n","----------\n","train Loss: 3.0051 Acc: 0.0585\n","val Loss: 3.0661 Acc: 0.0000\n","\n","Epoch 37/99\n","----------\n","train Loss: 3.0160 Acc: 0.0409\n","val Loss: 3.0027 Acc: 0.0000\n","\n","Epoch 38/99\n","----------\n","train Loss: 3.0014 Acc: 0.0468\n","val Loss: 3.0038 Acc: 0.0000\n","\n","Epoch 39/99\n","----------\n","train Loss: 3.0287 Acc: 0.0585\n","val Loss: 3.0367 Acc: 0.1053\n","\n","Epoch 40/99\n","----------\n","train Loss: 3.0225 Acc: 0.0468\n","val Loss: 3.0263 Acc: 0.1053\n","\n","Epoch 41/99\n","----------\n","train Loss: 2.9962 Acc: 0.0702\n","val Loss: 2.8951 Acc: 0.1053\n","\n","Epoch 42/99\n","----------\n","train Loss: 3.0108 Acc: 0.0351\n","val Loss: 2.9854 Acc: 0.0526\n","\n","Epoch 43/99\n","----------\n","train Loss: 3.0031 Acc: 0.0643\n","val Loss: 3.1071 Acc: 0.0526\n","\n","Epoch 44/99\n","----------\n","train Loss: 3.0180 Acc: 0.0468\n","val Loss: 2.9643 Acc: 0.0526\n","\n","Epoch 45/99\n","----------\n","train Loss: 2.9981 Acc: 0.0526\n","val Loss: 3.0321 Acc: 0.0526\n","\n","Epoch 46/99\n","----------\n","train Loss: 3.0115 Acc: 0.0409\n","val Loss: 3.0359 Acc: 0.0526\n","\n","Epoch 47/99\n","----------\n","train Loss: 3.0019 Acc: 0.0585\n","val Loss: 3.0099 Acc: 0.1053\n","\n","Epoch 48/99\n","----------\n","train Loss: 3.0247 Acc: 0.0526\n","val Loss: 3.1100 Acc: 0.0526\n","\n","Epoch 49/99\n","----------\n","train Loss: 2.9982 Acc: 0.0702\n","val Loss: 3.0260 Acc: 0.0000\n","\n","Epoch 50/99\n","----------\n","train Loss: 3.0062 Acc: 0.0351\n","val Loss: 3.0121 Acc: 0.0000\n","\n","Epoch 51/99\n","----------\n","train Loss: 3.0038 Acc: 0.0292\n","val Loss: 2.9703 Acc: 0.0526\n","\n","Epoch 52/99\n","----------\n","train Loss: 3.0150 Acc: 0.0409\n","val Loss: 3.0055 Acc: 0.0526\n","\n","Epoch 53/99\n","----------\n","train Loss: 3.0145 Acc: 0.0409\n","val Loss: 3.0396 Acc: 0.0526\n","\n","Epoch 54/99\n","----------\n","train Loss: 3.0100 Acc: 0.0351\n","val Loss: 3.0238 Acc: 0.0000\n","\n","Epoch 55/99\n","----------\n","train Loss: 3.0179 Acc: 0.0468\n","val Loss: 3.0375 Acc: 0.0526\n","\n","Epoch 56/99\n","----------\n","train Loss: 2.9775 Acc: 0.0526\n","val Loss: 2.9845 Acc: 0.0526\n","\n","Epoch 57/99\n","----------\n","train Loss: 3.0279 Acc: 0.0585\n","val Loss: 3.0899 Acc: 0.0526\n","\n","Epoch 58/99\n","----------\n","train Loss: 3.0217 Acc: 0.0585\n","val Loss: 2.9869 Acc: 0.0526\n","\n","Epoch 59/99\n","----------\n","train Loss: 3.0057 Acc: 0.0643\n","val Loss: 2.9652 Acc: 0.0526\n","\n","Epoch 60/99\n","----------\n","train Loss: 3.0218 Acc: 0.0468\n","val Loss: 3.0458 Acc: 0.0526\n","\n","Epoch 61/99\n","----------\n","train Loss: 3.0059 Acc: 0.0585\n","val Loss: 3.0149 Acc: 0.0526\n","\n","Epoch 62/99\n","----------\n","train Loss: 3.0169 Acc: 0.0526\n","val Loss: 2.9945 Acc: 0.0526\n","\n","Epoch 63/99\n","----------\n","train Loss: 3.0238 Acc: 0.0526\n","val Loss: 3.0652 Acc: 0.0000\n","\n","Epoch 64/99\n","----------\n","train Loss: 3.0249 Acc: 0.0468\n","val Loss: 3.0371 Acc: 0.0526\n","\n","Epoch 65/99\n","----------\n","train Loss: 3.0215 Acc: 0.0585\n","val Loss: 3.0155 Acc: 0.0526\n","\n","Epoch 66/99\n","----------\n","train Loss: 3.0027 Acc: 0.0643\n","val Loss: 3.0225 Acc: 0.1053\n","\n","Epoch 67/99\n","----------\n","train Loss: 3.0034 Acc: 0.0585\n","val Loss: 3.0073 Acc: 0.0526\n","\n","Epoch 68/99\n","----------\n","train Loss: 2.9893 Acc: 0.0468\n","val Loss: 3.0534 Acc: 0.0000\n","\n","Epoch 69/99\n","----------\n","train Loss: 3.0024 Acc: 0.0526\n","val Loss: 3.0748 Acc: 0.0526\n","\n","Epoch 70/99\n","----------\n","train Loss: 3.0027 Acc: 0.0526\n","val Loss: 3.0626 Acc: 0.0526\n","\n","Epoch 71/99\n","----------\n","train Loss: 3.0198 Acc: 0.0643\n","val Loss: 2.9775 Acc: 0.1053\n","\n","Epoch 72/99\n","----------\n","train Loss: 3.0266 Acc: 0.0468\n","val Loss: 3.0039 Acc: 0.0526\n","\n","Epoch 73/99\n","----------\n","train Loss: 3.0232 Acc: 0.0468\n","val Loss: 2.9800 Acc: 0.1579\n","\n","Epoch 74/99\n","----------\n","train Loss: 3.0210 Acc: 0.0585\n","val Loss: 2.9038 Acc: 0.0526\n","\n","Epoch 75/99\n","----------\n","train Loss: 3.0035 Acc: 0.0585\n","val Loss: 2.9798 Acc: 0.0526\n","\n","Epoch 76/99\n","----------\n","train Loss: 3.0173 Acc: 0.0585\n","val Loss: 3.0781 Acc: 0.0526\n","\n","Epoch 77/99\n","----------\n","train Loss: 3.0133 Acc: 0.0468\n","val Loss: 2.9887 Acc: 0.0526\n","\n","Epoch 78/99\n","----------\n","train Loss: 2.9991 Acc: 0.0585\n","val Loss: 3.1163 Acc: 0.0000\n","\n","Epoch 79/99\n","----------\n","train Loss: 3.0011 Acc: 0.0643\n","val Loss: 3.0819 Acc: 0.0526\n","\n","Epoch 80/99\n","----------\n","train Loss: 3.0245 Acc: 0.0526\n","val Loss: 2.9915 Acc: 0.1053\n","\n","Epoch 81/99\n","----------\n","train Loss: 3.0026 Acc: 0.0409\n","val Loss: 3.0399 Acc: 0.0526\n","\n","Epoch 82/99\n","----------\n","train Loss: 3.0026 Acc: 0.0292\n","val Loss: 3.0274 Acc: 0.0526\n","\n","Epoch 83/99\n","----------\n","train Loss: 3.0010 Acc: 0.0585\n","val Loss: 3.0967 Acc: 0.0000\n","\n","Epoch 84/99\n","----------\n","train Loss: 3.0231 Acc: 0.0468\n","val Loss: 3.0166 Acc: 0.1053\n","\n","Epoch 85/99\n","----------\n","train Loss: 3.0213 Acc: 0.0585\n","val Loss: 3.0670 Acc: 0.1053\n","\n","Epoch 86/99\n","----------\n","train Loss: 3.0095 Acc: 0.0760\n","val Loss: 3.1090 Acc: 0.1579\n","\n","Epoch 87/99\n","----------\n","train Loss: 3.0003 Acc: 0.0643\n","val Loss: 2.9726 Acc: 0.0000\n","\n","Epoch 88/99\n","----------\n","train Loss: 2.9865 Acc: 0.0643\n","val Loss: 3.0181 Acc: 0.0000\n","\n","Epoch 89/99\n","----------\n","train Loss: 3.0231 Acc: 0.0468\n","val Loss: 3.0119 Acc: 0.0526\n","\n","Epoch 90/99\n","----------\n","train Loss: 3.0075 Acc: 0.0468\n","val Loss: 3.0230 Acc: 0.0526\n","\n","Epoch 91/99\n","----------\n","train Loss: 2.9986 Acc: 0.0585\n","val Loss: 3.1216 Acc: 0.0000\n","\n","Epoch 92/99\n","----------\n","train Loss: 3.0103 Acc: 0.0526\n","val Loss: 3.1174 Acc: 0.0000\n","\n","Epoch 93/99\n","----------\n","train Loss: 3.0313 Acc: 0.0351\n","val Loss: 3.0408 Acc: 0.0526\n","\n","Epoch 94/99\n","----------\n","train Loss: 3.0239 Acc: 0.0468\n","val Loss: 2.9858 Acc: 0.0000\n","\n","Epoch 95/99\n","----------\n","train Loss: 3.0228 Acc: 0.0585\n","val Loss: 3.0186 Acc: 0.1053\n","\n","Epoch 96/99\n","----------\n","train Loss: 3.0038 Acc: 0.0643\n","val Loss: 3.0004 Acc: 0.0526\n","\n","Epoch 97/99\n","----------\n","train Loss: 2.9910 Acc: 0.0526\n","val Loss: 3.0063 Acc: 0.1053\n","\n","Epoch 98/99\n","----------\n","train Loss: 3.0099 Acc: 0.0526\n","val Loss: 3.0887 Acc: 0.0526\n","\n","Epoch 99/99\n","----------\n","train Loss: 3.0144 Acc: 0.0468\n","val Loss: 3.0204 Acc: 0.0526\n","\n","Training complete in 20m 38s\n","Best val Acc: 0.157895\n","************************* 第 4 折 *************************\n","train: 171 val: 19\n","Epoch 0/99\n","----------\n","train Loss: 3.0172 Acc: 0.0409\n","val Loss: 3.1255 Acc: 0.0526\n","\n","Epoch 1/99\n","----------\n","train Loss: 3.0125 Acc: 0.0526\n","val Loss: 3.0073 Acc: 0.0526\n","\n","Epoch 2/99\n","----------\n","train Loss: 3.0232 Acc: 0.0643\n","val Loss: 3.1000 Acc: 0.0000\n","\n","Epoch 3/99\n","----------\n","train Loss: 3.0063 Acc: 0.0526\n","val Loss: 3.0130 Acc: 0.0000\n","\n","Epoch 4/99\n","----------\n","train Loss: 2.9996 Acc: 0.0468\n","val Loss: 3.0114 Acc: 0.0000\n","\n","Epoch 5/99\n","----------\n","train Loss: 2.9859 Acc: 0.0468\n","val Loss: 2.9968 Acc: 0.0526\n","\n","Epoch 6/99\n","----------\n","train Loss: 2.9918 Acc: 0.0585\n","val Loss: 3.0060 Acc: 0.0526\n","\n","Epoch 7/99\n","----------\n","train Loss: 3.0188 Acc: 0.0643\n","val Loss: 2.9936 Acc: 0.1053\n","\n","Epoch 8/99\n","----------\n","train Loss: 2.9887 Acc: 0.0526\n","val Loss: 2.9254 Acc: 0.1053\n","\n","Epoch 9/99\n","----------\n","train Loss: 3.0089 Acc: 0.0702\n","val Loss: 3.0007 Acc: 0.0526\n","\n","Epoch 10/99\n","----------\n","train Loss: 3.0162 Acc: 0.0877\n","val Loss: 3.0027 Acc: 0.0000\n","\n","Epoch 11/99\n","----------\n","train Loss: 3.0011 Acc: 0.0819\n","val Loss: 3.0257 Acc: 0.1053\n","\n","Epoch 12/99\n","----------\n","train Loss: 3.0199 Acc: 0.0526\n","val Loss: 3.0210 Acc: 0.0526\n","\n","Epoch 13/99\n","----------\n","train Loss: 3.0022 Acc: 0.0351\n","val Loss: 3.0404 Acc: 0.0526\n","\n","Epoch 14/99\n","----------\n","train Loss: 3.0245 Acc: 0.0468\n","val Loss: 2.9799 Acc: 0.1053\n","\n","Epoch 15/99\n","----------\n","train Loss: 3.0188 Acc: 0.0702\n","val Loss: 3.0020 Acc: 0.1053\n","\n","Epoch 16/99\n","----------\n","train Loss: 2.9875 Acc: 0.0585\n","val Loss: 3.0692 Acc: 0.0000\n","\n","Epoch 17/99\n","----------\n","train Loss: 3.0063 Acc: 0.0468\n","val Loss: 2.9800 Acc: 0.0526\n","\n","Epoch 18/99\n","----------\n","train Loss: 3.0121 Acc: 0.0585\n","val Loss: 3.0214 Acc: 0.1053\n","\n","Epoch 19/99\n","----------\n","train Loss: 3.0196 Acc: 0.0468\n","val Loss: 2.9721 Acc: 0.0526\n","\n","Epoch 20/99\n","----------\n","train Loss: 3.0027 Acc: 0.0468\n","val Loss: 3.0731 Acc: 0.0000\n","\n","Epoch 21/99\n","----------\n","train Loss: 3.0088 Acc: 0.0468\n","val Loss: 3.0040 Acc: 0.0526\n","\n","Epoch 22/99\n","----------\n","train Loss: 2.9844 Acc: 0.0409\n","val Loss: 3.0388 Acc: 0.0000\n","\n","Epoch 23/99\n","----------\n","train Loss: 3.0086 Acc: 0.0468\n","val Loss: 2.9175 Acc: 0.0526\n","\n","Epoch 24/99\n","----------\n","train Loss: 3.0196 Acc: 0.0585\n","val Loss: 3.0540 Acc: 0.1053\n","\n","Epoch 25/99\n","----------\n","train Loss: 3.0221 Acc: 0.0702\n","val Loss: 3.0355 Acc: 0.0526\n","\n","Epoch 26/99\n","----------\n","train Loss: 3.0014 Acc: 0.0585\n","val Loss: 2.9093 Acc: 0.0526\n","\n","Epoch 27/99\n","----------\n","train Loss: 3.0255 Acc: 0.0468\n","val Loss: 3.0360 Acc: 0.1053\n","\n","Epoch 28/99\n","----------\n","train Loss: 3.0219 Acc: 0.0585\n","val Loss: 3.0464 Acc: 0.0526\n","\n","Epoch 29/99\n","----------\n","train Loss: 3.0126 Acc: 0.0468\n","val Loss: 2.9543 Acc: 0.0526\n","\n","Epoch 30/99\n","----------\n","train Loss: 3.0057 Acc: 0.0468\n","val Loss: 3.0024 Acc: 0.0000\n","\n","Epoch 31/99\n","----------\n","train Loss: 3.0166 Acc: 0.0526\n","val Loss: 3.0271 Acc: 0.0526\n","\n","Epoch 32/99\n","----------\n","train Loss: 3.0035 Acc: 0.0585\n","val Loss: 3.0194 Acc: 0.0526\n","\n","Epoch 33/99\n","----------\n","train Loss: 3.0131 Acc: 0.0643\n","val Loss: 3.0457 Acc: 0.0526\n","\n","Epoch 34/99\n","----------\n","train Loss: 3.0083 Acc: 0.0760\n","val Loss: 2.8917 Acc: 0.1579\n","\n","Epoch 35/99\n","----------\n","train Loss: 3.0129 Acc: 0.0526\n","val Loss: 3.1163 Acc: 0.0526\n","\n","Epoch 36/99\n","----------\n","train Loss: 3.0081 Acc: 0.0409\n","val Loss: 3.0457 Acc: 0.0000\n","\n","Epoch 37/99\n","----------\n","train Loss: 3.0050 Acc: 0.0351\n","val Loss: 3.0837 Acc: 0.0526\n","\n","Epoch 38/99\n","----------\n","train Loss: 3.0087 Acc: 0.0351\n","val Loss: 3.0235 Acc: 0.0000\n","\n","Epoch 39/99\n","----------\n","train Loss: 3.0310 Acc: 0.0526\n","val Loss: 3.0201 Acc: 0.1053\n","\n","Epoch 40/99\n","----------\n","train Loss: 3.0071 Acc: 0.0526\n","val Loss: 3.0755 Acc: 0.0526\n","\n","Epoch 41/99\n","----------\n","train Loss: 3.0359 Acc: 0.0351\n","val Loss: 2.9592 Acc: 0.0526\n","\n","Epoch 42/99\n","----------\n","train Loss: 3.0191 Acc: 0.0468\n","val Loss: 3.0057 Acc: 0.1053\n","\n","Epoch 43/99\n","----------\n","train Loss: 3.0155 Acc: 0.0351\n","val Loss: 2.9987 Acc: 0.0000\n","\n","Epoch 44/99\n","----------\n","train Loss: 3.0049 Acc: 0.0468\n","val Loss: 3.0115 Acc: 0.1053\n","\n","Epoch 45/99\n","----------\n","train Loss: 3.0328 Acc: 0.0526\n","val Loss: 3.0066 Acc: 0.0526\n","\n","Epoch 46/99\n","----------\n","train Loss: 3.0377 Acc: 0.0526\n","val Loss: 2.9189 Acc: 0.1053\n","\n","Epoch 47/99\n","----------\n","train Loss: 2.9906 Acc: 0.0585\n","val Loss: 3.0711 Acc: 0.0526\n","\n","Epoch 48/99\n","----------\n","train Loss: 3.0019 Acc: 0.0526\n","val Loss: 3.0180 Acc: 0.0526\n","\n","Epoch 49/99\n","----------\n","train Loss: 3.0149 Acc: 0.0643\n","val Loss: 2.9849 Acc: 0.0000\n","\n","Epoch 50/99\n","----------\n","train Loss: 3.0132 Acc: 0.0468\n","val Loss: 3.0443 Acc: 0.0000\n","\n","Epoch 51/99\n","----------\n","train Loss: 3.0112 Acc: 0.0409\n","val Loss: 2.9901 Acc: 0.0526\n","\n","Epoch 52/99\n","----------\n","train Loss: 2.9978 Acc: 0.0409\n","val Loss: 3.0051 Acc: 0.0526\n","\n","Epoch 53/99\n","----------\n","train Loss: 3.0288 Acc: 0.0643\n","val Loss: 2.9892 Acc: 0.0526\n","\n","Epoch 54/99\n","----------\n","train Loss: 3.0086 Acc: 0.0585\n","val Loss: 2.9094 Acc: 0.0526\n","\n","Epoch 55/99\n","----------\n","train Loss: 3.0281 Acc: 0.0468\n","val Loss: 3.0210 Acc: 0.1053\n","\n","Epoch 56/99\n","----------\n","train Loss: 3.0152 Acc: 0.0409\n","val Loss: 2.9461 Acc: 0.0526\n","\n","Epoch 57/99\n","----------\n","train Loss: 2.9990 Acc: 0.0585\n","val Loss: 2.9938 Acc: 0.0000\n","\n","Epoch 58/99\n","----------\n","train Loss: 3.0294 Acc: 0.0468\n","val Loss: 3.0310 Acc: 0.0000\n","\n","Epoch 59/99\n","----------\n","train Loss: 2.9981 Acc: 0.0526\n","val Loss: 3.0246 Acc: 0.0526\n","\n","Epoch 60/99\n","----------\n","train Loss: 3.0257 Acc: 0.0468\n","val Loss: 2.9921 Acc: 0.0526\n","\n","Epoch 61/99\n","----------\n","train Loss: 2.9993 Acc: 0.0526\n","val Loss: 2.9633 Acc: 0.0526\n","\n","Epoch 62/99\n","----------\n","train Loss: 3.0046 Acc: 0.0409\n","val Loss: 3.0284 Acc: 0.1053\n","\n","Epoch 63/99\n","----------\n","train Loss: 3.0366 Acc: 0.0526\n","val Loss: 3.0201 Acc: 0.0526\n","\n","Epoch 64/99\n","----------\n","train Loss: 3.0010 Acc: 0.0585\n","val Loss: 3.0650 Acc: 0.0000\n","\n","Epoch 65/99\n","----------\n","train Loss: 3.0008 Acc: 0.0702\n","val Loss: 3.0248 Acc: 0.0526\n","\n","Epoch 66/99\n","----------\n","train Loss: 3.0117 Acc: 0.0643\n","val Loss: 3.0296 Acc: 0.0526\n","\n","Epoch 67/99\n","----------\n","train Loss: 2.9966 Acc: 0.0526\n","val Loss: 3.0006 Acc: 0.0526\n","\n","Epoch 68/99\n","----------\n","train Loss: 3.0034 Acc: 0.0526\n","val Loss: 2.9445 Acc: 0.1053\n","\n","Epoch 69/99\n","----------\n","train Loss: 3.0136 Acc: 0.0409\n","val Loss: 2.9602 Acc: 0.0526\n","\n","Epoch 70/99\n","----------\n","train Loss: 3.0029 Acc: 0.0351\n","val Loss: 2.9958 Acc: 0.0526\n","\n","Epoch 71/99\n","----------\n","train Loss: 3.0246 Acc: 0.0468\n","val Loss: 2.9917 Acc: 0.0526\n","\n","Epoch 72/99\n","----------\n","train Loss: 3.0188 Acc: 0.0585\n","val Loss: 2.9288 Acc: 0.1053\n","\n","Epoch 73/99\n","----------\n","train Loss: 3.0233 Acc: 0.0702\n","val Loss: 2.9820 Acc: 0.0526\n","\n","Epoch 74/99\n","----------\n","train Loss: 3.0025 Acc: 0.0468\n","val Loss: 2.9918 Acc: 0.0526\n","\n","Epoch 75/99\n","----------\n","train Loss: 3.0207 Acc: 0.0585\n","val Loss: 2.9522 Acc: 0.0000\n","\n","Epoch 76/99\n","----------\n","train Loss: 3.0115 Acc: 0.0468\n","val Loss: 2.9717 Acc: 0.0526\n","\n","Epoch 77/99\n","----------\n","train Loss: 3.0136 Acc: 0.0468\n","val Loss: 3.0339 Acc: 0.1053\n","\n","Epoch 78/99\n","----------\n","train Loss: 3.0211 Acc: 0.0643\n","val Loss: 3.0491 Acc: 0.0000\n","\n","Epoch 79/99\n","----------\n","train Loss: 3.0441 Acc: 0.0468\n","val Loss: 2.9858 Acc: 0.0526\n","\n","Epoch 80/99\n","----------\n","train Loss: 3.0234 Acc: 0.0468\n","val Loss: 2.9532 Acc: 0.0526\n","\n","Epoch 81/99\n","----------\n","train Loss: 3.0119 Acc: 0.0468\n","val Loss: 3.0149 Acc: 0.0526\n","\n","Epoch 82/99\n","----------\n","train Loss: 3.0271 Acc: 0.0585\n","val Loss: 3.0422 Acc: 0.0526\n","\n","Epoch 83/99\n","----------\n","train Loss: 3.0130 Acc: 0.0526\n","val Loss: 3.0947 Acc: 0.0000\n","\n","Epoch 84/99\n","----------\n","train Loss: 3.0029 Acc: 0.0643\n","val Loss: 2.8885 Acc: 0.1053\n","\n","Epoch 85/99\n","----------\n","train Loss: 3.0195 Acc: 0.0409\n","val Loss: 3.0385 Acc: 0.0000\n","\n","Epoch 86/99\n","----------\n","train Loss: 3.0067 Acc: 0.0351\n","val Loss: 2.9582 Acc: 0.0526\n","\n","Epoch 87/99\n","----------\n","train Loss: 3.0010 Acc: 0.0643\n","val Loss: 3.0067 Acc: 0.0000\n","\n","Epoch 88/99\n","----------\n","train Loss: 3.0195 Acc: 0.0585\n","val Loss: 3.0245 Acc: 0.0000\n","\n","Epoch 89/99\n","----------\n","train Loss: 3.0106 Acc: 0.0526\n","val Loss: 2.9644 Acc: 0.0526\n","\n","Epoch 90/99\n","----------\n","train Loss: 2.9949 Acc: 0.0526\n","val Loss: 3.1030 Acc: 0.0526\n","\n","Epoch 91/99\n","----------\n","train Loss: 3.0263 Acc: 0.0643\n","val Loss: 3.0302 Acc: 0.0526\n","\n","Epoch 92/99\n","----------\n","train Loss: 3.0227 Acc: 0.0468\n","val Loss: 2.9132 Acc: 0.1053\n","\n","Epoch 93/99\n","----------\n","train Loss: 3.0074 Acc: 0.0585\n","val Loss: 3.0945 Acc: 0.0526\n","\n","Epoch 94/99\n","----------\n","train Loss: 3.0074 Acc: 0.0526\n","val Loss: 3.1094 Acc: 0.0526\n","\n","Epoch 95/99\n","----------\n","train Loss: 3.0181 Acc: 0.0468\n","val Loss: 3.0249 Acc: 0.0526\n","\n","Epoch 96/99\n","----------\n","train Loss: 3.0135 Acc: 0.0468\n","val Loss: 3.0128 Acc: 0.0526\n","\n","Epoch 97/99\n","----------\n","train Loss: 3.0029 Acc: 0.0526\n","val Loss: 2.9197 Acc: 0.0526\n","\n","Epoch 98/99\n","----------\n","train Loss: 3.0070 Acc: 0.0526\n","val Loss: 3.1562 Acc: 0.0000\n","\n","Epoch 99/99\n","----------\n","train Loss: 3.0158 Acc: 0.0468\n","val Loss: 3.0370 Acc: 0.0000\n","\n","Training complete in 20m 37s\n","Best val Acc: 0.157895\n","************************* 第 5 折 *************************\n","train: 171 val: 19\n","Epoch 0/99\n","----------\n","train Loss: 3.0291 Acc: 0.0585\n","val Loss: 2.9356 Acc: 0.0526\n","\n","Epoch 1/99\n","----------\n","train Loss: 3.0221 Acc: 0.0643\n","val Loss: 2.9908 Acc: 0.1053\n","\n","Epoch 2/99\n","----------\n","train Loss: 3.0117 Acc: 0.0468\n","val Loss: 3.0761 Acc: 0.0526\n","\n","Epoch 3/99\n","----------\n","train Loss: 3.0061 Acc: 0.0585\n","val Loss: 3.0454 Acc: 0.0526\n","\n","Epoch 4/99\n","----------\n","train Loss: 2.9965 Acc: 0.0351\n","val Loss: 3.0641 Acc: 0.0526\n","\n","Epoch 5/99\n","----------\n","train Loss: 2.9889 Acc: 0.0526\n","val Loss: 2.9424 Acc: 0.0526\n","\n","Epoch 6/99\n","----------\n","train Loss: 3.0260 Acc: 0.0585\n","val Loss: 3.0313 Acc: 0.0526\n","\n","Epoch 7/99\n","----------\n","train Loss: 3.0480 Acc: 0.0526\n","val Loss: 2.9839 Acc: 0.0526\n","\n","Epoch 8/99\n","----------\n","train Loss: 2.9919 Acc: 0.0643\n","val Loss: 2.9910 Acc: 0.0526\n","\n","Epoch 9/99\n","----------\n","train Loss: 3.0154 Acc: 0.0702\n","val Loss: 3.0088 Acc: 0.1053\n","\n","Epoch 10/99\n","----------\n","train Loss: 3.0032 Acc: 0.0468\n","val Loss: 2.9942 Acc: 0.0000\n","\n","Epoch 11/99\n","----------\n","train Loss: 3.0040 Acc: 0.0585\n","val Loss: 3.0213 Acc: 0.0000\n","\n","Epoch 12/99\n","----------\n","train Loss: 3.0131 Acc: 0.0468\n","val Loss: 2.9742 Acc: 0.0000\n","\n","Epoch 13/99\n","----------\n","train Loss: 3.0375 Acc: 0.0643\n","val Loss: 3.0058 Acc: 0.1053\n","\n","Epoch 14/99\n","----------\n","train Loss: 3.0114 Acc: 0.0643\n","val Loss: 2.9591 Acc: 0.1053\n","\n","Epoch 15/99\n","----------\n","train Loss: 3.0234 Acc: 0.0643\n","val Loss: 3.0571 Acc: 0.0526\n","\n","Epoch 16/99\n","----------\n","train Loss: 3.0027 Acc: 0.0526\n","val Loss: 3.0366 Acc: 0.0000\n","\n","Epoch 17/99\n","----------\n","train Loss: 3.0208 Acc: 0.0526\n","val Loss: 3.0326 Acc: 0.0526\n","\n","Epoch 18/99\n","----------\n","train Loss: 3.0187 Acc: 0.0643\n","val Loss: 2.9960 Acc: 0.0526\n","\n","Epoch 19/99\n","----------\n","train Loss: 3.0515 Acc: 0.0351\n","val Loss: 2.9525 Acc: 0.0526\n","\n","Epoch 20/99\n","----------\n","train Loss: 3.0076 Acc: 0.0409\n","val Loss: 2.9836 Acc: 0.0000\n","\n","Epoch 21/99\n","----------\n","train Loss: 3.0342 Acc: 0.0468\n","val Loss: 2.9965 Acc: 0.0526\n","\n","Epoch 22/99\n","----------\n","train Loss: 2.9997 Acc: 0.0468\n","val Loss: 3.0165 Acc: 0.0526\n","\n","Epoch 23/99\n","----------\n","train Loss: 2.9972 Acc: 0.0643\n","val Loss: 2.9535 Acc: 0.0526\n","\n","Epoch 24/99\n","----------\n","train Loss: 3.0048 Acc: 0.0585\n","val Loss: 3.0224 Acc: 0.0526\n","\n","Epoch 25/99\n","----------\n","train Loss: 3.0277 Acc: 0.0409\n","val Loss: 2.9753 Acc: 0.0526\n","\n","Epoch 26/99\n","----------\n","train Loss: 3.0049 Acc: 0.0351\n","val Loss: 2.9347 Acc: 0.0526\n","\n","Epoch 27/99\n","----------\n","train Loss: 3.0068 Acc: 0.0585\n","val Loss: 3.0125 Acc: 0.0000\n","\n","Epoch 28/99\n","----------\n","train Loss: 3.0126 Acc: 0.0643\n","val Loss: 2.9322 Acc: 0.1053\n","\n","Epoch 29/99\n","----------\n","train Loss: 2.9989 Acc: 0.0468\n","val Loss: 3.0025 Acc: 0.0000\n","\n","Epoch 30/99\n","----------\n","train Loss: 3.0305 Acc: 0.0409\n","val Loss: 2.9762 Acc: 0.0526\n","\n","Epoch 31/99\n","----------\n","train Loss: 2.9943 Acc: 0.0468\n","val Loss: 3.0269 Acc: 0.0526\n","\n","Epoch 32/99\n","----------\n","train Loss: 2.9954 Acc: 0.0585\n","val Loss: 2.9208 Acc: 0.0000\n","\n","Epoch 33/99\n","----------\n","train Loss: 3.0056 Acc: 0.0468\n","val Loss: 3.0134 Acc: 0.0526\n","\n","Epoch 34/99\n","----------\n","train Loss: 3.0258 Acc: 0.0585\n","val Loss: 3.0515 Acc: 0.0526\n","\n","Epoch 35/99\n","----------\n","train Loss: 3.0240 Acc: 0.0468\n","val Loss: 2.9037 Acc: 0.0526\n","\n","Epoch 36/99\n","----------\n","train Loss: 3.0071 Acc: 0.0585\n","val Loss: 2.9405 Acc: 0.0526\n","\n","Epoch 37/99\n","----------\n","train Loss: 2.9992 Acc: 0.0819\n","val Loss: 3.0440 Acc: 0.0526\n","\n","Epoch 38/99\n","----------\n","train Loss: 2.9882 Acc: 0.0643\n","val Loss: 3.0312 Acc: 0.0000\n","\n","Epoch 39/99\n","----------\n","train Loss: 3.0162 Acc: 0.0585\n","val Loss: 3.0424 Acc: 0.0526\n","\n","Epoch 40/99\n","----------\n","train Loss: 3.0114 Acc: 0.0643\n","val Loss: 3.0555 Acc: 0.0526\n","\n","Epoch 41/99\n","----------\n","train Loss: 2.9866 Acc: 0.0585\n","val Loss: 3.0014 Acc: 0.0000\n","\n","Epoch 42/99\n","----------\n","train Loss: 3.0116 Acc: 0.0526\n","val Loss: 3.0216 Acc: 0.0000\n","\n","Epoch 43/99\n","----------\n","train Loss: 3.0021 Acc: 0.0585\n","val Loss: 2.9117 Acc: 0.0526\n","\n","Epoch 44/99\n","----------\n","train Loss: 3.0014 Acc: 0.0585\n","val Loss: 3.0326 Acc: 0.0526\n","\n","Epoch 45/99\n","----------\n","train Loss: 3.0080 Acc: 0.0468\n","val Loss: 2.9548 Acc: 0.0526\n","\n","Epoch 46/99\n","----------\n","train Loss: 3.0070 Acc: 0.0585\n","val Loss: 3.0288 Acc: 0.0526\n","\n","Epoch 47/99\n","----------\n","train Loss: 2.9967 Acc: 0.0819\n","val Loss: 3.0249 Acc: 0.0000\n","\n","Epoch 48/99\n","----------\n","train Loss: 3.0511 Acc: 0.0526\n","val Loss: 3.0398 Acc: 0.0526\n","\n","Epoch 49/99\n","----------\n","train Loss: 2.9923 Acc: 0.0468\n","val Loss: 2.9340 Acc: 0.1579\n","\n","Epoch 50/99\n","----------\n","train Loss: 3.0042 Acc: 0.0526\n","val Loss: 2.9432 Acc: 0.0526\n","\n","Epoch 51/99\n","----------\n","train Loss: 2.9951 Acc: 0.0468\n","val Loss: 3.0811 Acc: 0.0526\n","\n","Epoch 52/99\n","----------\n","train Loss: 2.9979 Acc: 0.0409\n","val Loss: 2.9912 Acc: 0.1053\n","\n","Epoch 53/99\n","----------\n","train Loss: 2.9998 Acc: 0.0643\n","val Loss: 2.9692 Acc: 0.1053\n","\n","Epoch 54/99\n","----------\n","train Loss: 3.0282 Acc: 0.0409\n","val Loss: 3.0869 Acc: 0.0000\n","\n","Epoch 55/99\n","----------\n","train Loss: 3.0164 Acc: 0.0585\n","val Loss: 3.0164 Acc: 0.0526\n","\n","Epoch 56/99\n","----------\n","train Loss: 3.0019 Acc: 0.0409\n","val Loss: 3.0480 Acc: 0.0000\n","\n","Epoch 57/99\n","----------\n","train Loss: 3.0129 Acc: 0.0468\n","val Loss: 2.9646 Acc: 0.1053\n","\n","Epoch 58/99\n","----------\n","train Loss: 2.9955 Acc: 0.0643\n","val Loss: 3.0944 Acc: 0.0000\n","\n","Epoch 59/99\n","----------\n","train Loss: 3.0202 Acc: 0.0585\n","val Loss: 3.0617 Acc: 0.0526\n","\n","Epoch 60/99\n","----------\n","train Loss: 3.0099 Acc: 0.0468\n","val Loss: 2.9176 Acc: 0.1053\n","\n","Epoch 61/99\n","----------\n","train Loss: 3.0122 Acc: 0.0468\n","val Loss: 3.0649 Acc: 0.0000\n","\n","Epoch 62/99\n","----------\n","train Loss: 3.0195 Acc: 0.0526\n","val Loss: 3.0164 Acc: 0.0526\n","\n","Epoch 63/99\n","----------\n","train Loss: 3.0028 Acc: 0.0643\n","val Loss: 3.0141 Acc: 0.0526\n","\n","Epoch 64/99\n","----------\n","train Loss: 2.9848 Acc: 0.0643\n","val Loss: 3.1297 Acc: 0.0000\n","\n","Epoch 65/99\n","----------\n","train Loss: 3.0089 Acc: 0.0585\n","val Loss: 3.0397 Acc: 0.0526\n","\n","Epoch 66/99\n","----------\n","train Loss: 3.0092 Acc: 0.0526\n","val Loss: 2.9675 Acc: 0.0526\n","\n","Epoch 67/99\n","----------\n","train Loss: 3.0137 Acc: 0.0585\n","val Loss: 3.0008 Acc: 0.0000\n","\n","Epoch 68/99\n","----------\n","train Loss: 3.0132 Acc: 0.0468\n","val Loss: 3.0672 Acc: 0.0000\n","\n","Epoch 69/99\n","----------\n","train Loss: 3.0078 Acc: 0.0468\n","val Loss: 3.0038 Acc: 0.1053\n","\n","Epoch 70/99\n","----------\n","train Loss: 2.9990 Acc: 0.0409\n","val Loss: 2.9649 Acc: 0.0526\n","\n","Epoch 71/99\n","----------\n","train Loss: 3.0202 Acc: 0.0526\n","val Loss: 3.0173 Acc: 0.0526\n","\n","Epoch 72/99\n","----------\n","train Loss: 3.0191 Acc: 0.0643\n","val Loss: 2.9921 Acc: 0.0000\n","\n","Epoch 73/99\n","----------\n","train Loss: 2.9922 Acc: 0.0585\n","val Loss: 3.0559 Acc: 0.0526\n","\n","Epoch 74/99\n","----------\n","train Loss: 2.9994 Acc: 0.0468\n","val Loss: 3.0474 Acc: 0.0526\n","\n","Epoch 75/99\n","----------\n","train Loss: 3.0022 Acc: 0.0468\n","val Loss: 3.0333 Acc: 0.0526\n","\n","Epoch 76/99\n","----------\n","train Loss: 3.0231 Acc: 0.0643\n","val Loss: 2.9543 Acc: 0.0526\n","\n","Epoch 77/99\n","----------\n","train Loss: 3.0044 Acc: 0.0643\n","val Loss: 2.9906 Acc: 0.0526\n","\n","Epoch 78/99\n","----------\n","train Loss: 2.9919 Acc: 0.0468\n","val Loss: 2.9714 Acc: 0.0526\n","\n","Epoch 79/99\n","----------\n","train Loss: 2.9959 Acc: 0.0585\n","val Loss: 3.0766 Acc: 0.0526\n","\n","Epoch 80/99\n","----------\n","train Loss: 3.0171 Acc: 0.0351\n","val Loss: 3.0562 Acc: 0.0000\n","\n","Epoch 81/99\n","----------\n","train Loss: 2.9878 Acc: 0.0526\n","val Loss: 3.0223 Acc: 0.0000\n","\n","Epoch 82/99\n","----------\n","train Loss: 3.0240 Acc: 0.0526\n","val Loss: 3.0130 Acc: 0.0526\n","\n","Epoch 83/99\n","----------\n","train Loss: 2.9941 Acc: 0.0585\n","val Loss: 3.0356 Acc: 0.0526\n","\n","Epoch 84/99\n","----------\n","train Loss: 3.0299 Acc: 0.0351\n","val Loss: 3.1450 Acc: 0.0526\n","\n","Epoch 85/99\n","----------\n","train Loss: 3.0145 Acc: 0.0526\n","val Loss: 3.0703 Acc: 0.0526\n","\n","Epoch 86/99\n","----------\n","train Loss: 3.0077 Acc: 0.0526\n","val Loss: 3.0438 Acc: 0.0526\n","\n","Epoch 87/99\n","----------\n","train Loss: 2.9964 Acc: 0.0702\n","val Loss: 3.0499 Acc: 0.0000\n","\n","Epoch 88/99\n","----------\n","train Loss: 3.0124 Acc: 0.0468\n","val Loss: 2.9962 Acc: 0.0526\n","\n","Epoch 89/99\n","----------\n","train Loss: 3.0032 Acc: 0.0526\n","val Loss: 3.0222 Acc: 0.0526\n","\n","Epoch 90/99\n","----------\n","train Loss: 3.0177 Acc: 0.0468\n","val Loss: 2.9556 Acc: 0.0526\n","\n","Epoch 91/99\n","----------\n","train Loss: 3.0056 Acc: 0.0526\n","val Loss: 3.0400 Acc: 0.0526\n","\n","Epoch 92/99\n","----------\n","train Loss: 3.0025 Acc: 0.0526\n","val Loss: 3.0153 Acc: 0.0526\n","\n","Epoch 93/99\n","----------\n","train Loss: 3.0204 Acc: 0.0702\n","val Loss: 3.0182 Acc: 0.1053\n","\n","Epoch 94/99\n","----------\n","train Loss: 2.9973 Acc: 0.0643\n","val Loss: 3.0215 Acc: 0.0000\n","\n","Epoch 95/99\n","----------\n","train Loss: 3.0071 Acc: 0.0585\n","val Loss: 2.9370 Acc: 0.0526\n","\n","Epoch 96/99\n","----------\n","train Loss: 2.9889 Acc: 0.0409\n","val Loss: 3.0653 Acc: 0.0000\n","\n","Epoch 97/99\n","----------\n","train Loss: 2.9919 Acc: 0.0526\n","val Loss: 3.0569 Acc: 0.0526\n","\n","Epoch 98/99\n","----------\n","train Loss: 3.0317 Acc: 0.0409\n","val Loss: 2.9075 Acc: 0.0000\n","\n","Epoch 99/99\n","----------\n","train Loss: 2.9951 Acc: 0.0643\n","val Loss: 3.0116 Acc: 0.0526\n","\n","Training complete in 20m 38s\n","Best val Acc: 0.157895\n","************************* 第 6 折 *************************\n","train: 171 val: 19\n","Epoch 0/99\n","----------\n","train Loss: 2.9944 Acc: 0.0526\n","val Loss: 3.0619 Acc: 0.0526\n","\n","Epoch 1/99\n","----------\n","train Loss: 3.0253 Acc: 0.0468\n","val Loss: 2.9677 Acc: 0.0526\n","\n","Epoch 2/99\n","----------\n","train Loss: 3.0317 Acc: 0.0585\n","val Loss: 2.9357 Acc: 0.0526\n","\n","Epoch 3/99\n","----------\n","train Loss: 3.0065 Acc: 0.0643\n","val Loss: 2.9741 Acc: 0.0526\n","\n","Epoch 4/99\n","----------\n","train Loss: 3.0072 Acc: 0.0351\n","val Loss: 2.9027 Acc: 0.1053\n","\n","Epoch 5/99\n","----------\n","train Loss: 2.9941 Acc: 0.0526\n","val Loss: 2.9683 Acc: 0.0526\n","\n","Epoch 6/99\n","----------\n","train Loss: 3.0147 Acc: 0.0526\n","val Loss: 3.0067 Acc: 0.0526\n","\n","Epoch 7/99\n","----------\n","train Loss: 3.0177 Acc: 0.0468\n","val Loss: 2.9922 Acc: 0.1053\n","\n","Epoch 8/99\n","----------\n","train Loss: 2.9996 Acc: 0.0468\n","val Loss: 3.0054 Acc: 0.0526\n","\n","Epoch 9/99\n","----------\n","train Loss: 3.0222 Acc: 0.0409\n","val Loss: 3.0070 Acc: 0.0526\n","\n","Epoch 10/99\n","----------\n","train Loss: 3.0274 Acc: 0.0409\n","val Loss: 3.0652 Acc: 0.0526\n","\n","Epoch 11/99\n","----------\n","train Loss: 3.0088 Acc: 0.0468\n","val Loss: 2.9963 Acc: 0.0526\n","\n","Epoch 12/99\n","----------\n","train Loss: 3.0252 Acc: 0.0585\n","val Loss: 3.0348 Acc: 0.0000\n","\n","Epoch 13/99\n","----------\n","train Loss: 3.0117 Acc: 0.0643\n","val Loss: 2.9773 Acc: 0.1053\n","\n","Epoch 14/99\n","----------\n","train Loss: 2.9814 Acc: 0.0468\n","val Loss: 2.9558 Acc: 0.0526\n","\n","Epoch 15/99\n","----------\n","train Loss: 3.0268 Acc: 0.0468\n","val Loss: 3.0071 Acc: 0.1053\n","\n","Epoch 16/99\n","----------\n","train Loss: 3.0018 Acc: 0.0585\n","val Loss: 2.9951 Acc: 0.0526\n","\n","Epoch 17/99\n","----------\n","train Loss: 3.0141 Acc: 0.0468\n","val Loss: 3.0559 Acc: 0.1053\n","\n","Epoch 18/99\n","----------\n","train Loss: 3.0223 Acc: 0.0526\n","val Loss: 3.0520 Acc: 0.0526\n","\n","Epoch 19/99\n","----------\n","train Loss: 3.0141 Acc: 0.0468\n","val Loss: 3.0215 Acc: 0.0526\n","\n","Epoch 20/99\n","----------\n","train Loss: 3.0144 Acc: 0.0585\n","val Loss: 3.0077 Acc: 0.0526\n","\n","Epoch 21/99\n","----------\n","train Loss: 3.0038 Acc: 0.0468\n","val Loss: 2.9532 Acc: 0.0526\n","\n","Epoch 22/99\n","----------\n","train Loss: 3.0219 Acc: 0.0351\n","val Loss: 3.0879 Acc: 0.0526\n","\n","Epoch 23/99\n","----------\n","train Loss: 3.0376 Acc: 0.0585\n","val Loss: 2.9803 Acc: 0.0526\n","\n","Epoch 24/99\n","----------\n","train Loss: 2.9921 Acc: 0.0468\n","val Loss: 3.0669 Acc: 0.0526\n","\n","Epoch 25/99\n","----------\n","train Loss: 3.0362 Acc: 0.0643\n","val Loss: 3.0965 Acc: 0.0526\n","\n","Epoch 26/99\n","----------\n","train Loss: 2.9798 Acc: 0.0468\n","val Loss: 2.9965 Acc: 0.1053\n","\n","Epoch 27/99\n","----------\n","train Loss: 3.0320 Acc: 0.0409\n","val Loss: 3.0129 Acc: 0.1053\n","\n","Epoch 28/99\n","----------\n","train Loss: 3.0070 Acc: 0.0409\n","val Loss: 2.9726 Acc: 0.0000\n","\n","Epoch 29/99\n","----------\n","train Loss: 3.0129 Acc: 0.0760\n","val Loss: 3.0650 Acc: 0.1053\n","\n","Epoch 30/99\n","----------\n","train Loss: 2.9967 Acc: 0.0526\n","val Loss: 3.0468 Acc: 0.0526\n","\n","Epoch 31/99\n","----------\n","train Loss: 3.0246 Acc: 0.0643\n","val Loss: 3.0831 Acc: 0.0526\n","\n","Epoch 32/99\n","----------\n","train Loss: 3.0280 Acc: 0.0526\n","val Loss: 3.0191 Acc: 0.0526\n","\n","Epoch 33/99\n","----------\n","train Loss: 2.9904 Acc: 0.0643\n","val Loss: 2.9631 Acc: 0.0526\n","\n","Epoch 34/99\n","----------\n","train Loss: 3.0302 Acc: 0.0409\n","val Loss: 2.9501 Acc: 0.0000\n","\n","Epoch 35/99\n","----------\n","train Loss: 3.0380 Acc: 0.0526\n","val Loss: 3.0044 Acc: 0.0000\n","\n","Epoch 36/99\n","----------\n","train Loss: 3.0078 Acc: 0.0468\n","val Loss: 3.0805 Acc: 0.0526\n","\n","Epoch 37/99\n","----------\n","train Loss: 3.0059 Acc: 0.0585\n","val Loss: 2.9743 Acc: 0.0000\n","\n","Epoch 38/99\n","----------\n","train Loss: 3.0331 Acc: 0.0526\n","val Loss: 3.0071 Acc: 0.0526\n","\n","Epoch 39/99\n","----------\n","train Loss: 3.0039 Acc: 0.0643\n","val Loss: 3.0415 Acc: 0.1053\n","\n","Epoch 40/99\n","----------\n","train Loss: 3.0378 Acc: 0.0643\n","val Loss: 3.0324 Acc: 0.0526\n","\n","Epoch 41/99\n","----------\n","train Loss: 3.0324 Acc: 0.0351\n","val Loss: 3.0625 Acc: 0.0526\n","\n","Epoch 42/99\n","----------\n","train Loss: 3.0293 Acc: 0.0643\n","val Loss: 2.9671 Acc: 0.1053\n","\n","Epoch 43/99\n","----------\n","train Loss: 3.0282 Acc: 0.0409\n","val Loss: 2.9750 Acc: 0.1579\n","\n","Epoch 44/99\n","----------\n","train Loss: 2.9963 Acc: 0.0643\n","val Loss: 3.0431 Acc: 0.0526\n","\n","Epoch 45/99\n","----------\n","train Loss: 2.9868 Acc: 0.0526\n","val Loss: 3.0372 Acc: 0.0526\n","\n","Epoch 46/99\n","----------\n","train Loss: 3.0058 Acc: 0.0351\n","val Loss: 3.1073 Acc: 0.0526\n","\n","Epoch 47/99\n","----------\n","train Loss: 3.0144 Acc: 0.0526\n","val Loss: 2.9725 Acc: 0.2105\n","\n","Epoch 48/99\n","----------\n","train Loss: 3.0104 Acc: 0.0526\n","val Loss: 2.9138 Acc: 0.1053\n","\n","Epoch 49/99\n","----------\n","train Loss: 3.0090 Acc: 0.0526\n","val Loss: 2.9744 Acc: 0.0526\n","\n","Epoch 50/99\n","----------\n","train Loss: 3.0166 Acc: 0.0468\n","val Loss: 2.9263 Acc: 0.0526\n","\n","Epoch 51/99\n","----------\n","train Loss: 3.0057 Acc: 0.0643\n","val Loss: 3.0547 Acc: 0.0000\n","\n","Epoch 52/99\n","----------\n","train Loss: 2.9997 Acc: 0.0468\n","val Loss: 3.0404 Acc: 0.0526\n","\n","Epoch 53/99\n","----------\n","train Loss: 3.0094 Acc: 0.0585\n","val Loss: 3.0002 Acc: 0.0526\n","\n","Epoch 54/99\n","----------\n","train Loss: 3.0109 Acc: 0.0526\n","val Loss: 3.0074 Acc: 0.1053\n","\n","Epoch 55/99\n","----------\n","train Loss: 2.9966 Acc: 0.0526\n","val Loss: 2.9507 Acc: 0.0526\n","\n","Epoch 56/99\n","----------\n","train Loss: 3.0087 Acc: 0.0468\n","val Loss: 3.0189 Acc: 0.1053\n","\n","Epoch 57/99\n","----------\n","train Loss: 3.0207 Acc: 0.0643\n","val Loss: 2.9913 Acc: 0.0526\n","\n","Epoch 58/99\n","----------\n","train Loss: 3.0311 Acc: 0.0526\n","val Loss: 3.0174 Acc: 0.0526\n","\n","Epoch 59/99\n","----------\n","train Loss: 2.9978 Acc: 0.0468\n","val Loss: 2.9297 Acc: 0.0526\n","\n","Epoch 60/99\n","----------\n","train Loss: 3.0070 Acc: 0.0409\n","val Loss: 3.0123 Acc: 0.1053\n","\n","Epoch 61/99\n","----------\n","train Loss: 3.0015 Acc: 0.0702\n","val Loss: 2.9647 Acc: 0.0526\n","\n","Epoch 62/99\n","----------\n","train Loss: 3.0335 Acc: 0.0526\n","val Loss: 2.9926 Acc: 0.0000\n","\n","Epoch 63/99\n","----------\n","train Loss: 3.0014 Acc: 0.0585\n","val Loss: 3.0836 Acc: 0.0526\n","\n","Epoch 64/99\n","----------\n","train Loss: 3.0350 Acc: 0.0526\n","val Loss: 3.0260 Acc: 0.0526\n","\n","Epoch 65/99\n","----------\n","train Loss: 3.0019 Acc: 0.0526\n","val Loss: 2.9475 Acc: 0.0526\n","\n","Epoch 66/99\n","----------\n","train Loss: 3.0135 Acc: 0.0585\n","val Loss: 3.0404 Acc: 0.0526\n","\n","Epoch 67/99\n","----------\n","train Loss: 3.0071 Acc: 0.0643\n","val Loss: 2.9883 Acc: 0.1053\n","\n","Epoch 68/99\n","----------\n","train Loss: 3.0019 Acc: 0.0468\n","val Loss: 3.0034 Acc: 0.1053\n","\n","Epoch 69/99\n","----------\n","train Loss: 3.0154 Acc: 0.0585\n","val Loss: 2.9475 Acc: 0.0526\n","\n","Epoch 70/99\n","----------\n","train Loss: 3.0151 Acc: 0.0526\n","val Loss: 2.8963 Acc: 0.2105\n","\n","Epoch 71/99\n","----------\n","train Loss: 3.0400 Acc: 0.0409\n","val Loss: 3.0638 Acc: 0.0526\n","\n","Epoch 72/99\n","----------\n","train Loss: 2.9957 Acc: 0.0468\n","val Loss: 2.9952 Acc: 0.1053\n","\n","Epoch 73/99\n","----------\n","train Loss: 3.0094 Acc: 0.0526\n","val Loss: 3.0176 Acc: 0.1053\n","\n","Epoch 74/99\n","----------\n","train Loss: 2.9814 Acc: 0.0643\n","val Loss: 2.9640 Acc: 0.0000\n","\n","Epoch 75/99\n","----------\n","train Loss: 3.0120 Acc: 0.0585\n","val Loss: 3.0642 Acc: 0.0000\n","\n","Epoch 76/99\n","----------\n","train Loss: 3.0021 Acc: 0.0409\n","val Loss: 2.8871 Acc: 0.0526\n","\n","Epoch 77/99\n","----------\n","train Loss: 3.0282 Acc: 0.0526\n","val Loss: 3.1167 Acc: 0.0000\n","\n","Epoch 78/99\n","----------\n","train Loss: 3.0142 Acc: 0.0409\n","val Loss: 2.9682 Acc: 0.0526\n","\n","Epoch 79/99\n","----------\n","train Loss: 2.9984 Acc: 0.0526\n","val Loss: 2.9808 Acc: 0.1053\n","\n","Epoch 80/99\n","----------\n","train Loss: 3.0168 Acc: 0.0351\n","val Loss: 2.9980 Acc: 0.1053\n","\n","Epoch 81/99\n","----------\n","train Loss: 3.0147 Acc: 0.0526\n","val Loss: 2.9584 Acc: 0.1053\n","\n","Epoch 82/99\n","----------\n","train Loss: 3.0217 Acc: 0.0351\n","val Loss: 2.9810 Acc: 0.1053\n","\n","Epoch 83/99\n","----------\n","train Loss: 3.0234 Acc: 0.0526\n","val Loss: 3.0279 Acc: 0.0526\n","\n","Epoch 84/99\n","----------\n","train Loss: 3.0243 Acc: 0.0468\n","val Loss: 3.0071 Acc: 0.0526\n","\n","Epoch 85/99\n","----------\n","train Loss: 3.0209 Acc: 0.0585\n","val Loss: 3.0629 Acc: 0.0526\n","\n","Epoch 86/99\n","----------\n","train Loss: 2.9945 Acc: 0.0526\n","val Loss: 2.9749 Acc: 0.0526\n","\n","Epoch 87/99\n","----------\n","train Loss: 2.9921 Acc: 0.0526\n","val Loss: 3.0133 Acc: 0.0526\n","\n","Epoch 88/99\n","----------\n","train Loss: 3.0113 Acc: 0.0585\n","val Loss: 3.0511 Acc: 0.0526\n","\n","Epoch 89/99\n","----------\n","train Loss: 2.9907 Acc: 0.0468\n","val Loss: 3.0143 Acc: 0.1053\n","\n","Epoch 90/99\n","----------\n","train Loss: 3.0022 Acc: 0.0468\n","val Loss: 3.0210 Acc: 0.0526\n","\n","Epoch 91/99\n","----------\n","train Loss: 3.0217 Acc: 0.0409\n","val Loss: 2.9966 Acc: 0.0526\n","\n","Epoch 92/99\n","----------\n","train Loss: 2.9977 Acc: 0.0585\n","val Loss: 3.0135 Acc: 0.0526\n","\n","Epoch 93/99\n","----------\n","train Loss: 3.0232 Acc: 0.0760\n","val Loss: 3.0067 Acc: 0.0526\n","\n","Epoch 94/99\n","----------\n","train Loss: 3.0400 Acc: 0.0526\n","val Loss: 3.0644 Acc: 0.0000\n","\n","Epoch 95/99\n","----------\n","train Loss: 3.0118 Acc: 0.0643\n","val Loss: 2.9677 Acc: 0.0526\n","\n","Epoch 96/99\n","----------\n","train Loss: 3.0080 Acc: 0.0643\n","val Loss: 2.9808 Acc: 0.0526\n","\n","Epoch 97/99\n","----------\n","train Loss: 3.0126 Acc: 0.0292\n","val Loss: 3.0558 Acc: 0.0526\n","\n","Epoch 98/99\n","----------\n","train Loss: 3.0187 Acc: 0.0585\n","val Loss: 3.0197 Acc: 0.1053\n","\n","Epoch 99/99\n","----------\n","train Loss: 3.0169 Acc: 0.0585\n","val Loss: 3.0226 Acc: 0.0000\n","\n","Training complete in 20m 33s\n","Best val Acc: 0.210526\n","************************* 第 7 折 *************************\n","train: 171 val: 19\n","Epoch 0/99\n","----------\n","train Loss: 3.0190 Acc: 0.0468\n","val Loss: 3.0338 Acc: 0.0526\n","\n","Epoch 1/99\n","----------\n","train Loss: 3.0179 Acc: 0.0468\n","val Loss: 2.9768 Acc: 0.1053\n","\n","Epoch 2/99\n","----------\n","train Loss: 3.0111 Acc: 0.0468\n","val Loss: 2.9972 Acc: 0.0526\n","\n","Epoch 3/99\n","----------\n","train Loss: 3.0164 Acc: 0.0526\n","val Loss: 2.9652 Acc: 0.0000\n","\n","Epoch 4/99\n","----------\n","train Loss: 3.0373 Acc: 0.0585\n","val Loss: 2.9826 Acc: 0.0526\n","\n","Epoch 5/99\n","----------\n","train Loss: 3.0199 Acc: 0.0468\n","val Loss: 3.0623 Acc: 0.0526\n","\n","Epoch 6/99\n","----------\n","train Loss: 3.0082 Acc: 0.0468\n","val Loss: 2.9533 Acc: 0.1579\n","\n","Epoch 7/99\n","----------\n","train Loss: 3.0089 Acc: 0.0526\n","val Loss: 2.9934 Acc: 0.0000\n","\n","Epoch 8/99\n","----------\n","train Loss: 3.0327 Acc: 0.0585\n","val Loss: 3.0063 Acc: 0.0000\n","\n","Epoch 9/99\n","----------\n","train Loss: 3.0170 Acc: 0.0351\n","val Loss: 3.1412 Acc: 0.0526\n","\n","Epoch 10/99\n","----------\n","train Loss: 3.0148 Acc: 0.0643\n","val Loss: 3.0489 Acc: 0.0526\n","\n","Epoch 11/99\n","----------\n","train Loss: 3.0160 Acc: 0.0585\n","val Loss: 3.1551 Acc: 0.0000\n","\n","Epoch 12/99\n","----------\n","train Loss: 2.9953 Acc: 0.0526\n","val Loss: 2.9981 Acc: 0.0000\n","\n","Epoch 13/99\n","----------\n","train Loss: 3.0098 Acc: 0.0526\n","val Loss: 3.0508 Acc: 0.0526\n","\n","Epoch 14/99\n","----------\n","train Loss: 3.0277 Acc: 0.0351\n","val Loss: 3.0181 Acc: 0.1053\n","\n","Epoch 15/99\n","----------\n","train Loss: 3.0077 Acc: 0.0585\n","val Loss: 3.0700 Acc: 0.0526\n","\n","Epoch 16/99\n","----------\n","train Loss: 3.0039 Acc: 0.0468\n","val Loss: 3.0383 Acc: 0.0526\n","\n","Epoch 17/99\n","----------\n","train Loss: 3.0063 Acc: 0.0643\n","val Loss: 2.9738 Acc: 0.0526\n","\n","Epoch 18/99\n","----------\n","train Loss: 3.0187 Acc: 0.0292\n","val Loss: 2.9292 Acc: 0.0526\n","\n","Epoch 19/99\n","----------\n","train Loss: 3.0124 Acc: 0.0585\n","val Loss: 3.0202 Acc: 0.0526\n","\n","Epoch 20/99\n","----------\n","train Loss: 3.0077 Acc: 0.0585\n","val Loss: 3.0091 Acc: 0.0526\n","\n","Epoch 21/99\n","----------\n","train Loss: 3.0108 Acc: 0.0468\n","val Loss: 3.0588 Acc: 0.0526\n","\n","Epoch 22/99\n","----------\n","train Loss: 3.0210 Acc: 0.0819\n","val Loss: 2.9531 Acc: 0.0526\n","\n","Epoch 23/99\n","----------\n","train Loss: 3.0052 Acc: 0.0526\n","val Loss: 2.9917 Acc: 0.0526\n","\n","Epoch 24/99\n","----------\n","train Loss: 3.0039 Acc: 0.0468\n","val Loss: 2.9405 Acc: 0.1579\n","\n","Epoch 25/99\n","----------\n","train Loss: 3.0290 Acc: 0.0468\n","val Loss: 2.9799 Acc: 0.1053\n","\n","Epoch 26/99\n","----------\n","train Loss: 3.0008 Acc: 0.0409\n","val Loss: 2.9952 Acc: 0.0526\n","\n","Epoch 27/99\n","----------\n","train Loss: 3.0198 Acc: 0.0702\n","val Loss: 2.9744 Acc: 0.0526\n","\n","Epoch 28/99\n","----------\n","train Loss: 3.0130 Acc: 0.0526\n","val Loss: 2.9756 Acc: 0.1053\n","\n","Epoch 29/99\n","----------\n","train Loss: 3.0030 Acc: 0.0585\n","val Loss: 2.9546 Acc: 0.0526\n","\n","Epoch 30/99\n","----------\n","train Loss: 3.0273 Acc: 0.0526\n","val Loss: 2.9983 Acc: 0.1053\n","\n","Epoch 31/99\n","----------\n","train Loss: 3.0124 Acc: 0.0409\n","val Loss: 3.0641 Acc: 0.0526\n","\n","Epoch 32/99\n","----------\n","train Loss: 3.0217 Acc: 0.0643\n","val Loss: 3.0094 Acc: 0.0526\n","\n","Epoch 33/99\n","----------\n","train Loss: 3.0064 Acc: 0.0526\n","val Loss: 3.0219 Acc: 0.0526\n","\n","Epoch 34/99\n","----------\n","train Loss: 3.0133 Acc: 0.0585\n","val Loss: 3.0182 Acc: 0.0526\n","\n","Epoch 35/99\n","----------\n","train Loss: 3.0101 Acc: 0.0351\n","val Loss: 3.0409 Acc: 0.0526\n","\n","Epoch 36/99\n","----------\n","train Loss: 3.0221 Acc: 0.0468\n","val Loss: 2.9750 Acc: 0.0526\n","\n","Epoch 37/99\n","----------\n","train Loss: 3.0036 Acc: 0.0409\n","val Loss: 3.0074 Acc: 0.0526\n","\n","Epoch 38/99\n","----------\n","train Loss: 3.0258 Acc: 0.0526\n","val Loss: 3.0678 Acc: 0.0526\n","\n","Epoch 39/99\n","----------\n","train Loss: 2.9891 Acc: 0.0585\n","val Loss: 3.0089 Acc: 0.0526\n","\n","Epoch 40/99\n","----------\n","train Loss: 3.0241 Acc: 0.0526\n","val Loss: 2.9435 Acc: 0.1579\n","\n","Epoch 41/99\n","----------\n","train Loss: 3.0483 Acc: 0.0643\n","val Loss: 3.0105 Acc: 0.1053\n","\n","Epoch 42/99\n","----------\n","train Loss: 3.0105 Acc: 0.0643\n","val Loss: 3.0075 Acc: 0.0000\n","\n","Epoch 43/99\n","----------\n","train Loss: 3.0033 Acc: 0.0468\n","val Loss: 3.0460 Acc: 0.0000\n","\n","Epoch 44/99\n","----------\n","train Loss: 3.0179 Acc: 0.0526\n","val Loss: 2.9726 Acc: 0.0526\n","\n","Epoch 45/99\n","----------\n","train Loss: 3.0390 Acc: 0.0409\n","val Loss: 2.9518 Acc: 0.0526\n","\n","Epoch 46/99\n","----------\n","train Loss: 2.9893 Acc: 0.0643\n","val Loss: 2.9612 Acc: 0.0526\n","\n","Epoch 47/99\n","----------\n","train Loss: 3.0145 Acc: 0.0643\n","val Loss: 2.9719 Acc: 0.1053\n","\n","Epoch 48/99\n","----------\n","train Loss: 3.0379 Acc: 0.0643\n","val Loss: 3.0349 Acc: 0.0526\n","\n","Epoch 49/99\n","----------\n","train Loss: 2.9982 Acc: 0.0526\n","val Loss: 3.0364 Acc: 0.0000\n","\n","Epoch 50/99\n","----------\n","train Loss: 3.0066 Acc: 0.0585\n","val Loss: 2.9305 Acc: 0.0526\n","\n","Epoch 51/99\n","----------\n","train Loss: 3.0229 Acc: 0.0351\n","val Loss: 3.0594 Acc: 0.0526\n","\n","Epoch 52/99\n","----------\n","train Loss: 3.0250 Acc: 0.0468\n","val Loss: 3.0499 Acc: 0.0526\n","\n","Epoch 53/99\n","----------\n","train Loss: 2.9918 Acc: 0.0702\n","val Loss: 3.0379 Acc: 0.0526\n","\n","Epoch 54/99\n","----------\n","train Loss: 3.0279 Acc: 0.0468\n","val Loss: 3.0270 Acc: 0.0000\n","\n","Epoch 55/99\n","----------\n","train Loss: 3.0177 Acc: 0.0526\n","val Loss: 2.9540 Acc: 0.0000\n","\n","Epoch 56/99\n","----------\n","train Loss: 3.0107 Acc: 0.0585\n","val Loss: 3.0351 Acc: 0.0526\n","\n","Epoch 57/99\n","----------\n","train Loss: 3.0007 Acc: 0.0585\n","val Loss: 3.0044 Acc: 0.0526\n","\n","Epoch 58/99\n","----------\n","train Loss: 2.9977 Acc: 0.0585\n","val Loss: 2.9837 Acc: 0.0000\n","\n","Epoch 59/99\n","----------\n","train Loss: 3.0192 Acc: 0.0526\n","val Loss: 3.0232 Acc: 0.0526\n","\n","Epoch 60/99\n","----------\n","train Loss: 3.0300 Acc: 0.0526\n","val Loss: 3.0068 Acc: 0.0000\n","\n","Epoch 61/99\n","----------\n","train Loss: 3.0059 Acc: 0.0409\n","val Loss: 2.9550 Acc: 0.0526\n","\n","Epoch 62/99\n","----------\n","train Loss: 3.0244 Acc: 0.0351\n","val Loss: 2.9983 Acc: 0.0526\n","\n","Epoch 63/99\n","----------\n","train Loss: 3.0130 Acc: 0.0643\n","val Loss: 3.0142 Acc: 0.0526\n","\n","Epoch 64/99\n","----------\n","train Loss: 3.0002 Acc: 0.0468\n","val Loss: 3.0145 Acc: 0.0000\n","\n","Epoch 65/99\n","----------\n","train Loss: 3.0114 Acc: 0.0468\n","val Loss: 2.9589 Acc: 0.0526\n","\n","Epoch 66/99\n","----------\n","train Loss: 3.0130 Acc: 0.0643\n","val Loss: 2.9889 Acc: 0.0526\n","\n","Epoch 67/99\n","----------\n","train Loss: 3.0150 Acc: 0.0468\n","val Loss: 3.0416 Acc: 0.1053\n","\n","Epoch 68/99\n","----------\n","train Loss: 3.0339 Acc: 0.0468\n","val Loss: 3.0588 Acc: 0.0526\n","\n","Epoch 69/99\n","----------\n","train Loss: 3.0114 Acc: 0.0585\n","val Loss: 3.1114 Acc: 0.0526\n","\n","Epoch 70/99\n","----------\n","train Loss: 3.0197 Acc: 0.0351\n","val Loss: 3.0305 Acc: 0.0526\n","\n","Epoch 71/99\n","----------\n","train Loss: 3.0182 Acc: 0.0468\n","val Loss: 3.0275 Acc: 0.0526\n","\n","Epoch 72/99\n","----------\n","train Loss: 3.0097 Acc: 0.0526\n","val Loss: 2.9720 Acc: 0.0000\n","\n","Epoch 73/99\n","----------\n","train Loss: 2.9952 Acc: 0.0760\n","val Loss: 2.9739 Acc: 0.0526\n","\n","Epoch 74/99\n","----------\n","train Loss: 2.9988 Acc: 0.0351\n","val Loss: 2.9893 Acc: 0.0526\n","\n","Epoch 75/99\n","----------\n","train Loss: 3.0169 Acc: 0.0526\n","val Loss: 3.0471 Acc: 0.0526\n","\n","Epoch 76/99\n","----------\n","train Loss: 2.9938 Acc: 0.0585\n","val Loss: 2.9614 Acc: 0.0526\n","\n","Epoch 77/99\n","----------\n","train Loss: 3.0113 Acc: 0.0643\n","val Loss: 3.0149 Acc: 0.0526\n","\n","Epoch 78/99\n","----------\n","train Loss: 3.0149 Acc: 0.0468\n","val Loss: 3.0366 Acc: 0.0000\n","\n","Epoch 79/99\n","----------\n","train Loss: 3.0204 Acc: 0.0468\n","val Loss: 3.0512 Acc: 0.0000\n","\n","Epoch 80/99\n","----------\n","train Loss: 2.9992 Acc: 0.0643\n","val Loss: 3.0650 Acc: 0.0526\n","\n","Epoch 81/99\n","----------\n","train Loss: 3.0435 Acc: 0.0351\n","val Loss: 2.9899 Acc: 0.0526\n","\n","Epoch 82/99\n","----------\n","train Loss: 3.0240 Acc: 0.0409\n","val Loss: 3.0422 Acc: 0.1053\n","\n","Epoch 83/99\n","----------\n","train Loss: 3.0358 Acc: 0.0351\n","val Loss: 3.0200 Acc: 0.0526\n","\n","Epoch 84/99\n","----------\n","train Loss: 3.0360 Acc: 0.0468\n","val Loss: 3.0729 Acc: 0.0526\n","\n","Epoch 85/99\n","----------\n","train Loss: 3.0093 Acc: 0.0468\n","val Loss: 2.9924 Acc: 0.0000\n","\n","Epoch 86/99\n","----------\n","train Loss: 3.0331 Acc: 0.0585\n","val Loss: 3.0091 Acc: 0.0526\n","\n","Epoch 87/99\n","----------\n","train Loss: 3.0281 Acc: 0.0526\n","val Loss: 2.9508 Acc: 0.0526\n","\n","Epoch 88/99\n","----------\n","train Loss: 3.0122 Acc: 0.0526\n","val Loss: 3.0334 Acc: 0.0000\n","\n","Epoch 89/99\n","----------\n","train Loss: 3.0317 Acc: 0.0468\n","val Loss: 2.9346 Acc: 0.0000\n","\n","Epoch 90/99\n","----------\n","train Loss: 3.0115 Acc: 0.0409\n","val Loss: 3.0072 Acc: 0.0526\n","\n","Epoch 91/99\n","----------\n","train Loss: 3.0222 Acc: 0.0526\n","val Loss: 3.0414 Acc: 0.0000\n","\n","Epoch 92/99\n","----------\n","train Loss: 3.0100 Acc: 0.0585\n","val Loss: 3.0199 Acc: 0.0000\n","\n","Epoch 93/99\n","----------\n","train Loss: 3.0302 Acc: 0.0351\n","val Loss: 3.0375 Acc: 0.0000\n","\n","Epoch 94/99\n","----------\n","train Loss: 3.0176 Acc: 0.0409\n","val Loss: 3.0330 Acc: 0.0526\n","\n","Epoch 95/99\n","----------\n","train Loss: 3.0138 Acc: 0.0468\n","val Loss: 2.9760 Acc: 0.1053\n","\n","Epoch 96/99\n","----------\n","train Loss: 3.0400 Acc: 0.0526\n","val Loss: 2.9805 Acc: 0.0526\n","\n","Epoch 97/99\n","----------\n","train Loss: 3.0005 Acc: 0.0292\n","val Loss: 3.0592 Acc: 0.0000\n","\n","Epoch 98/99\n","----------\n","train Loss: 3.0224 Acc: 0.0351\n","val Loss: 3.0516 Acc: 0.1053\n","\n","Epoch 99/99\n","----------\n","train Loss: 3.0097 Acc: 0.0585\n","val Loss: 3.0255 Acc: 0.0526\n","\n","Training complete in 20m 25s\n","Best val Acc: 0.157895\n","************************* 第 8 折 *************************\n","train: 171 val: 19\n","Epoch 0/99\n","----------\n","train Loss: 2.9960 Acc: 0.0526\n","val Loss: 2.9959 Acc: 0.0526\n","\n","Epoch 1/99\n","----------\n","train Loss: 3.0092 Acc: 0.0585\n","val Loss: 2.9110 Acc: 0.1579\n","\n","Epoch 2/99\n","----------\n","train Loss: 3.0286 Acc: 0.0585\n","val Loss: 2.9890 Acc: 0.0526\n","\n","Epoch 3/99\n","----------\n","train Loss: 3.0055 Acc: 0.0351\n","val Loss: 3.0655 Acc: 0.0526\n","\n","Epoch 4/99\n","----------\n","train Loss: 3.0109 Acc: 0.0526\n","val Loss: 2.9905 Acc: 0.1053\n","\n","Epoch 5/99\n","----------\n","train Loss: 3.0371 Acc: 0.0292\n","val Loss: 3.0031 Acc: 0.0000\n","\n","Epoch 6/99\n","----------\n","train Loss: 3.0051 Acc: 0.0468\n","val Loss: 3.0941 Acc: 0.0526\n","\n","Epoch 7/99\n","----------\n","train Loss: 3.0318 Acc: 0.0526\n","val Loss: 2.9454 Acc: 0.0000\n","\n","Epoch 8/99\n","----------\n","train Loss: 3.0288 Acc: 0.0468\n","val Loss: 2.9643 Acc: 0.0000\n","\n","Epoch 9/99\n","----------\n","train Loss: 3.0093 Acc: 0.0409\n","val Loss: 2.9775 Acc: 0.0000\n","\n","Epoch 10/99\n","----------\n","train Loss: 3.0136 Acc: 0.0526\n","val Loss: 3.0083 Acc: 0.0526\n","\n","Epoch 11/99\n","----------\n","train Loss: 3.0171 Acc: 0.0409\n","val Loss: 3.0409 Acc: 0.0526\n","\n","Epoch 12/99\n","----------\n","train Loss: 3.0131 Acc: 0.0409\n","val Loss: 2.9743 Acc: 0.0526\n","\n","Epoch 13/99\n","----------\n","train Loss: 3.0068 Acc: 0.0468\n","val Loss: 3.0397 Acc: 0.0526\n","\n","Epoch 14/99\n","----------\n","train Loss: 2.9892 Acc: 0.0409\n","val Loss: 3.0046 Acc: 0.0526\n","\n","Epoch 15/99\n","----------\n","train Loss: 3.0007 Acc: 0.0409\n","val Loss: 3.0966 Acc: 0.0526\n","\n","Epoch 16/99\n","----------\n","train Loss: 2.9953 Acc: 0.0526\n","val Loss: 3.0498 Acc: 0.0526\n","\n","Epoch 17/99\n","----------\n","train Loss: 3.0307 Acc: 0.0409\n","val Loss: 2.9748 Acc: 0.1053\n","\n","Epoch 18/99\n","----------\n","train Loss: 3.0088 Acc: 0.0526\n","val Loss: 3.0142 Acc: 0.0526\n","\n","Epoch 19/99\n","----------\n","train Loss: 3.0020 Acc: 0.0526\n","val Loss: 2.9511 Acc: 0.1053\n","\n","Epoch 20/99\n","----------\n","train Loss: 3.0198 Acc: 0.0526\n","val Loss: 3.0089 Acc: 0.0526\n","\n","Epoch 21/99\n","----------\n","train Loss: 2.9931 Acc: 0.0409\n","val Loss: 2.9753 Acc: 0.0526\n","\n","Epoch 22/99\n","----------\n","train Loss: 3.0320 Acc: 0.0585\n","val Loss: 3.0296 Acc: 0.1053\n","\n","Epoch 23/99\n","----------\n","train Loss: 3.0153 Acc: 0.0468\n","val Loss: 2.9859 Acc: 0.0526\n","\n","Epoch 24/99\n","----------\n","train Loss: 3.0472 Acc: 0.0526\n","val Loss: 3.0092 Acc: 0.0526\n","\n","Epoch 25/99\n","----------\n","train Loss: 3.0251 Acc: 0.0643\n","val Loss: 3.0426 Acc: 0.1053\n","\n","Epoch 26/99\n","----------\n","train Loss: 3.0148 Acc: 0.0468\n","val Loss: 2.9765 Acc: 0.0000\n","\n","Epoch 27/99\n","----------\n","train Loss: 3.0320 Acc: 0.0526\n","val Loss: 3.0354 Acc: 0.0526\n","\n","Epoch 28/99\n","----------\n","train Loss: 3.0010 Acc: 0.0585\n","val Loss: 3.0876 Acc: 0.0526\n","\n","Epoch 29/99\n","----------\n","train Loss: 3.0198 Acc: 0.0585\n","val Loss: 2.9059 Acc: 0.0526\n","\n","Epoch 30/99\n","----------\n","train Loss: 3.0386 Acc: 0.0526\n","val Loss: 3.0879 Acc: 0.0526\n","\n","Epoch 31/99\n","----------\n","train Loss: 3.0033 Acc: 0.0585\n","val Loss: 3.0749 Acc: 0.0526\n","\n","Epoch 32/99\n","----------\n","train Loss: 3.0243 Acc: 0.0526\n","val Loss: 2.9842 Acc: 0.1053\n","\n","Epoch 33/99\n","----------\n","train Loss: 3.0124 Acc: 0.0409\n","val Loss: 3.0839 Acc: 0.0526\n","\n","Epoch 34/99\n","----------\n","train Loss: 2.9928 Acc: 0.0702\n","val Loss: 2.9242 Acc: 0.1053\n","\n","Epoch 35/99\n","----------\n","train Loss: 3.0094 Acc: 0.0643\n","val Loss: 2.9747 Acc: 0.0526\n","\n","Epoch 36/99\n","----------\n","train Loss: 3.0309 Acc: 0.0468\n","val Loss: 3.0085 Acc: 0.1053\n","\n","Epoch 37/99\n","----------\n","train Loss: 3.0148 Acc: 0.0643\n","val Loss: 2.9600 Acc: 0.1053\n","\n","Epoch 38/99\n","----------\n","train Loss: 3.0272 Acc: 0.0409\n","val Loss: 2.9807 Acc: 0.0000\n","\n","Epoch 39/99\n","----------\n","train Loss: 3.0018 Acc: 0.0526\n","val Loss: 3.0082 Acc: 0.0526\n","\n","Epoch 40/99\n","----------\n","train Loss: 2.9906 Acc: 0.0526\n","val Loss: 3.0048 Acc: 0.0526\n","\n","Epoch 41/99\n","----------\n","train Loss: 2.9883 Acc: 0.0468\n","val Loss: 3.0210 Acc: 0.0000\n","\n","Epoch 42/99\n","----------\n","train Loss: 3.0006 Acc: 0.0585\n","val Loss: 2.9757 Acc: 0.0526\n","\n","Epoch 43/99\n","----------\n","train Loss: 3.0174 Acc: 0.0468\n","val Loss: 3.0924 Acc: 0.0526\n","\n","Epoch 44/99\n","----------\n","train Loss: 3.0045 Acc: 0.0585\n","val Loss: 3.0397 Acc: 0.0526\n","\n","Epoch 45/99\n","----------\n","train Loss: 2.9891 Acc: 0.0526\n","val Loss: 3.0413 Acc: 0.0000\n","\n","Epoch 46/99\n","----------\n","train Loss: 3.0120 Acc: 0.0585\n","val Loss: 2.9909 Acc: 0.0526\n","\n","Epoch 47/99\n","----------\n","train Loss: 3.0368 Acc: 0.0643\n","val Loss: 3.0150 Acc: 0.0526\n","\n","Epoch 48/99\n","----------\n","train Loss: 3.0360 Acc: 0.0585\n","val Loss: 3.0033 Acc: 0.0526\n","\n","Epoch 49/99\n","----------\n","train Loss: 3.0146 Acc: 0.0526\n","val Loss: 2.9647 Acc: 0.1053\n","\n","Epoch 50/99\n","----------\n","train Loss: 3.0124 Acc: 0.0468\n","val Loss: 2.9957 Acc: 0.0526\n","\n","Epoch 51/99\n","----------\n","train Loss: 3.0118 Acc: 0.0468\n","val Loss: 3.0203 Acc: 0.1053\n","\n","Epoch 52/99\n","----------\n","train Loss: 3.0186 Acc: 0.0585\n","val Loss: 3.0159 Acc: 0.1053\n","\n","Epoch 53/99\n","----------\n","train Loss: 3.0024 Acc: 0.0468\n","val Loss: 3.1012 Acc: 0.0000\n","\n","Epoch 54/99\n","----------\n","train Loss: 3.0061 Acc: 0.0585\n","val Loss: 2.9868 Acc: 0.0526\n","\n","Epoch 55/99\n","----------\n","train Loss: 3.0307 Acc: 0.0585\n","val Loss: 2.9579 Acc: 0.1579\n","\n","Epoch 56/99\n","----------\n","train Loss: 3.0023 Acc: 0.0409\n","val Loss: 3.0229 Acc: 0.0526\n","\n","Epoch 57/99\n","----------\n","train Loss: 3.0008 Acc: 0.0526\n","val Loss: 2.9580 Acc: 0.0526\n","\n","Epoch 58/99\n","----------\n","train Loss: 2.9939 Acc: 0.0643\n","val Loss: 3.0042 Acc: 0.0526\n","\n","Epoch 59/99\n","----------\n","train Loss: 3.0065 Acc: 0.0702\n","val Loss: 3.0446 Acc: 0.0526\n","\n","Epoch 60/99\n","----------\n","train Loss: 3.0345 Acc: 0.0351\n","val Loss: 3.0622 Acc: 0.1053\n","\n","Epoch 61/99\n","----------\n","train Loss: 3.0166 Acc: 0.0409\n","val Loss: 3.0304 Acc: 0.0526\n","\n","Epoch 62/99\n","----------\n","train Loss: 3.0037 Acc: 0.0526\n","val Loss: 2.9868 Acc: 0.1053\n","\n","Epoch 63/99\n","----------\n","train Loss: 3.0150 Acc: 0.0526\n","val Loss: 2.9319 Acc: 0.1053\n","\n","Epoch 64/99\n","----------\n","train Loss: 3.0095 Acc: 0.0702\n","val Loss: 2.9942 Acc: 0.1053\n","\n","Epoch 65/99\n","----------\n","train Loss: 3.0123 Acc: 0.0526\n","val Loss: 3.0046 Acc: 0.1053\n","\n","Epoch 66/99\n","----------\n","train Loss: 3.0019 Acc: 0.0526\n","val Loss: 3.0163 Acc: 0.0000\n","\n","Epoch 67/99\n","----------\n","train Loss: 3.0156 Acc: 0.0468\n","val Loss: 3.0320 Acc: 0.0000\n","\n","Epoch 68/99\n","----------\n","train Loss: 3.0287 Acc: 0.0351\n","val Loss: 2.9789 Acc: 0.0526\n","\n","Epoch 69/99\n","----------\n","train Loss: 3.0199 Acc: 0.0585\n","val Loss: 3.0828 Acc: 0.0526\n","\n","Epoch 70/99\n","----------\n","train Loss: 3.0247 Acc: 0.0409\n","val Loss: 2.9771 Acc: 0.1579\n","\n","Epoch 71/99\n","----------\n","train Loss: 2.9961 Acc: 0.0643\n","val Loss: 3.0626 Acc: 0.0000\n","\n","Epoch 72/99\n","----------\n","train Loss: 3.0100 Acc: 0.0585\n","val Loss: 3.0414 Acc: 0.0526\n","\n","Epoch 73/99\n","----------\n","train Loss: 3.0137 Acc: 0.0760\n","val Loss: 3.0656 Acc: 0.0526\n","\n","Epoch 74/99\n","----------\n","train Loss: 3.0050 Acc: 0.0585\n","val Loss: 3.0359 Acc: 0.0000\n","\n","Epoch 75/99\n","----------\n","train Loss: 3.0038 Acc: 0.0643\n","val Loss: 3.0346 Acc: 0.0000\n","\n","Epoch 76/99\n","----------\n","train Loss: 3.0190 Acc: 0.0643\n","val Loss: 3.0100 Acc: 0.0526\n","\n","Epoch 77/99\n","----------\n","train Loss: 2.9930 Acc: 0.0585\n","val Loss: 3.0721 Acc: 0.0526\n","\n","Epoch 78/99\n","----------\n","train Loss: 3.0146 Acc: 0.0585\n","val Loss: 2.9085 Acc: 0.1053\n","\n","Epoch 79/99\n","----------\n","train Loss: 3.0021 Acc: 0.0526\n","val Loss: 3.0879 Acc: 0.0526\n","\n","Epoch 80/99\n","----------\n","train Loss: 3.0311 Acc: 0.0292\n","val Loss: 2.9893 Acc: 0.1053\n","\n","Epoch 81/99\n","----------\n","train Loss: 3.0085 Acc: 0.0351\n","val Loss: 3.0066 Acc: 0.1053\n","\n","Epoch 82/99\n","----------\n","train Loss: 3.0040 Acc: 0.0585\n","val Loss: 2.9535 Acc: 0.0000\n","\n","Epoch 83/99\n","----------\n","train Loss: 3.0146 Acc: 0.0643\n","val Loss: 3.1436 Acc: 0.0526\n","\n","Epoch 84/99\n","----------\n","train Loss: 3.0103 Acc: 0.0468\n","val Loss: 2.9755 Acc: 0.0526\n","\n","Epoch 85/99\n","----------\n","train Loss: 2.9961 Acc: 0.0409\n","val Loss: 3.0991 Acc: 0.0000\n","\n","Epoch 86/99\n","----------\n","train Loss: 3.0224 Acc: 0.0468\n","val Loss: 2.9374 Acc: 0.0000\n","\n","Epoch 87/99\n","----------\n","train Loss: 2.9738 Acc: 0.0585\n","val Loss: 2.9554 Acc: 0.0526\n","\n","Epoch 88/99\n","----------\n","train Loss: 3.0143 Acc: 0.0702\n","val Loss: 2.9251 Acc: 0.1053\n","\n","Epoch 89/99\n","----------\n","train Loss: 3.0276 Acc: 0.0526\n","val Loss: 3.0084 Acc: 0.1053\n","\n","Epoch 90/99\n","----------\n","train Loss: 3.0173 Acc: 0.0585\n","val Loss: 3.0445 Acc: 0.0526\n","\n","Epoch 91/99\n","----------\n","train Loss: 3.0237 Acc: 0.0643\n","val Loss: 2.9688 Acc: 0.1053\n","\n","Epoch 92/99\n","----------\n","train Loss: 2.9979 Acc: 0.0468\n","val Loss: 2.9532 Acc: 0.0526\n","\n","Epoch 93/99\n","----------\n","train Loss: 3.0217 Acc: 0.0526\n","val Loss: 3.0356 Acc: 0.0000\n","\n","Epoch 94/99\n","----------\n","train Loss: 3.0191 Acc: 0.0468\n","val Loss: 2.9574 Acc: 0.1579\n","\n","Epoch 95/99\n","----------\n","train Loss: 3.0009 Acc: 0.0468\n","val Loss: 3.0151 Acc: 0.0526\n","\n","Epoch 96/99\n","----------\n","train Loss: 3.0198 Acc: 0.0409\n","val Loss: 2.9857 Acc: 0.0000\n","\n","Epoch 97/99\n","----------\n","train Loss: 3.0331 Acc: 0.0468\n","val Loss: 2.9874 Acc: 0.0526\n","\n","Epoch 98/99\n","----------\n","train Loss: 3.0032 Acc: 0.0526\n","val Loss: 3.0218 Acc: 0.0526\n","\n","Epoch 99/99\n","----------\n","train Loss: 3.0166 Acc: 0.0468\n","val Loss: 3.0059 Acc: 0.0526\n","\n","Training complete in 20m 26s\n","Best val Acc: 0.157895\n","************************* 第 9 折 *************************\n","train: 171 val: 19\n","Epoch 0/99\n","----------\n","train Loss: 3.0374 Acc: 0.0351\n","val Loss: 3.0093 Acc: 0.0526\n","\n","Epoch 1/99\n","----------\n","train Loss: 3.0183 Acc: 0.0409\n","val Loss: 2.9702 Acc: 0.0526\n","\n","Epoch 2/99\n","----------\n","train Loss: 3.0077 Acc: 0.0526\n","val Loss: 3.0333 Acc: 0.0526\n","\n","Epoch 3/99\n","----------\n","train Loss: 2.9997 Acc: 0.0468\n","val Loss: 3.0341 Acc: 0.0526\n","\n","Epoch 4/99\n","----------\n","train Loss: 3.0251 Acc: 0.0409\n","val Loss: 2.8911 Acc: 0.0526\n","\n","Epoch 5/99\n","----------\n","train Loss: 3.0020 Acc: 0.0643\n","val Loss: 3.0146 Acc: 0.0526\n","\n","Epoch 6/99\n","----------\n","train Loss: 3.0207 Acc: 0.0585\n","val Loss: 2.9945 Acc: 0.1053\n","\n","Epoch 7/99\n","----------\n","train Loss: 3.0140 Acc: 0.0409\n","val Loss: 2.9431 Acc: 0.0526\n","\n","Epoch 8/99\n","----------\n","train Loss: 3.0508 Acc: 0.0585\n","val Loss: 3.0152 Acc: 0.0526\n","\n","Epoch 9/99\n","----------\n","train Loss: 3.0091 Acc: 0.0760\n","val Loss: 3.0464 Acc: 0.0526\n","\n","Epoch 10/99\n","----------\n","train Loss: 3.0372 Acc: 0.0468\n","val Loss: 3.0820 Acc: 0.1053\n","\n","Epoch 11/99\n","----------\n","train Loss: 3.0123 Acc: 0.0585\n","val Loss: 3.0562 Acc: 0.0000\n","\n","Epoch 12/99\n","----------\n","train Loss: 3.0080 Acc: 0.0351\n","val Loss: 3.0066 Acc: 0.0526\n","\n","Epoch 13/99\n","----------\n","train Loss: 2.9884 Acc: 0.0585\n","val Loss: 3.0086 Acc: 0.1053\n","\n","Epoch 14/99\n","----------\n","train Loss: 3.0229 Acc: 0.0702\n","val Loss: 2.9691 Acc: 0.0526\n","\n","Epoch 15/99\n","----------\n","train Loss: 3.0018 Acc: 0.0643\n","val Loss: 3.1007 Acc: 0.0000\n","\n","Epoch 16/99\n","----------\n","train Loss: 3.0050 Acc: 0.0585\n","val Loss: 2.9870 Acc: 0.0526\n","\n","Epoch 17/99\n","----------\n","train Loss: 3.0296 Acc: 0.0643\n","val Loss: 2.9223 Acc: 0.0000\n","\n","Epoch 18/99\n","----------\n","train Loss: 3.0058 Acc: 0.0526\n","val Loss: 2.9800 Acc: 0.0526\n","\n","Epoch 19/99\n","----------\n","train Loss: 3.0200 Acc: 0.0468\n","val Loss: 3.0129 Acc: 0.0526\n","\n","Epoch 20/99\n","----------\n","train Loss: 3.0135 Acc: 0.0585\n","val Loss: 2.9992 Acc: 0.0526\n","\n","Epoch 21/99\n","----------\n","train Loss: 3.0286 Acc: 0.0585\n","val Loss: 3.0348 Acc: 0.0000\n","\n","Epoch 22/99\n","----------\n","train Loss: 3.0231 Acc: 0.0351\n","val Loss: 3.0164 Acc: 0.1053\n","\n","Epoch 23/99\n","----------\n","train Loss: 3.0042 Acc: 0.0468\n","val Loss: 3.0100 Acc: 0.0526\n","\n","Epoch 24/99\n","----------\n","train Loss: 3.0116 Acc: 0.0643\n","val Loss: 2.8853 Acc: 0.1053\n","\n","Epoch 25/99\n","----------\n","train Loss: 2.9984 Acc: 0.0468\n","val Loss: 2.9904 Acc: 0.1053\n","\n","Epoch 26/99\n","----------\n","train Loss: 3.0324 Acc: 0.0468\n","val Loss: 3.0533 Acc: 0.0000\n","\n","Epoch 27/99\n","----------\n","train Loss: 2.9853 Acc: 0.0702\n","val Loss: 3.0165 Acc: 0.0526\n","\n","Epoch 28/99\n","----------\n","train Loss: 2.9960 Acc: 0.0526\n","val Loss: 3.0282 Acc: 0.0526\n","\n","Epoch 29/99\n","----------\n","train Loss: 3.0200 Acc: 0.0409\n","val Loss: 3.0571 Acc: 0.0526\n","\n","Epoch 30/99\n","----------\n","train Loss: 3.0193 Acc: 0.0526\n","val Loss: 2.9661 Acc: 0.1053\n","\n","Epoch 31/99\n","----------\n","train Loss: 2.9990 Acc: 0.0760\n","val Loss: 3.0061 Acc: 0.0000\n","\n","Epoch 32/99\n","----------\n","train Loss: 3.0309 Acc: 0.0468\n","val Loss: 2.9199 Acc: 0.0526\n","\n","Epoch 33/99\n","----------\n","train Loss: 3.0120 Acc: 0.0409\n","val Loss: 2.9863 Acc: 0.0526\n","\n","Epoch 34/99\n","----------\n","train Loss: 2.9944 Acc: 0.0409\n","val Loss: 2.9770 Acc: 0.0000\n","\n","Epoch 35/99\n","----------\n","train Loss: 3.0134 Acc: 0.0292\n","val Loss: 3.0685 Acc: 0.0526\n","\n","Epoch 36/99\n","----------\n","train Loss: 3.0218 Acc: 0.0468\n","val Loss: 3.0148 Acc: 0.0526\n","\n","Epoch 37/99\n","----------\n","train Loss: 3.0179 Acc: 0.0351\n","val Loss: 2.9864 Acc: 0.0526\n","\n","Epoch 38/99\n","----------\n","train Loss: 3.0132 Acc: 0.0468\n","val Loss: 3.0228 Acc: 0.0526\n","\n","Epoch 39/99\n","----------\n","train Loss: 3.0249 Acc: 0.0526\n","val Loss: 3.0687 Acc: 0.0526\n","\n","Epoch 40/99\n","----------\n","train Loss: 3.0327 Acc: 0.0702\n","val Loss: 2.9771 Acc: 0.0000\n","\n","Epoch 41/99\n","----------\n","train Loss: 2.9941 Acc: 0.0468\n","val Loss: 2.9916 Acc: 0.0000\n","\n","Epoch 42/99\n","----------\n","train Loss: 3.0314 Acc: 0.0526\n","val Loss: 3.0716 Acc: 0.0526\n","\n","Epoch 43/99\n","----------\n","train Loss: 3.0081 Acc: 0.0468\n","val Loss: 3.0032 Acc: 0.0526\n","\n","Epoch 44/99\n","----------\n","train Loss: 3.0030 Acc: 0.0585\n","val Loss: 3.0151 Acc: 0.1053\n","\n","Epoch 45/99\n","----------\n","train Loss: 3.0097 Acc: 0.0526\n","val Loss: 3.0186 Acc: 0.1053\n","\n","Epoch 46/99\n","----------\n","train Loss: 2.9940 Acc: 0.0409\n","val Loss: 3.0132 Acc: 0.1053\n","\n","Epoch 47/99\n","----------\n","train Loss: 2.9915 Acc: 0.0468\n","val Loss: 2.9453 Acc: 0.1579\n","\n","Epoch 48/99\n","----------\n","train Loss: 3.0059 Acc: 0.0409\n","val Loss: 3.0967 Acc: 0.0526\n","\n","Epoch 49/99\n","----------\n","train Loss: 3.0346 Acc: 0.0702\n","val Loss: 2.9911 Acc: 0.0000\n","\n","Epoch 50/99\n","----------\n","train Loss: 3.0102 Acc: 0.0702\n","val Loss: 2.9780 Acc: 0.1053\n","\n","Epoch 51/99\n","----------\n","train Loss: 3.0063 Acc: 0.0585\n","val Loss: 3.0215 Acc: 0.0526\n","\n","Epoch 52/99\n","----------\n","train Loss: 3.0075 Acc: 0.0234\n","val Loss: 3.1068 Acc: 0.0526\n","\n","Epoch 53/99\n","----------\n","train Loss: 3.0104 Acc: 0.0585\n","val Loss: 2.9558 Acc: 0.0526\n","\n","Epoch 54/99\n","----------\n","train Loss: 3.0088 Acc: 0.0526\n","val Loss: 3.0348 Acc: 0.0526\n","\n","Epoch 55/99\n","----------\n","train Loss: 3.0107 Acc: 0.0526\n","val Loss: 3.0920 Acc: 0.0000\n","\n","Epoch 56/99\n","----------\n","train Loss: 2.9771 Acc: 0.0760\n","val Loss: 3.0943 Acc: 0.0526\n","\n","Epoch 57/99\n","----------\n","train Loss: 3.0161 Acc: 0.0585\n","val Loss: 3.0297 Acc: 0.0526\n","\n","Epoch 58/99\n","----------\n","train Loss: 3.0044 Acc: 0.0702\n","val Loss: 3.0013 Acc: 0.0526\n","\n","Epoch 59/99\n","----------\n","train Loss: 3.0021 Acc: 0.0585\n","val Loss: 3.1155 Acc: 0.0526\n","\n","Epoch 60/99\n","----------\n","train Loss: 3.0439 Acc: 0.0526\n","val Loss: 2.9984 Acc: 0.0526\n","\n","Epoch 61/99\n","----------\n","train Loss: 3.0265 Acc: 0.0409\n","val Loss: 2.9886 Acc: 0.0526\n","\n","Epoch 62/99\n","----------\n","train Loss: 3.0056 Acc: 0.0409\n","val Loss: 3.0061 Acc: 0.0526\n","\n","Epoch 63/99\n","----------\n","train Loss: 3.0219 Acc: 0.0702\n","val Loss: 3.0883 Acc: 0.0526\n","\n","Epoch 64/99\n","----------\n","train Loss: 3.0193 Acc: 0.0526\n","val Loss: 3.0092 Acc: 0.0526\n","\n","Epoch 65/99\n","----------\n","train Loss: 2.9934 Acc: 0.0351\n","val Loss: 2.9562 Acc: 0.0000\n","\n","Epoch 66/99\n","----------\n","train Loss: 3.0112 Acc: 0.0643\n","val Loss: 3.0262 Acc: 0.0000\n","\n","Epoch 67/99\n","----------\n","train Loss: 3.0044 Acc: 0.0643\n","val Loss: 3.0397 Acc: 0.0000\n","\n","Epoch 68/99\n","----------\n","train Loss: 3.0075 Acc: 0.0468\n","val Loss: 3.0363 Acc: 0.0526\n","\n","Epoch 69/99\n","----------\n","train Loss: 2.9961 Acc: 0.0585\n","val Loss: 3.0480 Acc: 0.0526\n","\n","Epoch 70/99\n","----------\n","train Loss: 3.0185 Acc: 0.0585\n","val Loss: 3.0256 Acc: 0.0000\n","\n","Epoch 71/99\n","----------\n","train Loss: 3.0213 Acc: 0.0409\n","val Loss: 2.9247 Acc: 0.0000\n","\n","Epoch 72/99\n","----------\n","train Loss: 2.9868 Acc: 0.0643\n","val Loss: 3.0607 Acc: 0.1579\n","\n","Epoch 73/99\n","----------\n","train Loss: 3.0361 Acc: 0.0585\n","val Loss: 2.9808 Acc: 0.0526\n","\n","Epoch 74/99\n","----------\n","train Loss: 3.0187 Acc: 0.0702\n","val Loss: 3.0465 Acc: 0.0526\n","\n","Epoch 75/99\n","----------\n","train Loss: 3.0195 Acc: 0.0585\n","val Loss: 3.0632 Acc: 0.0526\n","\n","Epoch 76/99\n","----------\n","train Loss: 3.0102 Acc: 0.0526\n","val Loss: 2.9094 Acc: 0.1053\n","\n","Epoch 77/99\n","----------\n","train Loss: 2.9825 Acc: 0.0585\n","val Loss: 3.1694 Acc: 0.0526\n","\n","Epoch 78/99\n","----------\n","train Loss: 3.0176 Acc: 0.0468\n","val Loss: 3.0216 Acc: 0.0000\n","\n","Epoch 79/99\n","----------\n","train Loss: 3.0079 Acc: 0.0760\n","val Loss: 2.9862 Acc: 0.0526\n","\n","Epoch 80/99\n","----------\n","train Loss: 2.9883 Acc: 0.0468\n","val Loss: 3.0306 Acc: 0.1053\n","\n","Epoch 81/99\n","----------\n","train Loss: 3.0076 Acc: 0.0409\n","val Loss: 2.9793 Acc: 0.0526\n","\n","Epoch 82/99\n","----------\n","train Loss: 3.0028 Acc: 0.0643\n","val Loss: 3.0515 Acc: 0.0526\n","\n","Epoch 83/99\n","----------\n","train Loss: 3.0204 Acc: 0.0526\n","val Loss: 3.0549 Acc: 0.0526\n","\n","Epoch 84/99\n","----------\n","train Loss: 3.0311 Acc: 0.0526\n","val Loss: 3.0532 Acc: 0.0526\n","\n","Epoch 85/99\n","----------\n","train Loss: 3.0058 Acc: 0.0409\n","val Loss: 3.0397 Acc: 0.0526\n","\n","Epoch 86/99\n","----------\n","train Loss: 3.0116 Acc: 0.0526\n","val Loss: 2.9986 Acc: 0.0526\n","\n","Epoch 87/99\n","----------\n","train Loss: 3.0160 Acc: 0.0409\n","val Loss: 3.0069 Acc: 0.0000\n","\n","Epoch 88/99\n","----------\n","train Loss: 3.0128 Acc: 0.0409\n","val Loss: 3.0084 Acc: 0.0526\n","\n","Epoch 89/99\n","----------\n","train Loss: 3.0119 Acc: 0.0526\n","val Loss: 2.9717 Acc: 0.0526\n","\n","Epoch 90/99\n","----------\n","train Loss: 3.0243 Acc: 0.0468\n","val Loss: 2.9870 Acc: 0.0000\n","\n","Epoch 91/99\n","----------\n","train Loss: 3.0170 Acc: 0.0526\n","val Loss: 2.9988 Acc: 0.0526\n","\n","Epoch 92/99\n","----------\n","train Loss: 2.9884 Acc: 0.0468\n","val Loss: 3.0602 Acc: 0.0526\n","\n","Epoch 93/99\n","----------\n","train Loss: 3.0598 Acc: 0.0468\n","val Loss: 2.9793 Acc: 0.0000\n","\n","Epoch 94/99\n","----------\n","train Loss: 3.0138 Acc: 0.0409\n","val Loss: 3.0828 Acc: 0.0526\n","\n","Epoch 95/99\n","----------\n","train Loss: 3.0026 Acc: 0.0292\n","val Loss: 2.9596 Acc: 0.0526\n","\n","Epoch 96/99\n","----------\n","train Loss: 3.0185 Acc: 0.0409\n","val Loss: 3.0362 Acc: 0.0526\n","\n","Epoch 97/99\n","----------\n","train Loss: 2.9912 Acc: 0.0702\n","val Loss: 3.0600 Acc: 0.0526\n","\n","Epoch 98/99\n","----------\n","train Loss: 3.0227 Acc: 0.0351\n","val Loss: 3.0207 Acc: 0.0526\n","\n","Epoch 99/99\n","----------\n","train Loss: 3.0141 Acc: 0.0643\n","val Loss: 3.0092 Acc: 0.0000\n","\n","Training complete in 20m 31s\n","Best val Acc: 0.157895\n","************************* 第 10 折 *************************\n","train: 171 val: 19\n","Epoch 0/99\n","----------\n","train Loss: 3.0090 Acc: 0.0409\n","val Loss: 2.9947 Acc: 0.0526\n","\n","Epoch 1/99\n","----------\n","train Loss: 2.9983 Acc: 0.0409\n","val Loss: 3.0420 Acc: 0.0526\n","\n","Epoch 2/99\n","----------\n","train Loss: 3.0049 Acc: 0.0585\n","val Loss: 2.9345 Acc: 0.0526\n","\n","Epoch 3/99\n","----------\n","train Loss: 3.0057 Acc: 0.0526\n","val Loss: 3.0743 Acc: 0.1053\n","\n","Epoch 4/99\n","----------\n","train Loss: 3.0218 Acc: 0.0468\n","val Loss: 2.9816 Acc: 0.0000\n","\n","Epoch 5/99\n","----------\n","train Loss: 3.0050 Acc: 0.0468\n","val Loss: 3.0024 Acc: 0.1053\n","\n","Epoch 6/99\n","----------\n","train Loss: 2.9988 Acc: 0.0409\n","val Loss: 2.9906 Acc: 0.0526\n","\n","Epoch 7/99\n","----------\n","train Loss: 3.0237 Acc: 0.0409\n","val Loss: 3.0565 Acc: 0.0526\n","\n","Epoch 8/99\n","----------\n","train Loss: 3.0246 Acc: 0.0702\n","val Loss: 3.0135 Acc: 0.0000\n","\n","Epoch 9/99\n","----------\n","train Loss: 3.0276 Acc: 0.0468\n","val Loss: 3.0125 Acc: 0.0526\n","\n","Epoch 10/99\n","----------\n","train Loss: 3.0277 Acc: 0.0409\n","val Loss: 3.0086 Acc: 0.0526\n","\n","Epoch 11/99\n","----------\n","train Loss: 3.0254 Acc: 0.0468\n","val Loss: 2.9880 Acc: 0.0526\n","\n","Epoch 12/99\n","----------\n","train Loss: 2.9943 Acc: 0.0409\n","val Loss: 2.9782 Acc: 0.1579\n","\n","Epoch 13/99\n","----------\n","train Loss: 3.0006 Acc: 0.0585\n","val Loss: 3.0169 Acc: 0.0000\n","\n","Epoch 14/99\n","----------\n","train Loss: 3.0265 Acc: 0.0409\n","val Loss: 2.9374 Acc: 0.0526\n","\n","Epoch 15/99\n","----------\n","train Loss: 3.0035 Acc: 0.0468\n","val Loss: 2.9374 Acc: 0.0526\n","\n","Epoch 16/99\n","----------\n","train Loss: 2.9886 Acc: 0.0760\n","val Loss: 3.0422 Acc: 0.0000\n","\n","Epoch 17/99\n","----------\n","train Loss: 3.0188 Acc: 0.0292\n","val Loss: 3.0859 Acc: 0.0526\n","\n","Epoch 18/99\n","----------\n","train Loss: 3.0257 Acc: 0.0292\n","val Loss: 3.0670 Acc: 0.0000\n","\n","Epoch 19/99\n","----------\n","train Loss: 3.0154 Acc: 0.0292\n","val Loss: 2.8986 Acc: 0.0526\n","\n","Epoch 20/99\n","----------\n","train Loss: 3.0257 Acc: 0.0468\n","val Loss: 3.0242 Acc: 0.0000\n","\n","Epoch 21/99\n","----------\n","train Loss: 3.0074 Acc: 0.0526\n","val Loss: 3.0344 Acc: 0.0526\n","\n","Epoch 22/99\n","----------\n","train Loss: 3.0368 Acc: 0.0351\n","val Loss: 3.0706 Acc: 0.0526\n","\n","Epoch 23/99\n","----------\n","train Loss: 3.0213 Acc: 0.0643\n","val Loss: 2.9848 Acc: 0.0526\n","\n","Epoch 24/99\n","----------\n","train Loss: 3.0034 Acc: 0.0702\n","val Loss: 3.0151 Acc: 0.0000\n","\n","Epoch 25/99\n","----------\n","train Loss: 2.9872 Acc: 0.0643\n","val Loss: 2.9330 Acc: 0.0526\n","\n","Epoch 26/99\n","----------\n","train Loss: 3.0014 Acc: 0.0468\n","val Loss: 2.9711 Acc: 0.0526\n","\n","Epoch 27/99\n","----------\n","train Loss: 3.0160 Acc: 0.0409\n","val Loss: 2.9458 Acc: 0.0526\n","\n","Epoch 28/99\n","----------\n","train Loss: 3.0259 Acc: 0.0292\n","val Loss: 3.0226 Acc: 0.1053\n","\n","Epoch 29/99\n","----------\n","train Loss: 3.0089 Acc: 0.0526\n","val Loss: 3.0178 Acc: 0.0526\n","\n","Epoch 30/99\n","----------\n","train Loss: 3.0080 Acc: 0.0585\n","val Loss: 2.9573 Acc: 0.0526\n","\n","Epoch 31/99\n","----------\n","train Loss: 3.0228 Acc: 0.0585\n","val Loss: 3.0311 Acc: 0.0526\n","\n","Epoch 32/99\n","----------\n","train Loss: 3.0294 Acc: 0.0409\n","val Loss: 2.9807 Acc: 0.0526\n","\n","Epoch 33/99\n","----------\n","train Loss: 3.0192 Acc: 0.0409\n","val Loss: 3.0231 Acc: 0.0526\n","\n","Epoch 34/99\n","----------\n","train Loss: 3.0063 Acc: 0.0351\n","val Loss: 2.9157 Acc: 0.0526\n","\n","Epoch 35/99\n","----------\n","train Loss: 3.0108 Acc: 0.0526\n","val Loss: 2.9980 Acc: 0.0526\n","\n","Epoch 36/99\n","----------\n","train Loss: 3.0014 Acc: 0.0702\n","val Loss: 2.9298 Acc: 0.0526\n","\n","Epoch 37/99\n","----------\n","train Loss: 3.0307 Acc: 0.0585\n","val Loss: 2.9293 Acc: 0.0526\n","\n","Epoch 38/99\n","----------\n","train Loss: 3.0087 Acc: 0.0468\n","val Loss: 3.0074 Acc: 0.1053\n","\n","Epoch 39/99\n","----------\n","train Loss: 3.0377 Acc: 0.0468\n","val Loss: 3.0174 Acc: 0.1053\n","\n","Epoch 40/99\n","----------\n","train Loss: 3.0400 Acc: 0.0468\n","val Loss: 2.9396 Acc: 0.0526\n","\n","Epoch 41/99\n","----------\n","train Loss: 3.0072 Acc: 0.0643\n","val Loss: 3.0713 Acc: 0.0000\n","\n","Epoch 42/99\n","----------\n","train Loss: 3.0065 Acc: 0.0643\n","val Loss: 3.0361 Acc: 0.1053\n","\n","Epoch 43/99\n","----------\n","train Loss: 3.0123 Acc: 0.0409\n","val Loss: 2.9792 Acc: 0.1579\n","\n","Epoch 44/99\n","----------\n","train Loss: 3.0267 Acc: 0.0702\n","val Loss: 3.0695 Acc: 0.1053\n","\n","Epoch 45/99\n","----------\n","train Loss: 3.0129 Acc: 0.0643\n","val Loss: 2.9535 Acc: 0.1053\n","\n","Epoch 46/99\n","----------\n","train Loss: 3.0151 Acc: 0.0351\n","val Loss: 2.9849 Acc: 0.0000\n","\n","Epoch 47/99\n","----------\n","train Loss: 3.0011 Acc: 0.0526\n","val Loss: 2.9846 Acc: 0.1053\n","\n","Epoch 48/99\n","----------\n","train Loss: 3.0066 Acc: 0.0585\n","val Loss: 3.0820 Acc: 0.0526\n","\n","Epoch 49/99\n","----------\n","train Loss: 3.0196 Acc: 0.0351\n","val Loss: 2.9544 Acc: 0.0526\n","\n","Epoch 50/99\n","----------\n","train Loss: 2.9966 Acc: 0.0526\n","val Loss: 3.0014 Acc: 0.0526\n","\n","Epoch 51/99\n","----------\n","train Loss: 3.0274 Acc: 0.0409\n","val Loss: 3.0046 Acc: 0.0526\n","\n","Epoch 52/99\n","----------\n","train Loss: 3.0100 Acc: 0.0409\n","val Loss: 2.9690 Acc: 0.1053\n","\n","Epoch 53/99\n","----------\n","train Loss: 2.9920 Acc: 0.0526\n","val Loss: 2.9113 Acc: 0.0526\n","\n","Epoch 54/99\n","----------\n","train Loss: 3.0029 Acc: 0.0468\n","val Loss: 2.9963 Acc: 0.1053\n","\n","Epoch 55/99\n","----------\n","train Loss: 3.0295 Acc: 0.0526\n","val Loss: 3.0811 Acc: 0.0526\n","\n","Epoch 56/99\n","----------\n","train Loss: 3.0209 Acc: 0.0643\n","val Loss: 3.0779 Acc: 0.0000\n","\n","Epoch 57/99\n","----------\n","train Loss: 3.0170 Acc: 0.0526\n","val Loss: 3.0792 Acc: 0.0526\n","\n","Epoch 58/99\n","----------\n","train Loss: 3.0167 Acc: 0.0468\n","val Loss: 2.9663 Acc: 0.0526\n","\n","Epoch 59/99\n","----------\n","train Loss: 3.0179 Acc: 0.0702\n","val Loss: 3.0361 Acc: 0.0000\n","\n","Epoch 60/99\n","----------\n","train Loss: 3.0064 Acc: 0.0643\n","val Loss: 3.0070 Acc: 0.0526\n","\n","Epoch 61/99\n","----------\n","train Loss: 3.0019 Acc: 0.0468\n","val Loss: 3.0290 Acc: 0.0000\n","\n","Epoch 62/99\n","----------\n","train Loss: 3.0257 Acc: 0.0526\n","val Loss: 2.9956 Acc: 0.1053\n","\n","Epoch 63/99\n","----------\n","train Loss: 3.0131 Acc: 0.0585\n","val Loss: 2.9257 Acc: 0.0526\n","\n","Epoch 64/99\n","----------\n","train Loss: 2.9928 Acc: 0.0585\n","val Loss: 3.0750 Acc: 0.0000\n","\n","Epoch 65/99\n","----------\n","train Loss: 3.0012 Acc: 0.0643\n","val Loss: 2.8758 Acc: 0.1053\n","\n","Epoch 66/99\n","----------\n","train Loss: 3.0222 Acc: 0.0351\n","val Loss: 2.9649 Acc: 0.1053\n","\n","Epoch 67/99\n","----------\n","train Loss: 3.0177 Acc: 0.0643\n","val Loss: 2.9246 Acc: 0.0526\n","\n","Epoch 68/99\n","----------\n","train Loss: 3.0040 Acc: 0.0585\n","val Loss: 2.9832 Acc: 0.0526\n","\n","Epoch 69/99\n","----------\n","train Loss: 3.0239 Acc: 0.0526\n","val Loss: 3.0905 Acc: 0.0000\n","\n","Epoch 70/99\n","----------\n","train Loss: 2.9854 Acc: 0.0585\n","val Loss: 3.0907 Acc: 0.0000\n","\n","Epoch 71/99\n","----------\n","train Loss: 3.0064 Acc: 0.0409\n","val Loss: 3.0135 Acc: 0.1053\n","\n","Epoch 72/99\n","----------\n","train Loss: 3.0209 Acc: 0.0468\n","val Loss: 3.0241 Acc: 0.0526\n","\n","Epoch 73/99\n","----------\n","train Loss: 3.0057 Acc: 0.0702\n","val Loss: 3.0114 Acc: 0.0526\n","\n","Epoch 74/99\n","----------\n","train Loss: 3.0115 Acc: 0.0760\n","val Loss: 3.0434 Acc: 0.0526\n","\n","Epoch 75/99\n","----------\n","train Loss: 3.0402 Acc: 0.0409\n","val Loss: 3.0459 Acc: 0.0000\n","\n","Epoch 76/99\n","----------\n","train Loss: 3.0220 Acc: 0.0819\n","val Loss: 2.9396 Acc: 0.0526\n","\n","Epoch 77/99\n","----------\n","train Loss: 2.9878 Acc: 0.0468\n","val Loss: 2.9493 Acc: 0.0526\n","\n","Epoch 78/99\n","----------\n","train Loss: 3.0337 Acc: 0.0468\n","val Loss: 3.0050 Acc: 0.0526\n","\n","Epoch 79/99\n","----------\n","train Loss: 2.9909 Acc: 0.0585\n","val Loss: 2.9189 Acc: 0.1053\n","\n","Epoch 80/99\n","----------\n","train Loss: 3.0163 Acc: 0.0351\n","val Loss: 3.1037 Acc: 0.0000\n","\n","Epoch 81/99\n","----------\n","train Loss: 3.0093 Acc: 0.0468\n","val Loss: 3.0178 Acc: 0.0526\n","\n","Epoch 82/99\n","----------\n","train Loss: 3.0146 Acc: 0.0585\n","val Loss: 3.0495 Acc: 0.0526\n","\n","Epoch 83/99\n","----------\n","train Loss: 3.0219 Acc: 0.0468\n","val Loss: 3.1012 Acc: 0.1053\n","\n","Epoch 84/99\n","----------\n","train Loss: 3.0462 Acc: 0.0526\n","val Loss: 2.9679 Acc: 0.0526\n","\n","Epoch 85/99\n","----------\n","train Loss: 3.0231 Acc: 0.0643\n","val Loss: 2.9764 Acc: 0.0000\n","\n","Epoch 86/99\n","----------\n","train Loss: 3.0145 Acc: 0.0643\n","val Loss: 3.0121 Acc: 0.0000\n","\n","Epoch 87/99\n","----------\n","train Loss: 3.0159 Acc: 0.0468\n","val Loss: 2.9636 Acc: 0.1579\n","\n","Epoch 88/99\n","----------\n","train Loss: 3.0309 Acc: 0.0468\n","val Loss: 2.9244 Acc: 0.1053\n","\n","Epoch 89/99\n","----------\n","train Loss: 3.0118 Acc: 0.0702\n","val Loss: 3.0960 Acc: 0.0526\n","\n","Epoch 90/99\n","----------\n","train Loss: 2.9934 Acc: 0.0468\n","val Loss: 2.9957 Acc: 0.1053\n","\n","Epoch 91/99\n","----------\n","train Loss: 3.0212 Acc: 0.0526\n","val Loss: 2.9981 Acc: 0.0000\n","\n","Epoch 92/99\n","----------\n","train Loss: 3.0139 Acc: 0.0409\n","val Loss: 3.0002 Acc: 0.0526\n","\n","Epoch 93/99\n","----------\n","train Loss: 3.0171 Acc: 0.0409\n","val Loss: 3.0371 Acc: 0.1053\n","\n","Epoch 94/99\n","----------\n","train Loss: 3.0040 Acc: 0.0468\n","val Loss: 2.9552 Acc: 0.0526\n","\n","Epoch 95/99\n","----------\n","train Loss: 3.0267 Acc: 0.0526\n","val Loss: 2.9837 Acc: 0.0526\n","\n","Epoch 96/99\n","----------\n","train Loss: 2.9954 Acc: 0.0409\n","val Loss: 2.9977 Acc: 0.0000\n","\n","Epoch 97/99\n","----------\n","train Loss: 2.9853 Acc: 0.0585\n","val Loss: 3.0385 Acc: 0.0526\n","\n","Epoch 98/99\n","----------\n","train Loss: 3.0131 Acc: 0.0526\n","val Loss: 3.0407 Acc: 0.0526\n","\n","Epoch 99/99\n","----------\n","train Loss: 3.0156 Acc: 0.0409\n","val Loss: 2.9914 Acc: 0.0000\n","\n","Training complete in 20m 32s\n","Best val Acc: 0.157895\n","*********************** 第 11 折****END ***********************\n","the best acc in kfold tensor(0.8947, device='cuda:0', dtype=torch.float64)\n","teh average val acc 0.2368421052631579\n","CNN  sample_data\n","CNN  light_10fold.pth  sample_data\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"61mj4zcgID-1","colab_type":"text"},"source":["### 测试和保存"]},{"cell_type":"code","metadata":{"id":"-kFU0NUFY18s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1595380347735,"user_tz":-480,"elapsed":23263,"user":{"displayName":"周尉阳","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTKLMpcfIq9vdPt1QQyeIS64dyhcfjcYMqI2RN=s64","userId":"12403260934929188499"}},"outputId":"7ec451c3-76a5-43a3-8864-b9c3e2431714"},"source":["from google.colab import drive\n","drive.mount('/content/drive')   # 挂载云盘"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5LRsuUxKIfa0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595487781128,"user_tz":-480,"elapsed":3038,"user":{"displayName":"周尉阳","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTKLMpcfIq9vdPt1QQyeIS64dyhcfjcYMqI2RN=s64","userId":"12403260934929188499"}},"outputId":"dabc2063-28b0-46ee-9cfc-d07f19b81234"},"source":["os.chdir(\"/content\")\n","torch.save(model_ft,'light.pth')\n","!ls"],"execution_count":9,"outputs":[{"output_type":"stream","text":["CNN  light_10fold_night.pth  light.pth\tsample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j18g8ZADJK0D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595318540953,"user_tz":-480,"elapsed":4781,"user":{"displayName":"周尉阳","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTKLMpcfIq9vdPt1QQyeIS64dyhcfjcYMqI2RN=s64","userId":"12403260934929188499"}},"outputId":"67c57600-ecbc-42fa-dac9-64c524a33236"},"source":["os.chdir(\"/content/drive/My Drive\")\n","!ls\n","torch.save(model_ft,'light.pth')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["'2020 年日历.gsheet'  'Colab Notebooks'  'Science Journal'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lJqVtZvrHRcs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"ok","timestamp":1595487890636,"user_tz":-480,"elapsed":5127,"user":{"displayName":"周尉阳","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTKLMpcfIq9vdPt1QQyeIS64dyhcfjcYMqI2RN=s64","userId":"12403260934929188499"}},"outputId":"4f92dec2-5552-43c4-f18b-d80097f4008b"},"source":["# 训练集 扩充及正则化\n","# 验证集 仅正则化\n","data_dir = 'CNN/dataset/morning'    # 数据目录\n","!rm -rf CNN/dataset/morning/train\n","!rm -rf CNN/dataset/morning/val\n","data_transforms = transforms.Compose([\n","        transforms.Grayscale(1),\n","        transforms.CenterCrop(1080),\n","        transforms.Resize(224),\n","        transforms.ToTensor(),\n","        # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","        transforms.Normalize([0.1307], [0.3081])      # 灰度图标准化参数\n","    ])\n","\n","test_datasets = datasets.ImageFolder(data_dir, data_transforms)\n","test_dataloaders = torch.utils.data.DataLoader(test_datasets, batch_size=4, shuffle=True, num_workers=4)\n","\n","dataset_sizes = len(test_datasets)\n","class_names = test_datasets.classes\n","print (dataset_sizes)\n","print (test_datasets)\n","print (class_names)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["190\n","Dataset ImageFolder\n","    Number of datapoints: 190\n","    Root location: CNN/dataset/morning\n","    StandardTransform\n","Transform: Compose(\n","               Grayscale(num_output_channels=1)\n","               CenterCrop(size=(1080, 1080))\n","               Resize(size=224, interpolation=PIL.Image.BILINEAR)\n","               ToTensor()\n","               Normalize(mean=[0.1307], std=[0.3081])\n","           )\n","['0', '10', '15', '20', '25', '30', '35', '40', '45', '5', '50', '55', '60', '65', '70', '75', '80', '85', '90']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-8m6vi0ReiHG","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595487862168,"user_tz":-480,"elapsed":3046,"user":{"displayName":"周尉阳","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTKLMpcfIq9vdPt1QQyeIS64dyhcfjcYMqI2RN=s64","userId":"12403260934929188499"}}},"source":["test_dataloaders=torch.utils.data.DataLoader(image_datasets, batch_size=4, shuffle=True, num_workers=4)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"AO4w3k-NeHfV","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1595487903701,"user_tz":-480,"elapsed":8613,"user":{"displayName":"周尉阳","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjTKLMpcfIq9vdPt1QQyeIS64dyhcfjcYMqI2RN=s64","userId":"12403260934929188499"}},"outputId":"335f6137-8bc2-43e7-c435-3d5e329a9c1d"},"source":["#show acc\n","model = torch.load('light.pth')\n","eval_loss = 0.\n","eval_acc = 0.\n","s= 0.\n","with torch.no_grad():\n","  for i, (inputs, labels) in enumerate(test_dataloaders):\n","    inputs = inputs.to(device)\n","    labels = labels.to(device)\n","\n","    outputs = model(inputs)\n","    _, preds = torch.max(outputs, 1)\n","    for j in range(inputs.size()[0]):\n","      #s =s + int(class_names[preds[j]])\n","      #print(class_names[preds[j]])\n","      #if int(class_names[preds[j]]) == int(labels[j]):\n","      if class_names[preds[j]] == class_names[int(labels[j])]:\n","        s = s+1\n","print (s)\n","print (s/(len(test_dataloaders) * 4))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["14.0\n","0.07291666666666667\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HBwrwWa1DNvC","colab_type":"text"},"source":["# output"]},{"cell_type":"markdown","metadata":{"id":"BmTynIMdCrga","colab_type":"text"},"source":["#### g\n","************************* 第 1 折 *************************\n","train: 180 val: 10\n","Epoch 1/100\n","----------\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","流式输出内容被截断，只能显示最后 5000 行内容。\n","\n","Epoch 9/100\n","----------\n","train Loss: 2.9858 Acc: 0.0543\n","val Loss: 2.2544 Acc: 0.0000\n","\n","Epoch 10/100\n","----------\n","train Loss: 2.9556 Acc: 0.0435\n","val Loss: 2.2802 Acc: 0.0833\n","\n","Epoch 11/100\n","----------\n","train Loss: 2.9511 Acc: 0.0598\n","val Loss: 2.2826 Acc: 0.0833\n","\n","Epoch 12/100\n","----------\n","train Loss: 2.9826 Acc: 0.0489\n","val Loss: 2.2805 Acc: 0.0000\n","\n","Epoch 13/100\n","----------\n","train Loss: 2.9718 Acc: 0.0380\n","val Loss: 2.2297 Acc: 0.0000\n","\n","Epoch 14/100\n","----------\n","train Loss: 2.9678 Acc: 0.0326\n","val Loss: 2.2688 Acc: 0.0000\n","\n","Epoch 15/100\n","----------\n","train Loss: 2.9505 Acc: 0.0435\n","val Loss: 2.2342 Acc: 0.0833\n","\n","Epoch 16/100\n","----------\n","train Loss: 2.9635 Acc: 0.0380\n","val Loss: 2.2715 Acc: 0.0000\n","\n","Epoch 17/100\n","----------\n","train Loss: 2.9666 Acc: 0.0272\n","val Loss: 2.3357 Acc: 0.0833\n","\n","Epoch 18/100\n","----------\n","train Loss: 2.9397 Acc: 0.0543\n","val Loss: 2.2328 Acc: 0.0833\n","\n","Epoch 19/100\n","----------\n","train Loss: 2.9628 Acc: 0.0272\n","val Loss: 2.3843 Acc: 0.0000\n","\n","Epoch 20/100\n","----------\n","train Loss: 2.9415 Acc: 0.0543\n","val Loss: 2.1904 Acc: 0.1667\n","\n","Epoch 21/100\n","----------\n","train Loss: 2.9534 Acc: 0.0543\n","val Loss: 2.2940 Acc: 0.0833\n","\n","Epoch 22/100\n","----------\n","train Loss: 2.9796 Acc: 0.0435\n","val Loss: 2.2733 Acc: 0.0000\n","\n","Epoch 23/100\n","----------\n","train Loss: 2.9516 Acc: 0.0489\n","val Loss: 2.3172 Acc: 0.0833\n","\n","Epoch 24/100\n","----------\n","train Loss: 2.9849 Acc: 0.0489\n","val Loss: 2.3891 Acc: 0.0000\n","\n","Epoch 25/100\n","----------\n","train Loss: 2.9810 Acc: 0.0489\n","val Loss: 2.2641 Acc: 0.0833\n","\n","Epoch 26/100\n","----------\n","train Loss: 2.9713 Acc: 0.0326\n","val Loss: 2.2742 Acc: 0.0000\n","\n","Epoch 27/100\n","----------\n","train Loss: 2.9599 Acc: 0.0598\n","val Loss: 2.2678 Acc: 0.0000\n","\n","Epoch 28/100\n","----------\n","train Loss: 2.9528 Acc: 0.0815\n","val Loss: 2.2669 Acc: 0.0000\n","\n","Epoch 29/100\n","----------\n","train Loss: 2.9580 Acc: 0.0870\n","val Loss: 2.2459 Acc: 0.0000\n","\n","Epoch 30/100\n","----------\n","train Loss: 2.9579 Acc: 0.0543\n","val Loss: 2.3851 Acc: 0.0000\n","\n","Epoch 31/100\n","----------\n","train Loss: 2.9661 Acc: 0.0380\n","val Loss: 2.2857 Acc: 0.1667\n","\n","Epoch 32/100\n","----------\n","train Loss: 2.9647 Acc: 0.0380\n","val Loss: 2.3214 Acc: 0.0000\n","\n","Epoch 33/100\n","----------\n","train Loss: 2.9669 Acc: 0.0326\n","val Loss: 2.2380 Acc: 0.0833\n","\n","Epoch 34/100\n","----------\n","train Loss: 2.9362 Acc: 0.0707\n","val Loss: 2.2052 Acc: 0.0000\n","\n","Epoch 35/100\n","----------\n","train Loss: 2.9702 Acc: 0.0326\n","val Loss: 2.2526 Acc: 0.0000\n","\n","Epoch 36/100\n","----------\n","train Loss: 2.9499 Acc: 0.0326\n","val Loss: 2.3234 Acc: 0.0000\n","\n","Epoch 37/100\n","----------\n","train Loss: 2.9521 Acc: 0.0761\n","val Loss: 2.2595 Acc: 0.0000\n","\n","Epoch 38/100\n","----------\n","train Loss: 2.9754 Acc: 0.0489\n","val Loss: 2.3304 Acc: 0.0000\n","\n","Epoch 39/100\n","----------\n","train Loss: 2.9442 Acc: 0.0652\n","val Loss: 2.3096 Acc: 0.0000\n","\n","Epoch 40/100\n","----------\n","train Loss: 2.9438 Acc: 0.0707\n","val Loss: 2.2882 Acc: 0.0000\n","\n","Epoch 41/100\n","----------\n","train Loss: 2.9660 Acc: 0.0652\n","val Loss: 2.3883 Acc: 0.0000\n","\n","Epoch 42/100\n","----------\n","train Loss: 2.9581 Acc: 0.0652\n","val Loss: 2.3271 Acc: 0.0000\n","\n","Epoch 43/100\n","----------\n","train Loss: 2.9543 Acc: 0.0652\n","val Loss: 2.2966 Acc: 0.0000\n","\n","Epoch 44/100\n","----------\n","train Loss: 2.9618 Acc: 0.0543\n","val Loss: 2.3564 Acc: 0.0000\n","\n","Epoch 45/100\n","----------\n","train Loss: 2.9520 Acc: 0.0489\n","val Loss: 2.2223 Acc: 0.0000\n","\n","Epoch 46/100\n","----------\n","train Loss: 2.9852 Acc: 0.0163\n","val Loss: 2.2871 Acc: 0.0000\n","\n","Epoch 47/100\n","----------\n","train Loss: 2.9504 Acc: 0.0489\n","val Loss: 2.2706 Acc: 0.0000\n","\n","Epoch 48/100\n","----------\n","train Loss: 2.9640 Acc: 0.0435\n","val Loss: 2.1896 Acc: 0.0000\n","\n","Epoch 49/100\n","----------\n","train Loss: 2.9532 Acc: 0.0761\n","val Loss: 2.3182 Acc: 0.0000\n","\n","Epoch 50/100\n","----------\n","train Loss: 2.9536 Acc: 0.0489\n","val Loss: 2.3011 Acc: 0.0000\n","\n","Epoch 51/100\n","----------\n","train Loss: 2.9511 Acc: 0.0489\n","val Loss: 2.2948 Acc: 0.0000\n","\n","Epoch 52/100\n","----------\n","train Loss: 2.9757 Acc: 0.0380\n","val Loss: 2.2074 Acc: 0.0833\n","\n","Epoch 53/100\n","----------\n","train Loss: 2.9873 Acc: 0.0707\n","val Loss: 2.2511 Acc: 0.0833\n","\n","Epoch 54/100\n","----------\n","train Loss: 2.9651 Acc: 0.0489\n","val Loss: 2.2357 Acc: 0.0833\n","\n","Epoch 55/100\n","----------\n","train Loss: 2.9875 Acc: 0.0652\n","val Loss: 2.2204 Acc: 0.0000\n","\n","Epoch 56/100\n","----------\n","train Loss: 2.9591 Acc: 0.0598\n","val Loss: 2.2959 Acc: 0.0833\n","\n","Epoch 57/100\n","----------\n","train Loss: 2.9808 Acc: 0.0380\n","val Loss: 2.2486 Acc: 0.0000\n","\n","Epoch 58/100\n","----------\n","train Loss: 2.9846 Acc: 0.0489\n","val Loss: 2.2777 Acc: 0.0000\n","\n","Epoch 59/100\n","----------\n","train Loss: 2.9717 Acc: 0.0652\n","val Loss: 2.2913 Acc: 0.0000\n","\n","Epoch 60/100\n","----------\n","train Loss: 2.9905 Acc: 0.0598\n","val Loss: 2.2455 Acc: 0.0000\n","\n","Epoch 61/100\n","----------\n","train Loss: 2.9758 Acc: 0.0380\n","val Loss: 2.2013 Acc: 0.1667\n","\n","Epoch 62/100\n","----------\n","train Loss: 2.9694 Acc: 0.0326\n","val Loss: 2.2452 Acc: 0.0833\n","\n","Epoch 63/100\n","----------\n","train Loss: 2.9507 Acc: 0.0489\n","val Loss: 2.2457 Acc: 0.1667\n","\n","Epoch 64/100\n","----------\n","train Loss: 2.9628 Acc: 0.0489\n","val Loss: 2.3508 Acc: 0.0000\n","\n","Epoch 65/100\n","----------\n","train Loss: 2.9652 Acc: 0.0435\n","val Loss: 2.3018 Acc: 0.0000\n","\n","Epoch 66/100\n","----------\n","train Loss: 2.9766 Acc: 0.0380\n","val Loss: 2.2671 Acc: 0.0000\n","\n","Epoch 67/100\n","----------\n","train Loss: 2.9660 Acc: 0.0489\n","val Loss: 2.2801 Acc: 0.0000\n","\n","Epoch 68/100\n","----------\n","train Loss: 2.9763 Acc: 0.0435\n","val Loss: 2.2078 Acc: 0.0000\n","\n","Epoch 69/100\n","----------\n","train Loss: 2.9785 Acc: 0.0598\n","val Loss: 2.2309 Acc: 0.0833\n","\n","Epoch 70/100\n","----------\n","train Loss: 2.9747 Acc: 0.0489\n","val Loss: 2.3788 Acc: 0.0000\n","\n","Epoch 71/100\n","----------\n","train Loss: 2.9711 Acc: 0.0598\n","val Loss: 2.2041 Acc: 0.0000\n","\n","Epoch 72/100\n","----------\n","train Loss: 2.9679 Acc: 0.0543\n","val Loss: 2.1373 Acc: 0.1667\n","\n","Epoch 73/100\n","----------\n","train Loss: 2.9712 Acc: 0.0652\n","val Loss: 2.2989 Acc: 0.0000\n","\n","Epoch 74/100\n","----------\n","train Loss: 2.9551 Acc: 0.0707\n","val Loss: 2.2650 Acc: 0.0000\n","\n","Epoch 75/100\n","----------\n","train Loss: 2.9753 Acc: 0.0380\n","val Loss: 2.2441 Acc: 0.0000\n","\n","Epoch 76/100\n","----------\n","train Loss: 2.9422 Acc: 0.0598\n","val Loss: 2.3157 Acc: 0.0833\n","\n","Epoch 77/100\n","----------\n","train Loss: 2.9757 Acc: 0.0435\n","val Loss: 2.2965 Acc: 0.0000\n","\n","Epoch 78/100\n","----------\n","train Loss: 2.9545 Acc: 0.0489\n","val Loss: 2.2316 Acc: 0.0000\n","\n","Epoch 79/100\n","----------\n","train Loss: 2.9813 Acc: 0.0435\n","val Loss: 2.2218 Acc: 0.0000\n","\n","Epoch 80/100\n","----------\n","train Loss: 2.9754 Acc: 0.0435\n","val Loss: 2.3407 Acc: 0.0833\n","\n","Epoch 81/100\n","----------\n","train Loss: 2.9722 Acc: 0.0489\n","val Loss: 2.2668 Acc: 0.0000\n","\n","Epoch 82/100\n","----------\n","train Loss: 2.9904 Acc: 0.0598\n","val Loss: 2.3140 Acc: 0.0000\n","\n","Epoch 83/100\n","----------\n","train Loss: 2.9653 Acc: 0.0543\n","val Loss: 2.2582 Acc: 0.0833\n","\n","Epoch 84/100\n","----------\n","train Loss: 2.9801 Acc: 0.0489\n","val Loss: 2.3154 Acc: 0.0000\n","\n","Epoch 85/100\n","----------\n","train Loss: 2.9600 Acc: 0.0543\n","val Loss: 2.3552 Acc: 0.0000\n","\n","Epoch 86/100\n","----------\n","train Loss: 2.9691 Acc: 0.0489\n","val Loss: 2.2988 Acc: 0.0833\n","\n","Epoch 87/100\n","----------\n","train Loss: 2.9634 Acc: 0.0598\n","val Loss: 2.3138 Acc: 0.0000\n","\n","Epoch 88/100\n","----------\n","train Loss: 2.9606 Acc: 0.0489\n","val Loss: 2.2464 Acc: 0.0000\n","\n","Epoch 89/100\n","----------\n","train Loss: 2.9663 Acc: 0.0380\n","val Loss: 2.2564 Acc: 0.0000\n","\n","Epoch 90/100\n","----------\n","train Loss: 2.9620 Acc: 0.0435\n","val Loss: 2.2944 Acc: 0.0000\n","\n","Epoch 91/100\n","----------\n","train Loss: 2.9584 Acc: 0.0435\n","val Loss: 2.2866 Acc: 0.0000\n","\n","Epoch 92/100\n","----------\n","train Loss: 2.9508 Acc: 0.0326\n","val Loss: 2.2191 Acc: 0.0833\n","\n","Epoch 93/100\n","----------\n","train Loss: 2.9803 Acc: 0.0217\n","val Loss: 2.3061 Acc: 0.0833\n","\n","Epoch 94/100\n","----------\n","train Loss: 2.9578 Acc: 0.0870\n","val Loss: 2.2622 Acc: 0.0000\n","\n","Epoch 95/100\n","----------\n","train Loss: 2.9414 Acc: 0.0435\n","val Loss: 2.2914 Acc: 0.0000\n","\n","Epoch 96/100\n","----------\n","train Loss: 2.9504 Acc: 0.0380\n","val Loss: 2.2469 Acc: 0.0000\n","\n","Epoch 97/100\n","----------\n","train Loss: 2.9705 Acc: 0.0435\n","val Loss: 2.2953 Acc: 0.0000\n","\n","Epoch 98/100\n","----------\n","train Loss: 2.9623 Acc: 0.0489\n","val Loss: 2.2189 Acc: 0.0000\n","\n","Epoch 99/100\n","----------\n","train Loss: 2.9781 Acc: 0.0380\n","val Loss: 2.3541 Acc: 0.0000\n","\n","Epoch 100/100\n","----------\n","train Loss: 2.9656 Acc: 0.0489\n","val Loss: 2.2218 Acc: 0.0000\n","\n","Training complete in 20m 44s\n","Best val Acc: 0.166667\n","************************* 第 12 折 *************************\n","train: 181 val: 9\n","Epoch 1/100\n","----------\n","train Loss: 2.9747 Acc: 0.0489\n","val Loss: 2.2254 Acc: 0.0000\n","\n","Epoch 2/100\n","----------\n","train Loss: 2.9919 Acc: 0.0326\n","val Loss: 2.2853 Acc: 0.0000\n","\n","Epoch 3/100\n","----------\n","train Loss: 2.9740 Acc: 0.0489\n","val Loss: 2.2625 Acc: 0.0833\n","\n","Epoch 4/100\n","----------\n","train Loss: 2.9727 Acc: 0.0380\n","val Loss: 2.2825 Acc: 0.0000\n","\n","Epoch 5/100\n","----------\n","train Loss: 2.9716 Acc: 0.0598\n","val Loss: 2.1914 Acc: 0.0000\n","\n","Epoch 6/100\n","----------\n","train Loss: 2.9917 Acc: 0.0543\n","val Loss: 2.3077 Acc: 0.0000\n","\n","Epoch 7/100\n","----------\n","train Loss: 2.9744 Acc: 0.0598\n","val Loss: 2.2636 Acc: 0.0000\n","\n","Epoch 8/100\n","----------\n","train Loss: 2.9686 Acc: 0.0326\n","val Loss: 2.1919 Acc: 0.0000\n","\n","Epoch 9/100\n","----------\n","train Loss: 2.9894 Acc: 0.0380\n","val Loss: 2.2771 Acc: 0.0000\n","\n","Epoch 10/100\n","----------\n","train Loss: 2.9881 Acc: 0.0054\n","val Loss: 2.1810 Acc: 0.0000\n","\n","Epoch 11/100\n","----------\n","train Loss: 2.9710 Acc: 0.0435\n","val Loss: 2.2513 Acc: 0.0000\n","\n","Epoch 12/100\n","----------\n","train Loss: 2.9502 Acc: 0.0435\n","val Loss: 2.2200 Acc: 0.0000\n","\n","Epoch 13/100\n","----------\n","train Loss: 2.9855 Acc: 0.0380\n","val Loss: 2.2206 Acc: 0.0000\n","\n","Epoch 14/100\n","----------\n","train Loss: 2.9612 Acc: 0.0435\n","val Loss: 2.2256 Acc: 0.0000\n","\n","Epoch 15/100\n","----------\n","train Loss: 2.9874 Acc: 0.0489\n","val Loss: 2.2122 Acc: 0.0833\n","\n","Epoch 16/100\n","----------\n","train Loss: 2.9513 Acc: 0.0435\n","val Loss: 2.1738 Acc: 0.0833\n","\n","Epoch 17/100\n","----------\n","train Loss: 2.9879 Acc: 0.0543\n","val Loss: 2.1695 Acc: 0.0833\n","\n","Epoch 18/100\n","----------\n","train Loss: 2.9725 Acc: 0.0707\n","val Loss: 2.1501 Acc: 0.0000\n","\n","Epoch 19/100\n","----------\n","train Loss: 2.9822 Acc: 0.0435\n","val Loss: 2.1865 Acc: 0.0000\n","\n","Epoch 20/100\n","----------\n","train Loss: 3.0078 Acc: 0.0054\n","val Loss: 2.2494 Acc: 0.0000\n","\n","Epoch 21/100\n","----------\n","train Loss: 2.9676 Acc: 0.0435\n","val Loss: 2.2800 Acc: 0.0000\n","\n","Epoch 22/100\n","----------\n","train Loss: 2.9709 Acc: 0.0435\n","val Loss: 2.1478 Acc: 0.0833\n","\n","Epoch 23/100\n","----------\n","train Loss: 2.9431 Acc: 0.0326\n","val Loss: 2.2725 Acc: 0.0000\n","\n","Epoch 24/100\n","----------\n","train Loss: 2.9630 Acc: 0.0652\n","val Loss: 2.2244 Acc: 0.0000\n","\n","Epoch 25/100\n","----------\n","train Loss: 2.9880 Acc: 0.0435\n","val Loss: 2.2121 Acc: 0.1667\n","\n","Epoch 26/100\n","----------\n","train Loss: 2.9644 Acc: 0.0543\n","val Loss: 2.2282 Acc: 0.0000\n","\n","Epoch 27/100\n","----------\n","train Loss: 2.9790 Acc: 0.0707\n","val Loss: 2.3108 Acc: 0.0000\n","\n","Epoch 28/100\n","----------\n","train Loss: 2.9759 Acc: 0.0489\n","val Loss: 2.1459 Acc: 0.0000\n","\n","Epoch 29/100\n","----------\n","train Loss: 2.9813 Acc: 0.0435\n","val Loss: 2.2187 Acc: 0.0000\n","\n","Epoch 30/100\n","----------\n","train Loss: 2.9828 Acc: 0.0489\n","val Loss: 2.1694 Acc: 0.0000\n","\n","Epoch 31/100\n","----------\n","train Loss: 2.9558 Acc: 0.0543\n","val Loss: 2.1932 Acc: 0.0833\n","\n","Epoch 32/100\n","----------\n","train Loss: 2.9667 Acc: 0.0598\n","val Loss: 2.3344 Acc: 0.0000\n","\n","Epoch 33/100\n","----------\n","train Loss: 2.9865 Acc: 0.0489\n","val Loss: 2.2164 Acc: 0.0000\n","\n","Epoch 34/100\n","----------\n","train Loss: 2.9731 Acc: 0.0489\n","val Loss: 2.1943 Acc: 0.0000\n","\n","Epoch 35/100\n","----------\n","train Loss: 2.9830 Acc: 0.0380\n","val Loss: 2.2746 Acc: 0.0000\n","\n","Epoch 36/100\n","----------\n","train Loss: 2.9690 Acc: 0.0761\n","val Loss: 2.2680 Acc: 0.0000\n","\n","Epoch 37/100\n","----------\n","train Loss: 2.9794 Acc: 0.0543\n","val Loss: 2.1887 Acc: 0.0000\n","\n","Epoch 38/100\n","----------\n","train Loss: 2.9624 Acc: 0.0435\n","val Loss: 2.3163 Acc: 0.0000\n","\n","Epoch 39/100\n","----------\n","train Loss: 2.9774 Acc: 0.0217\n","val Loss: 2.2490 Acc: 0.0000\n","\n","Epoch 40/100\n","----------\n","train Loss: 2.9630 Acc: 0.0598\n","val Loss: 2.2045 Acc: 0.0833\n","\n","Epoch 41/100\n","----------\n","train Loss: 2.9809 Acc: 0.0217\n","val Loss: 2.2090 Acc: 0.1667\n","\n","Epoch 42/100\n","----------\n","train Loss: 2.9839 Acc: 0.0217\n","val Loss: 2.1401 Acc: 0.0833\n","\n","Epoch 43/100\n","----------\n","train Loss: 2.9653 Acc: 0.0707\n","val Loss: 2.2170 Acc: 0.0000\n","\n","Epoch 44/100\n","----------\n","train Loss: 2.9699 Acc: 0.0435\n","val Loss: 2.1823 Acc: 0.0000\n","\n","Epoch 45/100\n","----------\n","train Loss: 2.9744 Acc: 0.0435\n","val Loss: 2.1785 Acc: 0.0833\n","\n","Epoch 46/100\n","----------\n","train Loss: 2.9880 Acc: 0.0489\n","val Loss: 2.1930 Acc: 0.0833\n","\n","Epoch 47/100\n","----------\n","train Loss: 2.9641 Acc: 0.0380\n","val Loss: 2.2589 Acc: 0.0000\n","\n","Epoch 48/100\n","----------\n","train Loss: 2.9981 Acc: 0.0380\n","val Loss: 2.2231 Acc: 0.0833\n","\n","Epoch 49/100\n","----------\n","train Loss: 2.9817 Acc: 0.0598\n","val Loss: 2.2482 Acc: 0.0833\n","\n","Epoch 50/100\n","----------\n","train Loss: 2.9727 Acc: 0.0598\n","val Loss: 2.2751 Acc: 0.0833\n","\n","Epoch 51/100\n","----------\n","train Loss: 2.9711 Acc: 0.0435\n","val Loss: 2.2396 Acc: 0.0000\n","\n","Epoch 52/100\n","----------\n","train Loss: 2.9759 Acc: 0.0435\n","val Loss: 2.1872 Acc: 0.0833\n","\n","Epoch 53/100\n","----------\n","train Loss: 2.9931 Acc: 0.0380\n","val Loss: 2.2032 Acc: 0.0000\n","\n","Epoch 54/100\n","----------\n","train Loss: 2.9538 Acc: 0.0652\n","val Loss: 2.1812 Acc: 0.0833\n","\n","Epoch 55/100\n","----------\n","train Loss: 2.9746 Acc: 0.0217\n","val Loss: 2.2109 Acc: 0.0833\n","\n","Epoch 56/100\n","----------\n","train Loss: 2.9748 Acc: 0.0598\n","val Loss: 2.1614 Acc: 0.0833\n","\n","Epoch 57/100\n","----------\n","train Loss: 2.9618 Acc: 0.0326\n","val Loss: 2.2279 Acc: 0.0000\n","\n","Epoch 58/100\n","----------\n","train Loss: 2.9713 Acc: 0.0435\n","val Loss: 2.2733 Acc: 0.0000\n","\n","Epoch 59/100\n","----------\n","train Loss: 2.9411 Acc: 0.0489\n","val Loss: 2.3084 Acc: 0.0000\n","\n","Epoch 60/100\n","----------\n","train Loss: 2.9762 Acc: 0.0217\n","val Loss: 2.1969 Acc: 0.1667\n","\n","Epoch 61/100\n","----------\n","train Loss: 2.9702 Acc: 0.0489\n","val Loss: 2.2550 Acc: 0.0833\n","\n","Epoch 62/100\n","----------\n","train Loss: 2.9677 Acc: 0.0543\n","val Loss: 2.2594 Acc: 0.0000\n","\n","Epoch 63/100\n","----------\n","train Loss: 2.9718 Acc: 0.0598\n","val Loss: 2.2869 Acc: 0.0000\n","\n","Epoch 64/100\n","----------\n","train Loss: 2.9794 Acc: 0.0217\n","val Loss: 2.2807 Acc: 0.0000\n","\n","Epoch 65/100\n","----------\n","train Loss: 2.9660 Acc: 0.0326\n","val Loss: 2.2566 Acc: 0.0000\n","\n","Epoch 66/100\n","----------\n","train Loss: 2.9514 Acc: 0.0543\n","val Loss: 2.2996 Acc: 0.0000\n","\n","Epoch 67/100\n","----------\n","train Loss: 2.9770 Acc: 0.0380\n","val Loss: 2.3494 Acc: 0.0000\n","\n","Epoch 68/100\n","----------\n","train Loss: 2.9612 Acc: 0.0489\n","val Loss: 2.0761 Acc: 0.2500\n","\n","Epoch 69/100\n","----------\n","train Loss: 2.9734 Acc: 0.0598\n","val Loss: 2.2552 Acc: 0.0833\n","\n","Epoch 70/100\n","----------\n","train Loss: 2.9808 Acc: 0.0652\n","val Loss: 2.2553 Acc: 0.0000\n","\n","Epoch 71/100\n","----------\n","train Loss: 2.9890 Acc: 0.0380\n","val Loss: 2.1646 Acc: 0.1667\n","\n","Epoch 72/100\n","----------\n","train Loss: 2.9779 Acc: 0.0326\n","val Loss: 2.2434 Acc: 0.0000\n","\n","Epoch 73/100\n","----------\n","train Loss: 2.9770 Acc: 0.0435\n","val Loss: 2.2495 Acc: 0.0000\n","\n","Epoch 74/100\n","----------\n","train Loss: 2.9675 Acc: 0.0380\n","val Loss: 2.1697 Acc: 0.0833\n","\n","Epoch 75/100\n","----------\n","train Loss: 2.9564 Acc: 0.0489\n","val Loss: 2.1705 Acc: 0.0833\n","\n","Epoch 76/100\n","----------\n","train Loss: 2.9712 Acc: 0.0380\n","val Loss: 2.1840 Acc: 0.0000\n","\n","Epoch 77/100\n","----------\n","train Loss: 2.9596 Acc: 0.0598\n","val Loss: 2.3502 Acc: 0.0000\n","\n","Epoch 78/100\n","----------\n","train Loss: 2.9681 Acc: 0.0815\n","val Loss: 2.1450 Acc: 0.0000\n","\n","Epoch 79/100\n","----------\n","train Loss: 2.9676 Acc: 0.0326\n","val Loss: 2.3493 Acc: 0.0000\n","\n","Epoch 80/100\n","----------\n","train Loss: 2.9642 Acc: 0.0598\n","val Loss: 2.2462 Acc: 0.0000\n","\n","Epoch 81/100\n","----------\n","train Loss: 2.9515 Acc: 0.0543\n","val Loss: 2.3386 Acc: 0.0000\n","\n","Epoch 82/100\n","----------\n","train Loss: 2.9588 Acc: 0.0598\n","val Loss: 2.2476 Acc: 0.0000\n","\n","Epoch 83/100\n","----------\n","train Loss: 2.9795 Acc: 0.0272\n","val Loss: 2.1884 Acc: 0.0833\n","\n","Epoch 84/100\n","----------\n","train Loss: 2.9608 Acc: 0.0543\n","val Loss: 2.2129 Acc: 0.0000\n","\n","Epoch 85/100\n","----------\n","train Loss: 2.9855 Acc: 0.0380\n","val Loss: 2.2831 Acc: 0.0833\n","\n","Epoch 86/100\n","----------\n","train Loss: 2.9901 Acc: 0.0543\n","val Loss: 2.2214 Acc: 0.0833\n","\n","Epoch 87/100\n","----------\n","train Loss: 2.9608 Acc: 0.0272\n","val Loss: 2.2942 Acc: 0.0000\n","\n","Epoch 88/100\n","----------\n","train Loss: 2.9715 Acc: 0.0543\n","val Loss: 2.2725 Acc: 0.0000\n","\n","Epoch 89/100\n","----------\n","train Loss: 2.9705 Acc: 0.0598\n","val Loss: 2.2136 Acc: 0.0833\n","\n","Epoch 90/100\n","----------\n","train Loss: 2.9773 Acc: 0.0707\n","val Loss: 2.1792 Acc: 0.0000\n","\n","Epoch 91/100\n","----------\n","train Loss: 2.9803 Acc: 0.0598\n","val Loss: 2.1227 Acc: 0.1667\n","\n","Epoch 92/100\n","----------\n","train Loss: 2.9760 Acc: 0.0543\n","val Loss: 2.2040 Acc: 0.0833\n","\n","Epoch 93/100\n","----------\n","train Loss: 2.9556 Acc: 0.0380\n","val Loss: 2.3494 Acc: 0.0000\n","\n","Epoch 94/100\n","----------\n","train Loss: 2.9923 Acc: 0.0163\n","val Loss: 2.2764 Acc: 0.0000\n","\n","Epoch 95/100\n","----------\n","train Loss: 2.9702 Acc: 0.0272\n","val Loss: 2.2293 Acc: 0.0833\n","\n","Epoch 96/100\n","----------\n","train Loss: 2.9492 Acc: 0.0707\n","val Loss: 2.1709 Acc: 0.0833\n","\n","Epoch 97/100\n","----------\n","train Loss: 2.9715 Acc: 0.0435\n","val Loss: 2.2041 Acc: 0.0000\n","\n","Epoch 98/100\n","----------\n","train Loss: 2.9718 Acc: 0.0489\n","val Loss: 2.2762 Acc: 0.1667\n","\n","Epoch 99/100\n","----------\n","train Loss: 2.9723 Acc: 0.0326\n","val Loss: 2.2959 Acc: 0.0833\n","\n","Epoch 100/100\n","----------\n","train Loss: 2.9894 Acc: 0.0543\n","val Loss: 2.2751 Acc: 0.0000\n","\n","Training complete in 20m 46s\n","Best val Acc: 0.250000\n","************************* 第 13 折 *************************\n","train: 181 val: 9\n","Epoch 1/100\n","----------\n","train Loss: 2.9553 Acc: 0.0543\n","val Loss: 2.3786 Acc: 0.0000\n","\n","Epoch 2/100\n","----------\n","train Loss: 2.9702 Acc: 0.0435\n","val Loss: 2.3131 Acc: 0.0000\n","\n","Epoch 3/100\n","----------\n","train Loss: 2.9504 Acc: 0.0543\n","val Loss: 2.2932 Acc: 0.0833\n","\n","Epoch 4/100\n","----------\n","train Loss: 2.9545 Acc: 0.0380\n","val Loss: 2.3727 Acc: 0.0000\n","\n","Epoch 5/100\n","----------\n","train Loss: 2.9618 Acc: 0.0326\n","val Loss: 2.3159 Acc: 0.0833\n","\n","Epoch 6/100\n","----------\n","train Loss: 2.9525 Acc: 0.0489\n","val Loss: 2.2957 Acc: 0.0000\n","\n","Epoch 7/100\n","----------\n","train Loss: 2.9797 Acc: 0.0815\n","val Loss: 2.4976 Acc: 0.0000\n","\n","Epoch 8/100\n","----------\n","train Loss: 2.9313 Acc: 0.0380\n","val Loss: 2.2079 Acc: 0.0000\n","\n","Epoch 9/100\n","----------\n","train Loss: 2.9711 Acc: 0.0435\n","val Loss: 2.3105 Acc: 0.0000\n","\n","Epoch 10/100\n","----------\n","train Loss: 2.9622 Acc: 0.0489\n","val Loss: 2.2841 Acc: 0.0833\n","\n","Epoch 11/100\n","----------\n","train Loss: 2.9761 Acc: 0.0272\n","val Loss: 2.3434 Acc: 0.0000\n","\n","Epoch 12/100\n","----------\n","train Loss: 2.9830 Acc: 0.0435\n","val Loss: 2.3294 Acc: 0.0833\n","\n","Epoch 13/100\n","----------\n","train Loss: 2.9574 Acc: 0.0380\n","val Loss: 2.2651 Acc: 0.0000\n","\n","Epoch 14/100\n","----------\n","train Loss: 2.9608 Acc: 0.0598\n","val Loss: 2.2937 Acc: 0.0000\n","\n","Epoch 15/100\n","----------\n","train Loss: 2.9762 Acc: 0.0489\n","val Loss: 2.2466 Acc: 0.0833\n","\n","Epoch 16/100\n","----------\n","train Loss: 2.9848 Acc: 0.0380\n","val Loss: 2.2900 Acc: 0.0000\n","\n","Epoch 17/100\n","----------\n","train Loss: 2.9787 Acc: 0.0272\n","val Loss: 2.3461 Acc: 0.0000\n","\n","Epoch 18/100\n","----------\n","train Loss: 2.9645 Acc: 0.0435\n","val Loss: 2.3406 Acc: 0.0000\n","\n","Epoch 19/100\n","----------\n","train Loss: 2.9567 Acc: 0.0489\n","val Loss: 2.4017 Acc: 0.0000\n","\n","Epoch 20/100\n","----------\n","train Loss: 2.9668 Acc: 0.0543\n","val Loss: 2.3489 Acc: 0.0000\n","\n","Epoch 21/100\n","----------\n","train Loss: 2.9946 Acc: 0.0380\n","val Loss: 2.2655 Acc: 0.0000\n","\n","Epoch 22/100\n","----------\n","train Loss: 2.9549 Acc: 0.0598\n","val Loss: 2.3960 Acc: 0.0000\n","\n","Epoch 23/100\n","----------\n","train Loss: 2.9604 Acc: 0.0380\n","val Loss: 2.3039 Acc: 0.0833\n","\n","Epoch 24/100\n","----------\n","train Loss: 2.9538 Acc: 0.0652\n","val Loss: 2.2413 Acc: 0.0000\n","\n","Epoch 25/100\n","----------\n","train Loss: 2.9630 Acc: 0.0543\n","val Loss: 2.4588 Acc: 0.0000\n","\n","Epoch 26/100\n","----------\n","train Loss: 2.9528 Acc: 0.0489\n","val Loss: 2.3548 Acc: 0.0000\n","\n","Epoch 27/100\n","----------\n","train Loss: 2.9395 Acc: 0.0543\n","val Loss: 2.2863 Acc: 0.0000\n","\n","Epoch 28/100\n","----------\n","train Loss: 2.9664 Acc: 0.0489\n","val Loss: 2.3791 Acc: 0.0000\n","\n","Epoch 29/100\n","----------\n","train Loss: 2.9742 Acc: 0.0435\n","val Loss: 2.3371 Acc: 0.0000\n","\n","Epoch 30/100\n","----------\n","train Loss: 2.9746 Acc: 0.0543\n","val Loss: 2.4093 Acc: 0.0000\n","\n","Epoch 31/100\n","----------\n","train Loss: 2.9810 Acc: 0.0326\n","val Loss: 2.3250 Acc: 0.0833\n","\n","Epoch 32/100\n","----------\n","train Loss: 2.9753 Acc: 0.0380\n","val Loss: 2.1992 Acc: 0.0833\n","\n","Epoch 33/100\n","----------\n","train Loss: 2.9482 Acc: 0.0598\n","val Loss: 2.2921 Acc: 0.0000\n","\n","Epoch 34/100\n","----------\n","train Loss: 2.9794 Acc: 0.0489\n","val Loss: 2.3782 Acc: 0.0000\n","\n","Epoch 35/100\n","----------\n","train Loss: 2.9749 Acc: 0.0326\n","val Loss: 2.3032 Acc: 0.0833\n","\n","Epoch 36/100\n","----------\n","train Loss: 2.9596 Acc: 0.0543\n","val Loss: 2.2461 Acc: 0.0000\n","\n","Epoch 37/100\n","----------\n","train Loss: 2.9805 Acc: 0.0109\n","val Loss: 2.3621 Acc: 0.0833\n","\n","Epoch 38/100\n","----------\n","train Loss: 2.9651 Acc: 0.0489\n","val Loss: 2.2835 Acc: 0.0000\n","\n","Epoch 39/100\n","----------\n","train Loss: 2.9591 Acc: 0.0598\n","val Loss: 2.3911 Acc: 0.0833\n","\n","Epoch 40/100\n","----------\n","train Loss: 2.9701 Acc: 0.0435\n","val Loss: 2.3329 Acc: 0.0000\n","\n","Epoch 41/100\n","----------\n","train Loss: 2.9753 Acc: 0.0489\n","val Loss: 2.3452 Acc: 0.0000\n","\n","Epoch 42/100\n","----------\n","train Loss: 2.9467 Acc: 0.0543\n","val Loss: 2.3050 Acc: 0.0000\n","\n","Epoch 43/100\n","----------\n","train Loss: 2.9703 Acc: 0.0598\n","val Loss: 2.3415 Acc: 0.0000\n","\n","Epoch 44/100\n","----------\n","train Loss: 2.9751 Acc: 0.0272\n","val Loss: 2.2519 Acc: 0.0000\n","\n","Epoch 45/100\n","----------\n","train Loss: 2.9741 Acc: 0.0598\n","val Loss: 2.3132 Acc: 0.0000\n","\n","Epoch 46/100\n","----------\n","train Loss: 2.9643 Acc: 0.0326\n","val Loss: 2.2859 Acc: 0.0000\n","\n","Epoch 47/100\n","----------\n","train Loss: 2.9579 Acc: 0.0272\n","val Loss: 2.3676 Acc: 0.0000\n","\n","Epoch 48/100\n","----------\n","train Loss: 2.9727 Acc: 0.0435\n","val Loss: 2.3381 Acc: 0.0000\n","\n","Epoch 49/100\n","----------\n","train Loss: 2.9808 Acc: 0.0489\n","val Loss: 2.4044 Acc: 0.0000\n","\n","Epoch 50/100\n","----------\n","train Loss: 2.9816 Acc: 0.0380\n","val Loss: 2.4264 Acc: 0.0000\n","\n","Epoch 51/100\n","----------\n","train Loss: 2.9762 Acc: 0.0543\n","val Loss: 2.2863 Acc: 0.0000\n","\n","Epoch 52/100\n","----------\n","train Loss: 2.9534 Acc: 0.0598\n","val Loss: 2.2437 Acc: 0.0000\n","\n","Epoch 53/100\n","----------\n","train Loss: 2.9531 Acc: 0.0707\n","val Loss: 2.2895 Acc: 0.0833\n","\n","Epoch 54/100\n","----------\n","train Loss: 2.9569 Acc: 0.0707\n","val Loss: 2.2406 Acc: 0.0833\n","\n","Epoch 55/100\n","----------\n","train Loss: 2.9865 Acc: 0.0489\n","val Loss: 2.3547 Acc: 0.0000\n","\n","Epoch 56/100\n","----------\n","train Loss: 2.9630 Acc: 0.0326\n","val Loss: 2.2650 Acc: 0.0000\n","\n","Epoch 57/100\n","----------\n","train Loss: 2.9581 Acc: 0.0326\n","val Loss: 2.3396 Acc: 0.0000\n","\n","Epoch 58/100\n","----------\n","train Loss: 2.9740 Acc: 0.0272\n","val Loss: 2.2479 Acc: 0.0833\n","\n","Epoch 59/100\n","----------\n","train Loss: 2.9649 Acc: 0.0272\n","val Loss: 2.2829 Acc: 0.0000\n","\n","Epoch 60/100\n","----------\n","train Loss: 2.9629 Acc: 0.0326\n","val Loss: 2.4178 Acc: 0.0000\n","\n","Epoch 61/100\n","----------\n","train Loss: 2.9643 Acc: 0.0652\n","val Loss: 2.2433 Acc: 0.0833\n","\n","Epoch 62/100\n","----------\n","train Loss: 2.9600 Acc: 0.0707\n","val Loss: 2.3335 Acc: 0.0000\n","\n","Epoch 63/100\n","----------\n","train Loss: 2.9674 Acc: 0.0815\n","val Loss: 2.2320 Acc: 0.1667\n","\n","Epoch 64/100\n","----------\n","train Loss: 2.9649 Acc: 0.0489\n","val Loss: 2.3790 Acc: 0.0000\n","\n","Epoch 65/100\n","----------\n","train Loss: 2.9706 Acc: 0.0543\n","val Loss: 2.2024 Acc: 0.0833\n","\n","Epoch 66/100\n","----------\n","train Loss: 2.9481 Acc: 0.0870\n","val Loss: 2.2798 Acc: 0.0833\n","\n","Epoch 67/100\n","----------\n","train Loss: 2.9710 Acc: 0.0435\n","val Loss: 2.2756 Acc: 0.0833\n","\n","Epoch 68/100\n","----------\n","train Loss: 2.9645 Acc: 0.0272\n","val Loss: 2.2634 Acc: 0.0833\n","\n","Epoch 69/100\n","----------\n","train Loss: 2.9692 Acc: 0.0489\n","val Loss: 2.3458 Acc: 0.0000\n","\n","Epoch 70/100\n","----------\n","train Loss: 2.9644 Acc: 0.0380\n","val Loss: 2.2768 Acc: 0.0833\n","\n","Epoch 71/100\n","----------\n","train Loss: 2.9566 Acc: 0.0543\n","val Loss: 2.1770 Acc: 0.1667\n","\n","Epoch 72/100\n","----------\n","train Loss: 2.9545 Acc: 0.0598\n","val Loss: 2.3357 Acc: 0.0000\n","\n","Epoch 73/100\n","----------\n","train Loss: 2.9623 Acc: 0.0435\n","val Loss: 2.3604 Acc: 0.0000\n","\n","Epoch 74/100\n","----------\n","train Loss: 2.9652 Acc: 0.0489\n","val Loss: 2.3259 Acc: 0.0000\n","\n","Epoch 75/100\n","----------\n","train Loss: 2.9691 Acc: 0.0163\n","val Loss: 2.3370 Acc: 0.0000\n","\n","Epoch 76/100\n","----------\n","train Loss: 2.9534 Acc: 0.0652\n","val Loss: 2.3740 Acc: 0.0000\n","\n","Epoch 77/100\n","----------\n","train Loss: 2.9360 Acc: 0.0761\n","val Loss: 2.3104 Acc: 0.0000\n","\n","Epoch 78/100\n","----------\n","train Loss: 2.9311 Acc: 0.0652\n","val Loss: 2.2719 Acc: 0.0000\n","\n","Epoch 79/100\n","----------\n","train Loss: 2.9824 Acc: 0.0543\n","val Loss: 2.3929 Acc: 0.0833\n","\n","Epoch 80/100\n","----------\n","train Loss: 2.9815 Acc: 0.0435\n","val Loss: 2.3239 Acc: 0.0833\n","\n","Epoch 81/100\n","----------\n","train Loss: 2.9759 Acc: 0.0435\n","val Loss: 2.3785 Acc: 0.0000\n","\n","Epoch 82/100\n","----------\n","train Loss: 2.9699 Acc: 0.0380\n","val Loss: 2.3632 Acc: 0.0833\n","\n","Epoch 83/100\n","----------\n","train Loss: 2.9622 Acc: 0.0380\n","val Loss: 2.1757 Acc: 0.1667\n","\n","Epoch 84/100\n","----------\n","train Loss: 2.9689 Acc: 0.0489\n","val Loss: 2.3248 Acc: 0.1667\n","\n","Epoch 85/100\n","----------\n","train Loss: 2.9556 Acc: 0.0543\n","val Loss: 2.4220 Acc: 0.0000\n","\n","Epoch 86/100\n","----------\n","train Loss: 2.9509 Acc: 0.0761\n","val Loss: 2.3574 Acc: 0.0000\n","\n","Epoch 87/100\n","----------\n","train Loss: 2.9604 Acc: 0.0380\n","val Loss: 2.4392 Acc: 0.0000\n","\n","Epoch 88/100\n","----------\n","train Loss: 2.9702 Acc: 0.0272\n","val Loss: 2.2581 Acc: 0.0000\n","\n","Epoch 89/100\n","----------\n","train Loss: 2.9725 Acc: 0.0326\n","val Loss: 2.1971 Acc: 0.0000\n","\n","Epoch 90/100\n","----------\n","train Loss: 2.9801 Acc: 0.0652\n","val Loss: 2.3641 Acc: 0.0833\n","\n","Epoch 91/100\n","----------\n","train Loss: 2.9575 Acc: 0.0543\n","val Loss: 2.2627 Acc: 0.0000\n","\n","Epoch 92/100\n","----------\n","train Loss: 2.9468 Acc: 0.0435\n","val Loss: 2.3257 Acc: 0.0000\n","\n","Epoch 93/100\n","----------\n","train Loss: 2.9696 Acc: 0.0598\n","val Loss: 2.3179 Acc: 0.0000\n","\n","Epoch 94/100\n","----------\n","train Loss: 2.9741 Acc: 0.0435\n","val Loss: 2.3871 Acc: 0.0000\n","\n","Epoch 95/100\n","----------\n","train Loss: 2.9566 Acc: 0.0380\n","val Loss: 2.3494 Acc: 0.0000\n","\n","Epoch 96/100\n","----------\n","train Loss: 2.9641 Acc: 0.0326\n","val Loss: 2.3686 Acc: 0.0000\n","\n","Epoch 97/100\n","----------\n","train Loss: 2.9666 Acc: 0.0326\n","val Loss: 2.2504 Acc: 0.0833\n","\n","Epoch 98/100\n","----------\n","train Loss: 2.9591 Acc: 0.0489\n","val Loss: 2.2716 Acc: 0.0000\n","\n","Epoch 99/100\n","----------\n","train Loss: 2.9536 Acc: 0.0652\n","val Loss: 2.3607 Acc: 0.0000\n","\n","Epoch 100/100\n","----------\n","train Loss: 2.9694 Acc: 0.0380\n","val Loss: 2.4562 Acc: 0.0000\n","\n","Training complete in 20m 46s\n","Best val Acc: 0.166667\n","************************* 第 14 折 *************************\n","train: 181 val: 9\n","Epoch 1/100\n","----------\n","train Loss: 2.9908 Acc: 0.0163\n","val Loss: 2.3196 Acc: 0.0000\n","\n","Epoch 2/100\n","----------\n","train Loss: 2.9636 Acc: 0.0326\n","val Loss: 2.3032 Acc: 0.0000\n","\n","Epoch 3/100\n","----------\n","train Loss: 2.9515 Acc: 0.0272\n","val Loss: 2.3693 Acc: 0.0000\n","\n","Epoch 4/100\n","----------\n","train Loss: 2.9780 Acc: 0.0489\n","val Loss: 2.3245 Acc: 0.0833\n","\n","Epoch 5/100\n","----------\n","train Loss: 2.9738 Acc: 0.0598\n","val Loss: 2.3945 Acc: 0.0000\n","\n","Epoch 6/100\n","----------\n","train Loss: 2.9816 Acc: 0.0543\n","val Loss: 2.3099 Acc: 0.0000\n","\n","Epoch 7/100\n","----------\n","train Loss: 2.9614 Acc: 0.0489\n","val Loss: 2.4047 Acc: 0.0833\n","\n","Epoch 8/100\n","----------\n","train Loss: 2.9579 Acc: 0.0707\n","val Loss: 2.3769 Acc: 0.0000\n","\n","Epoch 9/100\n","----------\n","train Loss: 2.9740 Acc: 0.0543\n","val Loss: 2.3353 Acc: 0.0000\n","\n","Epoch 10/100\n","----------\n","train Loss: 2.9765 Acc: 0.0489\n","val Loss: 2.4308 Acc: 0.0000\n","\n","Epoch 11/100\n","----------\n","train Loss: 2.9489 Acc: 0.0761\n","val Loss: 2.4265 Acc: 0.0000\n","\n","Epoch 12/100\n","----------\n","train Loss: 2.9765 Acc: 0.0543\n","val Loss: 2.3657 Acc: 0.0000\n","\n","Epoch 13/100\n","----------\n","train Loss: 2.9430 Acc: 0.0761\n","val Loss: 2.4311 Acc: 0.0000\n","\n","Epoch 14/100\n","----------\n","train Loss: 2.9716 Acc: 0.0380\n","val Loss: 2.3802 Acc: 0.0000\n","\n","Epoch 15/100\n","----------\n","train Loss: 2.9828 Acc: 0.0652\n","val Loss: 2.4129 Acc: 0.0000\n","\n","Epoch 16/100\n","----------\n","train Loss: 2.9624 Acc: 0.0435\n","val Loss: 2.3725 Acc: 0.0000\n","\n","Epoch 17/100\n","----------\n","train Loss: 2.9650 Acc: 0.0761\n","val Loss: 2.3135 Acc: 0.0000\n","\n","Epoch 18/100\n","----------\n","train Loss: 2.9659 Acc: 0.0272\n","val Loss: 2.3305 Acc: 0.0000\n","\n","Epoch 19/100\n","----------\n","train Loss: 2.9667 Acc: 0.0326\n","val Loss: 2.3172 Acc: 0.0000\n","\n","Epoch 20/100\n","----------\n","train Loss: 2.9547 Acc: 0.0489\n","val Loss: 2.3603 Acc: 0.0000\n","\n","Epoch 21/100\n","----------\n","train Loss: 2.9488 Acc: 0.0598\n","val Loss: 2.3202 Acc: 0.0000\n","\n","Epoch 22/100\n","----------\n","train Loss: 2.9449 Acc: 0.0598\n","val Loss: 2.3182 Acc: 0.0000\n","\n","Epoch 23/100\n","----------\n","train Loss: 2.9580 Acc: 0.0652\n","val Loss: 2.3977 Acc: 0.0000\n","\n","Epoch 24/100\n","----------\n","train Loss: 2.9461 Acc: 0.0380\n","val Loss: 2.2969 Acc: 0.0833\n","\n","Epoch 25/100\n","----------\n","train Loss: 2.9563 Acc: 0.0815\n","val Loss: 2.2734 Acc: 0.0000\n","\n","Epoch 26/100\n","----------\n","train Loss: 2.9466 Acc: 0.0598\n","val Loss: 2.3980 Acc: 0.0000\n","\n","Epoch 27/100\n","----------\n","train Loss: 2.9526 Acc: 0.0380\n","val Loss: 2.3720 Acc: 0.0000\n","\n","Epoch 28/100\n","----------\n","train Loss: 2.9681 Acc: 0.0598\n","val Loss: 2.3007 Acc: 0.0833\n","\n","Epoch 29/100\n","----------\n","train Loss: 2.9686 Acc: 0.0652\n","val Loss: 2.3843 Acc: 0.0000\n","\n","Epoch 30/100\n","----------\n","train Loss: 2.9688 Acc: 0.0707\n","val Loss: 2.2637 Acc: 0.0000\n","\n","Epoch 31/100\n","----------\n","train Loss: 2.9647 Acc: 0.0652\n","val Loss: 2.3701 Acc: 0.0000\n","\n","Epoch 32/100\n","----------\n","train Loss: 2.9480 Acc: 0.0380\n","val Loss: 2.3245 Acc: 0.0000\n","\n","Epoch 33/100\n","----------\n","train Loss: 2.9829 Acc: 0.0543\n","val Loss: 2.3333 Acc: 0.0000\n","\n","Epoch 34/100\n","----------\n","train Loss: 2.9629 Acc: 0.0489\n","val Loss: 2.4065 Acc: 0.0000\n","\n","Epoch 35/100\n","----------\n","train Loss: 2.9774 Acc: 0.0326\n","val Loss: 2.3690 Acc: 0.0000\n","\n","Epoch 36/100\n","----------\n","train Loss: 2.9599 Acc: 0.0543\n","val Loss: 2.2934 Acc: 0.0000\n","\n","Epoch 37/100\n","----------\n","train Loss: 2.9838 Acc: 0.0489\n","val Loss: 2.3619 Acc: 0.0000\n","\n","Epoch 38/100\n","----------\n","train Loss: 2.9362 Acc: 0.0380\n","val Loss: 2.3833 Acc: 0.0000\n","\n","Epoch 39/100\n","----------\n","train Loss: 2.9844 Acc: 0.0380\n","val Loss: 2.4106 Acc: 0.0000\n","\n","Epoch 40/100\n","----------\n","train Loss: 2.9388 Acc: 0.0326\n","val Loss: 2.3618 Acc: 0.0000\n","\n","Epoch 41/100\n","----------\n","train Loss: 2.9622 Acc: 0.0435\n","val Loss: 2.3954 Acc: 0.0000\n","\n","Epoch 42/100\n","----------\n","train Loss: 2.9557 Acc: 0.0326\n","val Loss: 2.3134 Acc: 0.0000\n","\n","Epoch 43/100\n","----------\n","train Loss: 2.9730 Acc: 0.0543\n","val Loss: 2.4326 Acc: 0.0000\n","\n","Epoch 44/100\n","----------\n","train Loss: 2.9311 Acc: 0.0543\n","val Loss: 2.3637 Acc: 0.0000\n","\n","Epoch 45/100\n","----------\n","train Loss: 2.9639 Acc: 0.0543\n","val Loss: 2.3838 Acc: 0.0000\n","\n","Epoch 46/100\n","----------\n","train Loss: 2.9631 Acc: 0.0489\n","val Loss: 2.2915 Acc: 0.0000\n","\n","Epoch 47/100\n","----------\n","train Loss: 2.9736 Acc: 0.0217\n","val Loss: 2.4014 Acc: 0.0000\n","\n","Epoch 48/100\n","----------\n","train Loss: 2.9420 Acc: 0.0489\n","val Loss: 2.3321 Acc: 0.0000\n","\n","Epoch 49/100\n","----------\n","train Loss: 2.9469 Acc: 0.0707\n","val Loss: 2.3447 Acc: 0.0000\n","\n","Epoch 50/100\n","----------\n","train Loss: 2.9621 Acc: 0.0543\n","val Loss: 2.3946 Acc: 0.0000\n","\n","Epoch 51/100\n","----------\n","train Loss: 2.9741 Acc: 0.0217\n","val Loss: 2.3585 Acc: 0.0000\n","\n","Epoch 52/100\n","----------\n","train Loss: 2.9530 Acc: 0.0380\n","val Loss: 2.4278 Acc: 0.0000\n","\n","Epoch 53/100\n","----------\n","train Loss: 2.9595 Acc: 0.0435\n","val Loss: 2.3474 Acc: 0.0000\n","\n","Epoch 54/100\n","----------\n","train Loss: 2.9567 Acc: 0.0380\n","val Loss: 2.3662 Acc: 0.0000\n","\n","Epoch 55/100\n","----------\n","train Loss: 2.9715 Acc: 0.0489\n","val Loss: 2.3780 Acc: 0.0000\n","\n","Epoch 56/100\n","----------\n","train Loss: 2.9638 Acc: 0.0217\n","val Loss: 2.4113 Acc: 0.0000\n","\n","Epoch 57/100\n","----------\n","train Loss: 2.9713 Acc: 0.0489\n","val Loss: 2.4069 Acc: 0.0000\n","\n","Epoch 58/100\n","----------\n","train Loss: 2.9495 Acc: 0.0761\n","val Loss: 2.4353 Acc: 0.0000\n","\n","Epoch 59/100\n","----------\n","train Loss: 2.9817 Acc: 0.0326\n","val Loss: 2.3732 Acc: 0.0000\n","\n","Epoch 60/100\n","----------\n","train Loss: 2.9460 Acc: 0.0543\n","val Loss: 2.4589 Acc: 0.0000\n","\n","Epoch 61/100\n","----------\n","train Loss: 2.9559 Acc: 0.0272\n","val Loss: 2.3786 Acc: 0.0833\n","\n","Epoch 62/100\n","----------\n","train Loss: 2.9645 Acc: 0.0652\n","val Loss: 2.3517 Acc: 0.0000\n","\n","Epoch 63/100\n","----------\n","train Loss: 2.9893 Acc: 0.0326\n","val Loss: 2.3708 Acc: 0.0000\n","\n","Epoch 64/100\n","----------\n","train Loss: 2.9582 Acc: 0.0924\n","val Loss: 2.3554 Acc: 0.0000\n","\n","Epoch 65/100\n","----------\n","train Loss: 2.9672 Acc: 0.0598\n","val Loss: 2.3364 Acc: 0.0000\n","\n","Epoch 66/100\n","----------\n","train Loss: 2.9638 Acc: 0.0380\n","val Loss: 2.3763 Acc: 0.0000\n","\n","Epoch 67/100\n","----------\n","train Loss: 2.9634 Acc: 0.0652\n","val Loss: 2.4106 Acc: 0.0000\n","\n","Epoch 68/100\n","----------\n","train Loss: 2.9652 Acc: 0.0326\n","val Loss: 2.3578 Acc: 0.0000\n","\n","Epoch 69/100\n","----------\n","train Loss: 2.9846 Acc: 0.0272\n","val Loss: 2.4268 Acc: 0.0000\n","\n","Epoch 70/100\n","----------\n","train Loss: 2.9582 Acc: 0.0707\n","val Loss: 2.4013 Acc: 0.0000\n","\n","Epoch 71/100\n","----------\n","train Loss: 2.9648 Acc: 0.0380\n","val Loss: 2.4247 Acc: 0.0000\n","\n","Epoch 72/100\n","----------\n","train Loss: 2.9446 Acc: 0.0598\n","val Loss: 2.3071 Acc: 0.0000\n","\n","Epoch 73/100\n","----------\n","train Loss: 2.9469 Acc: 0.0598\n","val Loss: 2.3385 Acc: 0.0000\n","\n","Epoch 74/100\n","----------\n","train Loss: 2.9730 Acc: 0.0489\n","val Loss: 2.3568 Acc: 0.0833\n","\n","Epoch 75/100\n","----------\n","train Loss: 2.9635 Acc: 0.0598\n","val Loss: 2.3859 Acc: 0.0000\n","\n","Epoch 76/100\n","----------\n","train Loss: 2.9754 Acc: 0.0326\n","val Loss: 2.3732 Acc: 0.0000\n","\n","Epoch 77/100\n","----------\n","train Loss: 2.9679 Acc: 0.0435\n","val Loss: 2.3366 Acc: 0.0000\n","\n","Epoch 78/100\n","----------\n","train Loss: 2.9620 Acc: 0.0489\n","val Loss: 2.3882 Acc: 0.0000\n","\n","Epoch 79/100\n","----------\n","train Loss: 2.9789 Acc: 0.0326\n","val Loss: 2.3293 Acc: 0.0000\n","\n","Epoch 80/100\n","----------\n","train Loss: 2.9598 Acc: 0.0598\n","val Loss: 2.3246 Acc: 0.0000\n","\n","Epoch 81/100\n","----------\n","train Loss: 2.9489 Acc: 0.0598\n","val Loss: 2.4132 Acc: 0.0000\n","\n","Epoch 82/100\n","----------\n","train Loss: 2.9778 Acc: 0.0652\n","val Loss: 2.3750 Acc: 0.0000\n","\n","Epoch 83/100\n","----------\n","train Loss: 2.9567 Acc: 0.0489\n","val Loss: 2.3469 Acc: 0.0000\n","\n","Epoch 84/100\n","----------\n","train Loss: 2.9551 Acc: 0.0489\n","val Loss: 2.3854 Acc: 0.0000\n","\n","Epoch 85/100\n","----------\n","train Loss: 2.9587 Acc: 0.0598\n","val Loss: 2.2720 Acc: 0.0000\n","\n","Epoch 86/100\n","----------\n","train Loss: 2.9527 Acc: 0.0435\n","val Loss: 2.4316 Acc: 0.0000\n","\n","Epoch 87/100\n","----------\n","train Loss: 2.9753 Acc: 0.0707\n","val Loss: 2.3779 Acc: 0.0000\n","\n","Epoch 88/100\n","----------\n","train Loss: 2.9600 Acc: 0.0326\n","val Loss: 2.4142 Acc: 0.0000\n","\n","Epoch 89/100\n","----------\n","train Loss: 2.9683 Acc: 0.0380\n","val Loss: 2.3663 Acc: 0.0000\n","\n","Epoch 90/100\n","----------\n","train Loss: 2.9483 Acc: 0.0707\n","val Loss: 2.4568 Acc: 0.0000\n","\n","Epoch 91/100\n","----------\n","train Loss: 2.9284 Acc: 0.0815\n","val Loss: 2.2915 Acc: 0.0833\n","\n","Epoch 92/100\n","----------\n","train Loss: 2.9853 Acc: 0.0598\n","val Loss: 2.3675 Acc: 0.0000\n","\n","Epoch 93/100\n","----------\n","train Loss: 2.9567 Acc: 0.0652\n","val Loss: 2.2861 Acc: 0.0000\n","\n","Epoch 94/100\n","----------\n","train Loss: 2.9822 Acc: 0.0326\n","val Loss: 2.3616 Acc: 0.0000\n","\n","Epoch 95/100\n","----------\n","train Loss: 2.9659 Acc: 0.0380\n","val Loss: 2.2918 Acc: 0.0000\n","\n","Epoch 96/100\n","----------\n","train Loss: 2.9609 Acc: 0.0435\n","val Loss: 2.3210 Acc: 0.0000\n","\n","Epoch 97/100\n","----------\n","train Loss: 2.9672 Acc: 0.0489\n","val Loss: 2.3801 Acc: 0.0000\n","\n","Epoch 98/100\n","----------\n","train Loss: 2.9659 Acc: 0.0489\n","val Loss: 2.4158 Acc: 0.0000\n","\n","Epoch 99/100\n","----------\n","train Loss: 2.9600 Acc: 0.0543\n","val Loss: 2.3484 Acc: 0.0000\n","\n","Epoch 100/100\n","----------\n","train Loss: 2.9695 Acc: 0.0326\n","val Loss: 2.3442 Acc: 0.0000\n","\n","Training complete in 20m 47s\n","Best val Acc: 0.083333\n","************************* 第 15 折 *************************\n","train: 181 val: 9\n","Epoch 1/100\n","----------\n","train Loss: 2.9864 Acc: 0.0326\n","val Loss: 2.0472 Acc: 0.1667\n","\n","Epoch 2/100\n","----------\n","train Loss: 2.9814 Acc: 0.0435\n","val Loss: 2.1887 Acc: 0.0833\n","\n","Epoch 3/100\n","----------\n","train Loss: 2.9917 Acc: 0.0380\n","val Loss: 2.2010 Acc: 0.0000\n","\n","Epoch 4/100\n","----------\n","train Loss: 2.9496 Acc: 0.0815\n","val Loss: 2.2430 Acc: 0.1667\n","\n","Epoch 5/100\n","----------\n","train Loss: 2.9549 Acc: 0.0652\n","val Loss: 2.1302 Acc: 0.0833\n","\n","Epoch 6/100\n","----------\n","train Loss: 2.9580 Acc: 0.0761\n","val Loss: 2.1646 Acc: 0.0833\n","\n","Epoch 7/100\n","----------\n","train Loss: 2.9444 Acc: 0.0380\n","val Loss: 2.1355 Acc: 0.1667\n","\n","Epoch 8/100\n","----------\n","train Loss: 2.9748 Acc: 0.0380\n","val Loss: 2.1663 Acc: 0.1667\n","\n","Epoch 9/100\n","----------\n","train Loss: 2.9934 Acc: 0.0272\n","val Loss: 2.1319 Acc: 0.0833\n","\n","Epoch 10/100\n","----------\n","train Loss: 2.9680 Acc: 0.0272\n","val Loss: 2.2243 Acc: 0.0833\n","\n","Epoch 11/100\n","----------\n","train Loss: 2.9655 Acc: 0.0489\n","val Loss: 2.2659 Acc: 0.0000\n","\n","Epoch 12/100\n","----------\n","train Loss: 2.9719 Acc: 0.0489\n","val Loss: 2.2034 Acc: 0.0000\n","\n","Epoch 13/100\n","----------\n","train Loss: 2.9669 Acc: 0.0707\n","val Loss: 2.1696 Acc: 0.0000\n","\n","Epoch 14/100\n","----------\n","train Loss: 2.9777 Acc: 0.0543\n","val Loss: 2.1739 Acc: 0.2500\n","\n","Epoch 15/100\n","----------\n","train Loss: 2.9682 Acc: 0.0435\n","val Loss: 2.2101 Acc: 0.0833\n","\n","Epoch 16/100\n","----------\n","train Loss: 2.9766 Acc: 0.0272\n","val Loss: 2.2428 Acc: 0.1667\n","\n","Epoch 17/100\n","----------\n","train Loss: 2.9581 Acc: 0.0761\n","val Loss: 2.1019 Acc: 0.0833\n","\n","Epoch 18/100\n","----------\n","train Loss: 2.9863 Acc: 0.0272\n","val Loss: 2.1627 Acc: 0.1667\n","\n","Epoch 19/100\n","----------\n","train Loss: 2.9890 Acc: 0.0326\n","val Loss: 2.2050 Acc: 0.0000\n","\n","Epoch 20/100\n","----------\n","train Loss: 2.9805 Acc: 0.0543\n","val Loss: 2.2636 Acc: 0.0000\n","\n","Epoch 21/100\n","----------\n","train Loss: 2.9663 Acc: 0.0489\n","val Loss: 2.1472 Acc: 0.0833\n","\n","Epoch 22/100\n","----------\n","train Loss: 2.9729 Acc: 0.0435\n","val Loss: 2.2341 Acc: 0.0000\n","\n","Epoch 23/100\n","----------\n","train Loss: 2.9809 Acc: 0.0272\n","val Loss: 2.2300 Acc: 0.0000\n","\n","Epoch 24/100\n","----------\n","train Loss: 2.9933 Acc: 0.0435\n","val Loss: 2.2437 Acc: 0.0000\n","\n","Epoch 25/100\n","----------\n","train Loss: 2.9755 Acc: 0.0489\n","val Loss: 2.0896 Acc: 0.1667\n","\n","Epoch 26/100\n","----------\n","train Loss: 2.9820 Acc: 0.0326\n","val Loss: 2.1677 Acc: 0.1667\n","\n","Epoch 27/100\n","----------\n","train Loss: 2.9744 Acc: 0.0489\n","val Loss: 2.2429 Acc: 0.0000\n","\n","Epoch 28/100\n","----------\n","train Loss: 2.9866 Acc: 0.0652\n","val Loss: 2.1747 Acc: 0.0000\n","\n","Epoch 29/100\n","----------\n","train Loss: 2.9803 Acc: 0.0380\n","val Loss: 2.1135 Acc: 0.1667\n","\n","Epoch 30/100\n","----------\n","train Loss: 2.9813 Acc: 0.0380\n","val Loss: 2.1813 Acc: 0.1667\n","\n","Epoch 31/100\n","----------\n","train Loss: 2.9657 Acc: 0.0272\n","val Loss: 2.1774 Acc: 0.0833\n","\n","Epoch 32/100\n","----------\n","train Loss: 2.9933 Acc: 0.0272\n","val Loss: 2.1666 Acc: 0.0833\n","\n","Epoch 33/100\n","----------\n","train Loss: 2.9853 Acc: 0.0326\n","val Loss: 2.2343 Acc: 0.0833\n","\n","Epoch 34/100\n","----------\n","train Loss: 2.9789 Acc: 0.0272\n","val Loss: 2.1691 Acc: 0.0833\n","\n","Epoch 35/100\n","----------\n","train Loss: 2.9809 Acc: 0.0109\n","val Loss: 2.1625 Acc: 0.0833\n","\n","Epoch 36/100\n","----------\n","train Loss: 2.9663 Acc: 0.0543\n","val Loss: 2.1822 Acc: 0.0000\n","\n","Epoch 37/100\n","----------\n","train Loss: 2.9804 Acc: 0.0326\n","val Loss: 2.1203 Acc: 0.0833\n","\n","Epoch 38/100\n","----------\n","train Loss: 2.9889 Acc: 0.0543\n","val Loss: 2.0874 Acc: 0.0833\n","\n","Epoch 39/100\n","----------\n","train Loss: 2.9696 Acc: 0.0707\n","val Loss: 2.2209 Acc: 0.0000\n","\n","Epoch 40/100\n","----------\n","train Loss: 2.9655 Acc: 0.0326\n","val Loss: 2.1637 Acc: 0.0833\n","\n","Epoch 41/100\n","----------\n","train Loss: 2.9606 Acc: 0.0598\n","val Loss: 2.1839 Acc: 0.0833\n","\n","Epoch 42/100\n","----------\n","train Loss: 2.9770 Acc: 0.0326\n","val Loss: 2.1334 Acc: 0.1667\n","\n","Epoch 43/100\n","----------\n","train Loss: 2.9557 Acc: 0.0326\n","val Loss: 2.1690 Acc: 0.1667\n","\n","Epoch 44/100\n","----------\n","train Loss: 2.9602 Acc: 0.0435\n","val Loss: 2.1770 Acc: 0.1667\n","\n","Epoch 45/100\n","----------\n","train Loss: 2.9951 Acc: 0.0380\n","val Loss: 2.2645 Acc: 0.0000\n","\n","Epoch 46/100\n","----------\n","train Loss: 2.9842 Acc: 0.0326\n","val Loss: 2.2403 Acc: 0.0000\n","\n","Epoch 47/100\n","----------\n","train Loss: 2.9662 Acc: 0.0598\n","val Loss: 2.1723 Acc: 0.0833\n","\n","Epoch 48/100\n","----------\n","train Loss: 2.9677 Acc: 0.0489\n","val Loss: 2.1295 Acc: 0.0000\n","\n","Epoch 49/100\n","----------\n","train Loss: 2.9962 Acc: 0.0435\n","val Loss: 2.2208 Acc: 0.1667\n","\n","Epoch 50/100\n","----------\n","train Loss: 2.9565 Acc: 0.0326\n","val Loss: 2.1853 Acc: 0.0833\n","\n","Epoch 51/100\n","----------\n","train Loss: 2.9839 Acc: 0.0217\n","val Loss: 2.2421 Acc: 0.0833\n","\n","Epoch 52/100\n","----------\n","train Loss: 2.9712 Acc: 0.0652\n","val Loss: 2.1691 Acc: 0.1667\n","\n","Epoch 53/100\n","----------\n","train Loss: 2.9768 Acc: 0.0380\n","val Loss: 2.1504 Acc: 0.0833\n","\n","Epoch 54/100\n","----------\n","train Loss: 3.0028 Acc: 0.0380\n","val Loss: 2.1644 Acc: 0.0833\n","\n","Epoch 55/100\n","----------\n","train Loss: 2.9681 Acc: 0.0380\n","val Loss: 2.1984 Acc: 0.0833\n","\n","Epoch 56/100\n","----------\n","train Loss: 2.9401 Acc: 0.0598\n","val Loss: 2.2354 Acc: 0.0833\n","\n","Epoch 57/100\n","----------\n","train Loss: 2.9749 Acc: 0.0543\n","val Loss: 2.1038 Acc: 0.1667\n","\n","Epoch 58/100\n","----------\n","train Loss: 2.9769 Acc: 0.0435\n","val Loss: 2.1733 Acc: 0.0000\n","\n","Epoch 59/100\n","----------\n","train Loss: 2.9790 Acc: 0.0217\n","val Loss: 2.2006 Acc: 0.0833\n","\n","Epoch 60/100\n","----------\n","train Loss: 2.9950 Acc: 0.0217\n","val Loss: 2.1488 Acc: 0.0000\n","\n","Epoch 61/100\n","----------\n","train Loss: 2.9743 Acc: 0.0435\n","val Loss: 2.2024 Acc: 0.0833\n","\n","Epoch 62/100\n","----------\n","train Loss: 2.9826 Acc: 0.0489\n","val Loss: 2.1060 Acc: 0.0833\n","\n","Epoch 63/100\n","----------\n","train Loss: 2.9634 Acc: 0.0543\n","val Loss: 2.2293 Acc: 0.0833\n","\n","Epoch 64/100\n","----------\n","train Loss: 2.9763 Acc: 0.0489\n","val Loss: 2.1757 Acc: 0.0833\n","\n","Epoch 65/100\n","----------\n","train Loss: 2.9711 Acc: 0.0380\n","val Loss: 2.1424 Acc: 0.2500\n","\n","Epoch 66/100\n","----------\n","train Loss: 2.9755 Acc: 0.0489\n","val Loss: 2.1747 Acc: 0.0000\n","\n","Epoch 67/100\n","----------\n","train Loss: 2.9906 Acc: 0.0543\n","val Loss: 2.1558 Acc: 0.0833\n","\n","Epoch 68/100\n","----------\n","train Loss: 2.9631 Acc: 0.0435\n","val Loss: 2.1111 Acc: 0.0833\n","\n","Epoch 69/100\n","----------\n","train Loss: 2.9702 Acc: 0.0761\n","val Loss: 2.2003 Acc: 0.1667\n","\n","Epoch 70/100\n","----------\n","train Loss: 2.9618 Acc: 0.0272\n","val Loss: 2.2071 Acc: 0.0833\n","\n","Epoch 71/100\n","----------\n","train Loss: 2.9774 Acc: 0.0380\n","val Loss: 2.3161 Acc: 0.0000\n","\n","Epoch 72/100\n","----------\n","train Loss: 2.9815 Acc: 0.0326\n","val Loss: 2.1485 Acc: 0.0833\n","\n","Epoch 73/100\n","----------\n","train Loss: 2.9479 Acc: 0.0761\n","val Loss: 2.1823 Acc: 0.0833\n","\n","Epoch 74/100\n","----------\n","train Loss: 2.9964 Acc: 0.0489\n","val Loss: 2.2050 Acc: 0.0000\n","\n","Epoch 75/100\n","----------\n","train Loss: 2.9629 Acc: 0.0489\n","val Loss: 2.1663 Acc: 0.0000\n","\n","Epoch 76/100\n","----------\n","train Loss: 2.9600 Acc: 0.0489\n","val Loss: 2.1524 Acc: 0.1667\n","\n","Epoch 77/100\n","----------\n","train Loss: 2.9634 Acc: 0.0217\n","val Loss: 2.2072 Acc: 0.0000\n","\n","Epoch 78/100\n","----------\n","train Loss: 2.9845 Acc: 0.0326\n","val Loss: 2.2885 Acc: 0.0000\n","\n","Epoch 79/100\n","----------\n","train Loss: 2.9714 Acc: 0.0380\n","val Loss: 2.2176 Acc: 0.0000\n","\n","Epoch 80/100\n","----------\n","train Loss: 2.9706 Acc: 0.0435\n","val Loss: 2.2460 Acc: 0.0000\n","\n","Epoch 81/100\n","----------\n","train Loss: 2.9743 Acc: 0.0326\n","val Loss: 2.1681 Acc: 0.0000\n","\n","Epoch 82/100\n","----------\n","train Loss: 2.9670 Acc: 0.0543\n","val Loss: 2.1661 Acc: 0.0833\n","\n","Epoch 83/100\n","----------\n","train Loss: 2.9816 Acc: 0.0380\n","val Loss: 2.1583 Acc: 0.1667\n","\n","Epoch 84/100\n","----------\n","train Loss: 2.9880 Acc: 0.0598\n","val Loss: 2.2010 Acc: 0.0000\n","\n","Epoch 85/100\n","----------\n","train Loss: 2.9681 Acc: 0.0326\n","val Loss: 2.1839 Acc: 0.1667\n","\n","Epoch 86/100\n","----------\n","train Loss: 2.9720 Acc: 0.0272\n","val Loss: 2.1156 Acc: 0.1667\n","\n","Epoch 87/100\n","----------\n","train Loss: 2.9804 Acc: 0.0435\n","val Loss: 2.1679 Acc: 0.0000\n","\n","Epoch 88/100\n","----------\n","train Loss: 2.9772 Acc: 0.0543\n","val Loss: 2.2393 Acc: 0.0000\n","\n","Epoch 89/100\n","----------\n","train Loss: 2.9937 Acc: 0.0326\n","val Loss: 2.2553 Acc: 0.0000\n","\n","Epoch 90/100\n","----------\n","train Loss: 2.9673 Acc: 0.0543\n","val Loss: 2.2400 Acc: 0.0833\n","\n","Epoch 91/100\n","----------\n","train Loss: 2.9650 Acc: 0.0326\n","val Loss: 2.2375 Acc: 0.0833\n","\n","Epoch 92/100\n","----------\n","train Loss: 2.9664 Acc: 0.0272\n","val Loss: 2.3164 Acc: 0.0833\n","\n","Epoch 93/100\n","----------\n","train Loss: 2.9856 Acc: 0.0326\n","val Loss: 2.2123 Acc: 0.0833\n","\n","Epoch 94/100\n","----------\n","train Loss: 2.9584 Acc: 0.0815\n","val Loss: 2.1578 Acc: 0.1667\n","\n","Epoch 95/100\n","----------\n","train Loss: 2.9951 Acc: 0.0326\n","val Loss: 2.1578 Acc: 0.0000\n","\n","Epoch 96/100\n","----------\n","train Loss: 2.9665 Acc: 0.0380\n","val Loss: 2.2089 Acc: 0.0833\n","\n","Epoch 97/100\n","----------\n","train Loss: 2.9714 Acc: 0.0598\n","val Loss: 2.1782 Acc: 0.0833\n","\n","Epoch 98/100\n","----------\n","train Loss: 2.9672 Acc: 0.0543\n","val Loss: 2.1840 Acc: 0.1667\n","\n","Epoch 99/100\n","----------\n","train Loss: 2.9771 Acc: 0.0380\n","val Loss: 2.2861 Acc: 0.0000\n","\n","Epoch 100/100\n","----------\n","train Loss: 3.0040 Acc: 0.0380\n","val Loss: 2.1547 Acc: 0.0000\n","\n","Training complete in 20m 47s\n","Best val Acc: 0.250000\n","************************* 第 16 折 *************************\n","train: 181 val: 9\n","Epoch 1/100\n","----------\n","train Loss: 2.9613 Acc: 0.0543\n","val Loss: 2.3261 Acc: 0.0833\n","\n","Epoch 2/100\n","----------\n","train Loss: 2.9572 Acc: 0.0272\n","val Loss: 2.3515 Acc: 0.0000\n","\n","Epoch 3/100\n","----------\n","train Loss: 2.9657 Acc: 0.0380\n","val Loss: 2.4654 Acc: 0.0000\n","\n","Epoch 4/100\n","----------\n","train Loss: 2.9480 Acc: 0.0435\n","val Loss: 2.3255 Acc: 0.0000\n","\n","Epoch 5/100\n","----------\n","train Loss: 2.9598 Acc: 0.0598\n","val Loss: 2.3271 Acc: 0.0000\n","\n","Epoch 6/100\n","----------\n","train Loss: 2.9644 Acc: 0.0380\n","val Loss: 2.3140 Acc: 0.0833\n","\n","Epoch 7/100\n","----------\n","train Loss: 2.9679 Acc: 0.0272\n","val Loss: 2.3024 Acc: 0.0000\n","\n","Epoch 8/100\n","----------\n","train Loss: 2.9674 Acc: 0.0435\n","val Loss: 2.2568 Acc: 0.0000\n","\n","Epoch 9/100\n","----------\n","train Loss: 2.9524 Acc: 0.0489\n","val Loss: 2.2993 Acc: 0.0000\n","\n","Epoch 10/100\n","----------\n","train Loss: 2.9554 Acc: 0.0652\n","val Loss: 2.3055 Acc: 0.0000\n","\n","Epoch 11/100\n","----------\n","train Loss: 2.9556 Acc: 0.0652\n","val Loss: 2.3252 Acc: 0.0000\n","\n","Epoch 12/100\n","----------\n","train Loss: 2.9397 Acc: 0.0707\n","val Loss: 2.2310 Acc: 0.0000\n","\n","Epoch 13/100\n","----------\n","train Loss: 2.9572 Acc: 0.0435\n","val Loss: 2.2755 Acc: 0.0833\n","\n","Epoch 14/100\n","----------\n","train Loss: 2.9629 Acc: 0.0326\n","val Loss: 2.2418 Acc: 0.0833\n","\n","Epoch 15/100\n","----------\n","train Loss: 2.9727 Acc: 0.0489\n","val Loss: 2.4226 Acc: 0.0833\n","\n","Epoch 16/100\n","----------\n","train Loss: 2.9723 Acc: 0.0217\n","val Loss: 2.3533 Acc: 0.0833\n","\n","Epoch 17/100\n","----------\n","train Loss: 2.9688 Acc: 0.0707\n","val Loss: 2.2565 Acc: 0.0000\n","\n","Epoch 18/100\n","----------\n","train Loss: 2.9598 Acc: 0.0435\n","val Loss: 2.2577 Acc: 0.0000\n","\n","Epoch 19/100\n","----------\n","train Loss: 2.9595 Acc: 0.0380\n","val Loss: 2.3314 Acc: 0.0833\n","\n","Epoch 20/100\n","----------\n","train Loss: 2.9626 Acc: 0.0489\n","val Loss: 2.2185 Acc: 0.0833\n","\n","Epoch 21/100\n","----------\n","train Loss: 2.9651 Acc: 0.0380\n","val Loss: 2.3814 Acc: 0.0000\n","\n","Epoch 22/100\n","----------\n","train Loss: 2.9673 Acc: 0.0380\n","val Loss: 2.2295 Acc: 0.0833\n","\n","Epoch 23/100\n","----------\n","train Loss: 2.9804 Acc: 0.0598\n","val Loss: 2.2538 Acc: 0.0833\n","\n","Epoch 24/100\n","----------\n","train Loss: 2.9767 Acc: 0.0598\n","val Loss: 2.2359 Acc: 0.0833\n","\n","Epoch 25/100\n","----------\n","train Loss: 2.9665 Acc: 0.0217\n","val Loss: 2.3096 Acc: 0.0000\n","\n","Epoch 26/100\n","----------\n","train Loss: 2.9593 Acc: 0.0489\n","val Loss: 2.3065 Acc: 0.1667\n","\n","Epoch 27/100\n","----------\n","train Loss: 2.9635 Acc: 0.0489\n","val Loss: 2.3300 Acc: 0.0000\n","\n","Epoch 28/100\n","----------\n","train Loss: 2.9466 Acc: 0.0707\n","val Loss: 2.2913 Acc: 0.0000\n","\n","Epoch 29/100\n","----------\n","train Loss: 2.9844 Acc: 0.0380\n","val Loss: 2.2526 Acc: 0.0833\n","\n","Epoch 30/100\n","----------\n","train Loss: 2.9484 Acc: 0.0543\n","val Loss: 2.3633 Acc: 0.0000\n","\n","Epoch 31/100\n","----------\n","train Loss: 2.9664 Acc: 0.0707\n","val Loss: 2.3248 Acc: 0.0833\n","\n","Epoch 32/100\n","----------\n","train Loss: 2.9532 Acc: 0.0598\n","val Loss: 2.3080 Acc: 0.0000\n","\n","Epoch 33/100\n","----------\n","train Loss: 2.9667 Acc: 0.0380\n","val Loss: 2.3125 Acc: 0.0000\n","\n","Epoch 34/100\n","----------\n","train Loss: 2.9671 Acc: 0.0489\n","val Loss: 2.3287 Acc: 0.0000\n","\n","Epoch 35/100\n","----------\n","train Loss: 2.9552 Acc: 0.0598\n","val Loss: 2.2574 Acc: 0.0000\n","\n","Epoch 36/100\n","----------\n","train Loss: 2.9696 Acc: 0.0326\n","val Loss: 2.3534 Acc: 0.0000\n","\n","Epoch 37/100\n","----------\n","train Loss: 2.9589 Acc: 0.0380\n","val Loss: 2.3890 Acc: 0.0833\n","\n","Epoch 38/100\n","----------\n","train Loss: 2.9668 Acc: 0.0380\n","val Loss: 2.3659 Acc: 0.0833\n","\n","Epoch 39/100\n","----------\n","train Loss: 2.9722 Acc: 0.0489\n","val Loss: 2.3281 Acc: 0.0000\n","\n","Epoch 40/100\n","----------\n","train Loss: 2.9832 Acc: 0.0163\n","val Loss: 2.4074 Acc: 0.0000\n","\n","Epoch 41/100\n","----------\n","train Loss: 2.9791 Acc: 0.0380\n","val Loss: 2.3413 Acc: 0.0833\n","\n","Epoch 42/100\n","----------\n","train Loss: 2.9719 Acc: 0.0054\n","val Loss: 2.2790 Acc: 0.0833\n","\n","Epoch 43/100\n","----------\n","train Loss: 2.9840 Acc: 0.0598\n","val Loss: 2.3413 Acc: 0.0000\n","\n","Epoch 44/100\n","----------\n","train Loss: 2.9538 Acc: 0.0598\n","val Loss: 2.2428 Acc: 0.0833\n","\n","Epoch 45/100\n","----------\n","train Loss: 2.9636 Acc: 0.0598\n","val Loss: 2.2959 Acc: 0.0000\n","\n","Epoch 46/100\n","----------\n","train Loss: 2.9520 Acc: 0.0870\n","val Loss: 2.3373 Acc: 0.0000\n","\n","Epoch 47/100\n","----------\n","train Loss: 2.9623 Acc: 0.0543\n","val Loss: 2.3051 Acc: 0.0833\n","\n","Epoch 48/100\n","----------\n","train Loss: 2.9623 Acc: 0.0489\n","val Loss: 2.3679 Acc: 0.0000\n","\n","Epoch 49/100\n","----------\n","train Loss: 2.9561 Acc: 0.0326\n","val Loss: 2.2691 Acc: 0.0833\n","\n","Epoch 50/100\n","----------\n","train Loss: 2.9447 Acc: 0.0707\n","val Loss: 2.3160 Acc: 0.0000\n","\n","Epoch 51/100\n","----------\n","train Loss: 2.9578 Acc: 0.0707\n","val Loss: 2.3303 Acc: 0.0000\n","\n","Epoch 52/100\n","----------\n","train Loss: 2.9745 Acc: 0.0489\n","val Loss: 2.3310 Acc: 0.0000\n","\n","Epoch 53/100\n","----------\n","train Loss: 2.9697 Acc: 0.0380\n","val Loss: 2.3442 Acc: 0.0000\n","\n","Epoch 54/100\n","----------\n","train Loss: 2.9680 Acc: 0.0326\n","val Loss: 2.2691 Acc: 0.1667\n","\n","Epoch 55/100\n","----------\n","train Loss: 2.9590 Acc: 0.0326\n","val Loss: 2.2843 Acc: 0.0833\n","\n","Epoch 56/100\n","----------\n","train Loss: 2.9923 Acc: 0.0272\n","val Loss: 2.3290 Acc: 0.0000\n","\n","Epoch 57/100\n","----------\n","train Loss: 2.9692 Acc: 0.0326\n","val Loss: 2.3977 Acc: 0.0000\n","\n","Epoch 58/100\n","----------\n","train Loss: 2.9546 Acc: 0.0598\n","val Loss: 2.2717 Acc: 0.0833\n","\n","Epoch 59/100\n","----------\n","train Loss: 2.9657 Acc: 0.0489\n","val Loss: 2.2110 Acc: 0.0833\n","\n","Epoch 60/100\n","----------\n","train Loss: 2.9575 Acc: 0.0543\n","val Loss: 2.2577 Acc: 0.0000\n","\n","Epoch 61/100\n","----------\n","train Loss: 2.9845 Acc: 0.0435\n","val Loss: 2.2512 Acc: 0.0000\n","\n","Epoch 62/100\n","----------\n","train Loss: 2.9632 Acc: 0.0543\n","val Loss: 2.3321 Acc: 0.0000\n","\n","Epoch 63/100\n","----------\n","train Loss: 2.9751 Acc: 0.0435\n","val Loss: 2.2534 Acc: 0.0000\n","\n","Epoch 64/100\n","----------\n","train Loss: 2.9674 Acc: 0.0326\n","val Loss: 2.2974 Acc: 0.0833\n","\n","Epoch 65/100\n","----------\n","train Loss: 2.9556 Acc: 0.0598\n","val Loss: 2.2707 Acc: 0.0000\n","\n","Epoch 66/100\n","----------\n","train Loss: 2.9427 Acc: 0.0543\n","val Loss: 2.2436 Acc: 0.0833\n","\n","Epoch 67/100\n","----------\n","train Loss: 2.9621 Acc: 0.0435\n","val Loss: 2.3418 Acc: 0.0000\n","\n","Epoch 68/100\n","----------\n","train Loss: 2.9554 Acc: 0.0543\n","val Loss: 2.3322 Acc: 0.0000\n","\n","Epoch 69/100\n","----------\n","train Loss: 2.9640 Acc: 0.0326\n","val Loss: 2.3948 Acc: 0.0000\n","\n","Epoch 70/100\n","----------\n","train Loss: 2.9649 Acc: 0.0272\n","val Loss: 2.2890 Acc: 0.0000\n","\n","Epoch 71/100\n","----------\n","train Loss: 2.9414 Acc: 0.0761\n","val Loss: 2.3055 Acc: 0.0000\n","\n","Epoch 72/100\n","----------\n","train Loss: 2.9700 Acc: 0.0435\n","val Loss: 2.3400 Acc: 0.0000\n","\n","Epoch 73/100\n","----------\n","train Loss: 2.9599 Acc: 0.0707\n","val Loss: 2.2612 Acc: 0.0000\n","\n","Epoch 74/100\n","----------\n","train Loss: 2.9655 Acc: 0.0380\n","val Loss: 2.3893 Acc: 0.0000\n","\n","Epoch 75/100\n","----------\n","train Loss: 2.9639 Acc: 0.0326\n","val Loss: 2.3252 Acc: 0.0000\n","\n","Epoch 76/100\n","----------\n","train Loss: 2.9575 Acc: 0.0598\n","val Loss: 2.3586 Acc: 0.0000\n","\n","Epoch 77/100\n","----------\n","train Loss: 2.9670 Acc: 0.0217\n","val Loss: 2.2998 Acc: 0.0833\n","\n","Epoch 78/100\n","----------\n","train Loss: 2.9593 Acc: 0.0761\n","val Loss: 2.3532 Acc: 0.0833\n","\n","Epoch 79/100\n","----------\n","train Loss: 2.9777 Acc: 0.0380\n","val Loss: 2.2797 Acc: 0.0833\n","\n","Epoch 80/100\n","----------\n","train Loss: 2.9554 Acc: 0.0543\n","val Loss: 2.3444 Acc: 0.0000\n","\n","Epoch 81/100\n","----------\n","train Loss: 2.9846 Acc: 0.0435\n","val Loss: 2.3721 Acc: 0.0833\n","\n","Epoch 82/100\n","----------\n","train Loss: 2.9603 Acc: 0.0380\n","val Loss: 2.2464 Acc: 0.0000\n","\n","Epoch 83/100\n","----------\n","train Loss: 2.9502 Acc: 0.0435\n","val Loss: 2.3659 Acc: 0.0000\n","\n","Epoch 84/100\n","----------\n","train Loss: 2.9809 Acc: 0.0326\n","val Loss: 2.3599 Acc: 0.0000\n","\n","Epoch 85/100\n","----------\n","train Loss: 2.9688 Acc: 0.0326\n","val Loss: 2.2704 Acc: 0.0833\n","\n","Epoch 86/100\n","----------\n","train Loss: 2.9774 Acc: 0.0272\n","val Loss: 2.3760 Acc: 0.0000\n","\n","Epoch 87/100\n","----------\n","train Loss: 2.9686 Acc: 0.0598\n","val Loss: 2.3416 Acc: 0.0000\n","\n","Epoch 88/100\n","----------\n","train Loss: 2.9552 Acc: 0.0598\n","val Loss: 2.2400 Acc: 0.0833\n","\n","Epoch 89/100\n","----------\n","train Loss: 2.9603 Acc: 0.0326\n","val Loss: 2.2702 Acc: 0.0833\n","\n","Epoch 90/100\n","----------\n","train Loss: 2.9587 Acc: 0.0543\n","val Loss: 2.2981 Acc: 0.0833\n","\n","Epoch 91/100\n","----------\n","train Loss: 2.9585 Acc: 0.0652\n","val Loss: 2.2838 Acc: 0.0000\n","\n","Epoch 92/100\n","----------\n","train Loss: 2.9690 Acc: 0.0598\n","val Loss: 2.3105 Acc: 0.0000\n","\n","Epoch 93/100\n","----------\n","train Loss: 2.9608 Acc: 0.0489\n","val Loss: 2.2952 Acc: 0.0000\n","\n","Epoch 94/100\n","----------\n","train Loss: 2.9666 Acc: 0.0543\n","val Loss: 2.3590 Acc: 0.0000\n","\n","Epoch 95/100\n","----------\n","train Loss: 2.9493 Acc: 0.0707\n","val Loss: 2.2484 Acc: 0.0833\n","\n","Epoch 96/100\n","----------\n","train Loss: 2.9550 Acc: 0.0598\n","val Loss: 2.3577 Acc: 0.0000\n","\n","Epoch 97/100\n","----------\n","train Loss: 2.9433 Acc: 0.0489\n","val Loss: 2.2294 Acc: 0.0000\n","\n","Epoch 98/100\n","----------\n","train Loss: 2.9481 Acc: 0.0543\n","val Loss: 2.2639 Acc: 0.0000\n","\n","Epoch 99/100\n","----------\n","train Loss: 2.9536 Acc: 0.0326\n","val Loss: 2.3265 Acc: 0.0833\n","\n","Epoch 100/100\n","----------\n","train Loss: 2.9663 Acc: 0.0435\n","val Loss: 2.3766 Acc: 0.0000\n","\n","Training complete in 20m 47s\n","Best val Acc: 0.166667\n","************************* 第 17 折 *************************\n","train: 181 val: 9\n","Epoch 1/100\n","----------\n","train Loss: 2.9789 Acc: 0.0761\n","val Loss: 2.3992 Acc: 0.0000\n","\n","Epoch 2/100\n","----------\n","train Loss: 2.9731 Acc: 0.0272\n","val Loss: 2.3387 Acc: 0.0000\n","\n","Epoch 3/100\n","----------\n","train Loss: 2.9780 Acc: 0.0380\n","val Loss: 2.3557 Acc: 0.0833\n","\n","Epoch 4/100\n","----------\n","train Loss: 2.9851 Acc: 0.0435\n","val Loss: 2.2937 Acc: 0.0833\n","\n","Epoch 5/100\n","----------\n","train Loss: 2.9781 Acc: 0.0326\n","val Loss: 2.3502 Acc: 0.0000\n","\n","Epoch 6/100\n","----------\n","train Loss: 2.9566 Acc: 0.0761\n","val Loss: 2.2767 Acc: 0.0833\n","\n","Epoch 7/100\n","----------\n","train Loss: 2.9791 Acc: 0.0707\n","val Loss: 2.3139 Acc: 0.2500\n","\n","Epoch 8/100\n","----------\n","train Loss: 2.9514 Acc: 0.0435\n","val Loss: 2.3486 Acc: 0.0000\n","\n","Epoch 9/100\n","----------\n","train Loss: 2.9677 Acc: 0.0272\n","val Loss: 2.3359 Acc: 0.0000\n","\n","Epoch 10/100\n","----------\n","train Loss: 2.9687 Acc: 0.0380\n","val Loss: 2.3158 Acc: 0.0000\n","\n","Epoch 11/100\n","----------\n","train Loss: 2.9527 Acc: 0.0598\n","val Loss: 2.2571 Acc: 0.0000\n","\n","Epoch 12/100\n","----------\n","train Loss: 2.9747 Acc: 0.0652\n","val Loss: 2.2882 Acc: 0.0000\n","\n","Epoch 13/100\n","----------\n","train Loss: 2.9502 Acc: 0.0217\n","val Loss: 2.2845 Acc: 0.0000\n","\n","Epoch 14/100\n","----------\n","train Loss: 2.9711 Acc: 0.0489\n","val Loss: 2.3594 Acc: 0.0000\n","\n","Epoch 15/100\n","----------\n","train Loss: 2.9690 Acc: 0.0489\n","val Loss: 2.3513 Acc: 0.0833\n","\n","Epoch 16/100\n","----------\n","train Loss: 2.9786 Acc: 0.0380\n","val Loss: 2.3914 Acc: 0.0000\n","\n","Epoch 17/100\n","----------\n","train Loss: 2.9686 Acc: 0.0543\n","val Loss: 2.2840 Acc: 0.0833\n","\n","Epoch 18/100\n","----------\n","train Loss: 2.9524 Acc: 0.0380\n","val Loss: 2.3599 Acc: 0.0000\n","\n","Epoch 19/100\n","----------\n","train Loss: 2.9422 Acc: 0.0380\n","val Loss: 2.3562 Acc: 0.0000\n","\n","Epoch 20/100\n","----------\n","train Loss: 2.9606 Acc: 0.0272\n","val Loss: 2.3560 Acc: 0.0000\n","\n","Epoch 21/100\n","----------\n","train Loss: 2.9770 Acc: 0.0380\n","val Loss: 2.3797 Acc: 0.0000\n","\n","Epoch 22/100\n","----------\n","train Loss: 2.9661 Acc: 0.0380\n","val Loss: 2.3879 Acc: 0.0000\n","\n","Epoch 23/100\n","----------\n","train Loss: 2.9752 Acc: 0.0435\n","val Loss: 2.2718 Acc: 0.0833\n","\n","Epoch 24/100\n","----------\n","train Loss: 2.9598 Acc: 0.0598\n","val Loss: 2.2908 Acc: 0.0833\n","\n","Epoch 25/100\n","----------\n","train Loss: 2.9771 Acc: 0.0326\n","val Loss: 2.3434 Acc: 0.0000\n","\n","Epoch 26/100\n","----------\n","train Loss: 2.9615 Acc: 0.0217\n","val Loss: 2.2744 Acc: 0.0833\n","\n","Epoch 27/100\n","----------\n","train Loss: 2.9731 Acc: 0.0380\n","val Loss: 2.3380 Acc: 0.0000\n","\n","Epoch 28/100\n","----------\n","train Loss: 2.9450 Acc: 0.0543\n","val Loss: 2.3437 Acc: 0.0833\n","\n","Epoch 29/100\n","----------\n","train Loss: 2.9545 Acc: 0.0435\n","val Loss: 2.3308 Acc: 0.0833\n","\n","Epoch 30/100\n","----------\n","train Loss: 2.9343 Acc: 0.0489\n","val Loss: 2.3396 Acc: 0.0000\n","\n","Epoch 31/100\n","----------\n","train Loss: 2.9710 Acc: 0.0380\n","val Loss: 2.3257 Acc: 0.0000\n","\n","Epoch 32/100\n","----------\n","train Loss: 2.9697 Acc: 0.0489\n","val Loss: 2.2788 Acc: 0.0000\n","\n","Epoch 33/100\n","----------\n","train Loss: 2.9654 Acc: 0.0543\n","val Loss: 2.3192 Acc: 0.0833\n","\n","Epoch 34/100\n","----------\n","train Loss: 2.9752 Acc: 0.0272\n","val Loss: 2.3523 Acc: 0.0000\n","\n","Epoch 35/100\n","----------\n","train Loss: 2.9527 Acc: 0.0543\n","val Loss: 2.2592 Acc: 0.0000\n","\n","Epoch 36/100\n","----------\n","train Loss: 2.9574 Acc: 0.0543\n","val Loss: 2.2539 Acc: 0.0833\n","\n","Epoch 37/100\n","----------\n","train Loss: 2.9655 Acc: 0.0489\n","val Loss: 2.3466 Acc: 0.0833\n","\n","Epoch 38/100\n","----------\n","train Loss: 2.9504 Acc: 0.0380\n","val Loss: 2.3534 Acc: 0.0833\n","\n","Epoch 39/100\n","----------\n","train Loss: 2.9429 Acc: 0.0272\n","val Loss: 2.3556 Acc: 0.0000\n","\n","Epoch 40/100\n","----------\n","train Loss: 2.9618 Acc: 0.0435\n","val Loss: 2.2271 Acc: 0.0000\n","\n","Epoch 41/100\n","----------\n","train Loss: 2.9676 Acc: 0.0435\n","val Loss: 2.3483 Acc: 0.0000\n","\n","Epoch 42/100\n","----------\n","train Loss: 2.9812 Acc: 0.0543\n","val Loss: 2.3243 Acc: 0.0833\n","\n","Epoch 43/100\n","----------\n","train Loss: 2.9673 Acc: 0.0272\n","val Loss: 2.3913 Acc: 0.0833\n","\n","Epoch 44/100\n","----------\n","train Loss: 2.9650 Acc: 0.0435\n","val Loss: 2.4034 Acc: 0.0000\n","\n","Epoch 45/100\n","----------\n","train Loss: 2.9457 Acc: 0.0707\n","val Loss: 2.3135 Acc: 0.0833\n","\n","Epoch 46/100\n","----------\n","train Loss: 2.9702 Acc: 0.0761\n","val Loss: 2.3166 Acc: 0.0833\n","\n","Epoch 47/100\n","----------\n","train Loss: 2.9580 Acc: 0.0435\n","val Loss: 2.3124 Acc: 0.0000\n","\n","Epoch 48/100\n","----------\n","train Loss: 2.9599 Acc: 0.0598\n","val Loss: 2.2639 Acc: 0.0000\n","\n","Epoch 49/100\n","----------\n","train Loss: 2.9645 Acc: 0.0598\n","val Loss: 2.3237 Acc: 0.0000\n","\n","Epoch 50/100\n","----------\n","train Loss: 2.9553 Acc: 0.0272\n","val Loss: 2.3295 Acc: 0.0000\n","\n","Epoch 51/100\n","----------\n","train Loss: 2.9270 Acc: 0.0543\n","val Loss: 2.3935 Acc: 0.0000\n","\n","Epoch 52/100\n","----------\n","train Loss: 2.9589 Acc: 0.0543\n","val Loss: 2.3560 Acc: 0.0000\n","\n","Epoch 53/100\n","----------\n","train Loss: 2.9575 Acc: 0.0598\n","val Loss: 2.4022 Acc: 0.0833\n","\n","Epoch 54/100\n","----------\n","train Loss: 2.9592 Acc: 0.0435\n","val Loss: 2.3021 Acc: 0.0000\n","\n","Epoch 55/100\n","----------\n","train Loss: 2.9495 Acc: 0.0489\n","val Loss: 2.3116 Acc: 0.0000\n","\n","Epoch 56/100\n","----------\n","train Loss: 2.9685 Acc: 0.0489\n","val Loss: 2.3840 Acc: 0.0000\n","\n","Epoch 57/100\n","----------\n","train Loss: 2.9604 Acc: 0.0435\n","val Loss: 2.4010 Acc: 0.0000\n","\n","Epoch 58/100\n","----------\n","train Loss: 2.9616 Acc: 0.0380\n","val Loss: 2.3220 Acc: 0.0833\n","\n","Epoch 59/100\n","----------\n","train Loss: 2.9753 Acc: 0.0435\n","val Loss: 2.4795 Acc: 0.0000\n","\n","Epoch 60/100\n","----------\n","train Loss: 2.9641 Acc: 0.0163\n","val Loss: 2.3646 Acc: 0.0000\n","\n","Epoch 61/100\n","----------\n","train Loss: 2.9684 Acc: 0.0435\n","val Loss: 2.2948 Acc: 0.0000\n","\n","Epoch 62/100\n","----------\n","train Loss: 2.9840 Acc: 0.0489\n","val Loss: 2.3421 Acc: 0.0000\n","\n","Epoch 63/100\n","----------\n","train Loss: 2.9802 Acc: 0.0489\n","val Loss: 2.4078 Acc: 0.0000\n","\n","Epoch 64/100\n","----------\n","train Loss: 2.9768 Acc: 0.0489\n","val Loss: 2.3934 Acc: 0.0000\n","\n","Epoch 65/100\n","----------\n","train Loss: 2.9616 Acc: 0.0326\n","val Loss: 2.3316 Acc: 0.0833\n","\n","Epoch 66/100\n","----------\n","train Loss: 2.9577 Acc: 0.0652\n","val Loss: 2.3718 Acc: 0.0000\n","\n","Epoch 67/100\n","----------\n","train Loss: 2.9650 Acc: 0.0380\n","val Loss: 2.3475 Acc: 0.0000\n","\n","Epoch 68/100\n","----------\n","train Loss: 2.9656 Acc: 0.0652\n","val Loss: 2.4081 Acc: 0.0000\n","\n","Epoch 69/100\n","----------\n","train Loss: 2.9612 Acc: 0.0543\n","val Loss: 2.2915 Acc: 0.1667\n","\n","Epoch 70/100\n","----------\n","train Loss: 2.9663 Acc: 0.0326\n","val Loss: 2.4943 Acc: 0.0000\n","\n","Epoch 71/100\n","----------\n","train Loss: 2.9723 Acc: 0.0380\n","val Loss: 2.2787 Acc: 0.0000\n","\n","Epoch 72/100\n","----------\n","train Loss: 2.9434 Acc: 0.0652\n","val Loss: 2.3846 Acc: 0.0000\n","\n","Epoch 73/100\n","----------\n","train Loss: 2.9598 Acc: 0.0761\n","val Loss: 2.3854 Acc: 0.0000\n","\n","Epoch 74/100\n","----------\n","train Loss: 2.9545 Acc: 0.0380\n","val Loss: 2.3856 Acc: 0.0833\n","\n","Epoch 75/100\n","----------\n","train Loss: 2.9740 Acc: 0.0435\n","val Loss: 2.4066 Acc: 0.0000\n","\n","Epoch 76/100\n","----------\n","train Loss: 2.9712 Acc: 0.0326\n","val Loss: 2.3215 Acc: 0.0000\n","\n","Epoch 77/100\n","----------\n","train Loss: 2.9706 Acc: 0.0272\n","val Loss: 2.2747 Acc: 0.0833\n","\n","Epoch 78/100\n","----------\n","train Loss: 2.9429 Acc: 0.0543\n","val Loss: 2.3115 Acc: 0.0000\n","\n","Epoch 79/100\n","----------\n","train Loss: 2.9558 Acc: 0.0598\n","val Loss: 2.3380 Acc: 0.0000\n","\n","Epoch 80/100\n","----------\n","train Loss: 2.9852 Acc: 0.0435\n","val Loss: 2.3243 Acc: 0.0000\n","\n","Epoch 81/100\n","----------\n","train Loss: 2.9473 Acc: 0.0380\n","val Loss: 2.3359 Acc: 0.0000\n","\n","Epoch 82/100\n","----------\n","train Loss: 2.9569 Acc: 0.0543\n","val Loss: 2.3366 Acc: 0.0000\n","\n","Epoch 83/100\n","----------\n","train Loss: 2.9621 Acc: 0.0380\n","val Loss: 2.3655 Acc: 0.0000\n","\n","Epoch 84/100\n","----------\n","train Loss: 2.9624 Acc: 0.0326\n","val Loss: 2.3710 Acc: 0.0000\n","\n","Epoch 85/100\n","----------\n","train Loss: 2.9716 Acc: 0.0326\n","val Loss: 2.3156 Acc: 0.0000\n","\n","Epoch 86/100\n","----------\n","train Loss: 2.9721 Acc: 0.0543\n","val Loss: 2.3781 Acc: 0.0000\n","\n","Epoch 87/100\n","----------\n","train Loss: 2.9763 Acc: 0.0380\n","val Loss: 2.2685 Acc: 0.0000\n","\n","Epoch 88/100\n","----------\n","train Loss: 2.9612 Acc: 0.0489\n","val Loss: 2.3171 Acc: 0.0000\n","\n","Epoch 89/100\n","----------\n","train Loss: 2.9750 Acc: 0.0217\n","val Loss: 2.3540 Acc: 0.0000\n","\n","Epoch 90/100\n","----------\n","train Loss: 2.9655 Acc: 0.0598\n","val Loss: 2.3100 Acc: 0.0000\n","\n","Epoch 91/100\n","----------\n","train Loss: 2.9630 Acc: 0.0707\n","val Loss: 2.3324 Acc: 0.0000\n","\n","Epoch 92/100\n","----------\n","train Loss: 2.9748 Acc: 0.0435\n","val Loss: 2.3049 Acc: 0.0000\n","\n","Epoch 93/100\n","----------\n","train Loss: 2.9598 Acc: 0.0543\n","val Loss: 2.3764 Acc: 0.0000\n","\n","Epoch 94/100\n","----------\n","train Loss: 2.9441 Acc: 0.0489\n","val Loss: 2.3026 Acc: 0.0833\n","\n","Epoch 95/100\n","----------\n","train Loss: 2.9741 Acc: 0.0435\n","val Loss: 2.3381 Acc: 0.0000\n","\n","Epoch 96/100\n","----------\n","train Loss: 2.9478 Acc: 0.0543\n","val Loss: 2.2819 Acc: 0.0833\n","\n","Epoch 97/100\n","----------\n","train Loss: 2.9504 Acc: 0.0326\n","val Loss: 2.2242 Acc: 0.0833\n","\n","Epoch 98/100\n","----------\n","train Loss: 2.9608 Acc: 0.0652\n","val Loss: 2.3385 Acc: 0.0000\n","\n","Epoch 99/100\n","----------\n","train Loss: 2.9598 Acc: 0.0163\n","val Loss: 2.3653 Acc: 0.0000\n","\n","Epoch 100/100\n","----------\n","train Loss: 2.9884 Acc: 0.0163\n","val Loss: 2.3487 Acc: 0.0000\n","\n","Training complete in 20m 48s\n","Best val Acc: 0.250000\n","************************* 第 18 折 *************************\n","train: 181 val: 9\n","Epoch 1/100\n","----------\n","train Loss: 2.9721 Acc: 0.0435\n","val Loss: 2.2468 Acc: 0.0833\n","\n","Epoch 2/100\n","----------\n","train Loss: 2.9576 Acc: 0.0380\n","val Loss: 2.3162 Acc: 0.0833\n","\n","Epoch 3/100\n","----------\n","train Loss: 2.9516 Acc: 0.0707\n","val Loss: 2.2399 Acc: 0.0000\n","\n","Epoch 4/100\n","----------\n","train Loss: 2.9745 Acc: 0.0598\n","val Loss: 2.2417 Acc: 0.0833\n","\n","Epoch 5/100\n","----------\n","train Loss: 2.9597 Acc: 0.0543\n","val Loss: 2.4190 Acc: 0.0000\n","\n","Epoch 6/100\n","----------\n","train Loss: 2.9535 Acc: 0.0543\n","val Loss: 2.2081 Acc: 0.0833\n","\n","Epoch 7/100\n","----------\n","train Loss: 2.9940 Acc: 0.0543\n","val Loss: 2.2512 Acc: 0.1667\n","\n","Epoch 8/100\n","----------\n","train Loss: 2.9558 Acc: 0.0652\n","val Loss: 2.3455 Acc: 0.0000\n","\n","Epoch 9/100\n","----------\n","train Loss: 2.9836 Acc: 0.0163\n","val Loss: 2.2134 Acc: 0.0000\n","\n","Epoch 10/100\n","----------\n","train Loss: 2.9755 Acc: 0.0435\n","val Loss: 2.2545 Acc: 0.0833\n","\n","Epoch 11/100\n","----------\n","train Loss: 2.9558 Acc: 0.0380\n","val Loss: 2.2216 Acc: 0.1667\n","\n","Epoch 12/100\n","----------\n","train Loss: 2.9769 Acc: 0.0217\n","val Loss: 2.1759 Acc: 0.0833\n","\n","Epoch 13/100\n","----------\n","train Loss: 2.9501 Acc: 0.0489\n","val Loss: 2.3678 Acc: 0.0000\n","\n","Epoch 14/100\n","----------\n","train Loss: 2.9713 Acc: 0.0652\n","val Loss: 2.2819 Acc: 0.0833\n","\n","Epoch 15/100\n","----------\n","train Loss: 2.9446 Acc: 0.0435\n","val Loss: 2.1115 Acc: 0.2500\n","\n","Epoch 16/100\n","----------\n","train Loss: 2.9676 Acc: 0.0326\n","val Loss: 2.3585 Acc: 0.0000\n","\n","Epoch 17/100\n","----------\n","train Loss: 2.9736 Acc: 0.0326\n","val Loss: 2.2571 Acc: 0.0833\n","\n","Epoch 18/100\n","----------\n","train Loss: 2.9780 Acc: 0.0435\n","val Loss: 2.3760 Acc: 0.0000\n","\n","Epoch 19/100\n","----------\n","train Loss: 2.9569 Acc: 0.0707\n","val Loss: 2.2545 Acc: 0.0000\n","\n","Epoch 20/100\n","----------\n","train Loss: 2.9370 Acc: 0.0435\n","val Loss: 2.3092 Acc: 0.0833\n","\n","Epoch 21/100\n","----------\n","train Loss: 2.9392 Acc: 0.0380\n","val Loss: 2.3681 Acc: 0.0000\n","\n","Epoch 22/100\n","----------\n","train Loss: 2.9644 Acc: 0.0435\n","val Loss: 2.2845 Acc: 0.0000\n","\n","Epoch 23/100\n","----------\n","train Loss: 2.9598 Acc: 0.0435\n","val Loss: 2.3176 Acc: 0.0000\n","\n","Epoch 24/100\n","----------\n","train Loss: 2.9721 Acc: 0.0272\n","val Loss: 2.3628 Acc: 0.0000\n","\n","Epoch 25/100\n","----------\n","train Loss: 2.9662 Acc: 0.0272\n","val Loss: 2.2702 Acc: 0.0833\n","\n","Epoch 26/100\n","----------\n","train Loss: 2.9723 Acc: 0.0489\n","val Loss: 2.2609 Acc: 0.0000\n","\n","Epoch 27/100\n","----------\n","train Loss: 2.9552 Acc: 0.0489\n","val Loss: 2.2719 Acc: 0.0000\n","\n","Epoch 28/100\n","----------\n","train Loss: 2.9896 Acc: 0.0489\n","val Loss: 2.3550 Acc: 0.0000\n","\n","Epoch 29/100\n","----------\n","train Loss: 2.9639 Acc: 0.0598\n","val Loss: 2.2463 Acc: 0.1667\n","\n","Epoch 30/100\n","----------\n","train Loss: 2.9763 Acc: 0.0652\n","val Loss: 2.3712 Acc: 0.0000\n","\n","Epoch 31/100\n","----------\n","train Loss: 2.9515 Acc: 0.0543\n","val Loss: 2.3098 Acc: 0.0000\n","\n","Epoch 32/100\n","----------\n","train Loss: 2.9685 Acc: 0.0435\n","val Loss: 2.2701 Acc: 0.0833\n","\n","Epoch 33/100\n","----------\n","train Loss: 2.9579 Acc: 0.0543\n","val Loss: 2.3047 Acc: 0.0000\n","\n","Epoch 34/100\n","----------\n","train Loss: 2.9536 Acc: 0.0652\n","val Loss: 2.2751 Acc: 0.0833\n","\n","Epoch 35/100\n","----------\n","train Loss: 2.9656 Acc: 0.0543\n","val Loss: 2.2461 Acc: 0.0000\n","\n","Epoch 36/100\n","----------\n","train Loss: 2.9743 Acc: 0.0380\n","val Loss: 2.2946 Acc: 0.0000\n","\n","Epoch 37/100\n","----------\n","train Loss: 2.9522 Acc: 0.0435\n","val Loss: 2.3068 Acc: 0.0000\n","\n","Epoch 38/100\n","----------\n","train Loss: 2.9736 Acc: 0.0326\n","val Loss: 2.1749 Acc: 0.0833\n","\n","Epoch 39/100\n","----------\n","train Loss: 2.9665 Acc: 0.0380\n","val Loss: 2.2394 Acc: 0.0000\n","\n","Epoch 40/100\n","----------\n","train Loss: 2.9587 Acc: 0.0435\n","val Loss: 2.3128 Acc: 0.0000\n","\n","Epoch 41/100\n","----------\n","train Loss: 2.9657 Acc: 0.0435\n","val Loss: 2.2784 Acc: 0.0000\n","\n","Epoch 42/100\n","----------\n","train Loss: 2.9660 Acc: 0.0435\n","val Loss: 2.2563 Acc: 0.0000\n","\n","Epoch 43/100\n","----------\n","train Loss: 2.9761 Acc: 0.0598\n","val Loss: 2.3140 Acc: 0.0833\n","\n","Epoch 44/100\n","----------\n","train Loss: 2.9583 Acc: 0.0543\n","val Loss: 2.2666 Acc: 0.0833\n","\n","Epoch 45/100\n","----------\n","train Loss: 2.9792 Acc: 0.0435\n","val Loss: 2.2755 Acc: 0.0000\n","\n","Epoch 46/100\n","----------\n","train Loss: 2.9609 Acc: 0.0652\n","val Loss: 2.2578 Acc: 0.0833\n","\n","Epoch 47/100\n","----------\n","train Loss: 2.9814 Acc: 0.0543\n","val Loss: 2.1964 Acc: 0.0000\n","\n","Epoch 48/100\n","----------\n","train Loss: 2.9497 Acc: 0.0489\n","val Loss: 2.3252 Acc: 0.0000\n","\n","Epoch 49/100\n","----------\n","train Loss: 2.9507 Acc: 0.0543\n","val Loss: 2.2403 Acc: 0.0833\n","\n","Epoch 50/100\n","----------\n","train Loss: 2.9784 Acc: 0.0326\n","val Loss: 2.1802 Acc: 0.0833\n","\n","Epoch 51/100\n","----------\n","train Loss: 2.9797 Acc: 0.0543\n","val Loss: 2.3509 Acc: 0.0000\n","\n","Epoch 52/100\n","----------\n","train Loss: 2.9570 Acc: 0.0217\n","val Loss: 2.3678 Acc: 0.0000\n","\n","Epoch 53/100\n","----------\n","train Loss: 2.9769 Acc: 0.0435\n","val Loss: 2.2125 Acc: 0.0000\n","\n","Epoch 54/100\n","----------\n","train Loss: 2.9627 Acc: 0.0435\n","val Loss: 2.3950 Acc: 0.0833\n","\n","Epoch 55/100\n","----------\n","train Loss: 2.9660 Acc: 0.0380\n","val Loss: 2.3328 Acc: 0.0000\n","\n","Epoch 56/100\n","----------\n","train Loss: 2.9856 Acc: 0.0435\n","val Loss: 2.3108 Acc: 0.1667\n","\n","Epoch 57/100\n","----------\n","train Loss: 2.9585 Acc: 0.0652\n","val Loss: 2.2504 Acc: 0.1667\n","\n","Epoch 58/100\n","----------\n","train Loss: 2.9838 Acc: 0.0543\n","val Loss: 2.3389 Acc: 0.0000\n","\n","Epoch 59/100\n","----------\n","train Loss: 2.9543 Acc: 0.0489\n","val Loss: 2.2501 Acc: 0.0833\n","\n","Epoch 60/100\n","----------\n","train Loss: 2.9594 Acc: 0.0272\n","val Loss: 2.2714 Acc: 0.0000\n","\n","Epoch 61/100\n","----------\n","train Loss: 2.9777 Acc: 0.0489\n","val Loss: 2.3222 Acc: 0.0000\n","\n","Epoch 62/100\n","----------\n","train Loss: 2.9831 Acc: 0.0761\n","val Loss: 2.3577 Acc: 0.0000\n","\n","Epoch 63/100\n","----------\n","train Loss: 2.9862 Acc: 0.0272\n","val Loss: 2.3460 Acc: 0.0000\n","\n","Epoch 64/100\n","----------\n","train Loss: 2.9624 Acc: 0.0489\n","val Loss: 2.3477 Acc: 0.0000\n","\n","Epoch 65/100\n","----------\n","train Loss: 2.9543 Acc: 0.0652\n","val Loss: 2.3262 Acc: 0.0000\n","\n","Epoch 66/100\n","----------\n","train Loss: 2.9653 Acc: 0.0380\n","val Loss: 2.2530 Acc: 0.0833\n","\n","Epoch 67/100\n","----------\n","train Loss: 2.9787 Acc: 0.0489\n","val Loss: 2.2517 Acc: 0.0833\n","\n","Epoch 68/100\n","----------\n","train Loss: 2.9723 Acc: 0.0543\n","val Loss: 2.3795 Acc: 0.0000\n","\n","Epoch 69/100\n","----------\n","train Loss: 2.9587 Acc: 0.0380\n","val Loss: 2.1981 Acc: 0.0000\n","\n","Epoch 70/100\n","----------\n","train Loss: 2.9638 Acc: 0.0217\n","val Loss: 2.2726 Acc: 0.0833\n","\n","Epoch 71/100\n","----------\n","train Loss: 2.9572 Acc: 0.0326\n","val Loss: 2.2881 Acc: 0.0833\n","\n","Epoch 72/100\n","----------\n","train Loss: 2.9649 Acc: 0.0543\n","val Loss: 2.2851 Acc: 0.0000\n","\n","Epoch 73/100\n","----------\n","train Loss: 2.9667 Acc: 0.0380\n","val Loss: 2.2867 Acc: 0.0000\n","\n","Epoch 74/100\n","----------\n","train Loss: 2.9615 Acc: 0.0489\n","val Loss: 2.2425 Acc: 0.0000\n","\n","Epoch 75/100\n","----------\n","train Loss: 2.9630 Acc: 0.0217\n","val Loss: 2.3478 Acc: 0.0833\n","\n","Epoch 76/100\n","----------\n","train Loss: 2.9551 Acc: 0.0652\n","val Loss: 2.2528 Acc: 0.0833\n","\n","Epoch 77/100\n","----------\n","train Loss: 2.9700 Acc: 0.0652\n","val Loss: 2.2132 Acc: 0.0833\n","\n","Epoch 78/100\n","----------\n","train Loss: 2.9761 Acc: 0.0598\n","val Loss: 2.2824 Acc: 0.0000\n","\n","Epoch 79/100\n","----------\n","train Loss: 2.9581 Acc: 0.0598\n","val Loss: 2.3633 Acc: 0.0000\n","\n","Epoch 80/100\n","----------\n","train Loss: 2.9855 Acc: 0.0489\n","val Loss: 2.2347 Acc: 0.0833\n","\n","Epoch 81/100\n","----------\n","train Loss: 2.9693 Acc: 0.0435\n","val Loss: 2.3363 Acc: 0.0000\n","\n","Epoch 82/100\n","----------\n","train Loss: 2.9742 Acc: 0.0598\n","val Loss: 2.2737 Acc: 0.0000\n","\n","Epoch 83/100\n","----------\n","train Loss: 2.9533 Acc: 0.0435\n","val Loss: 2.2329 Acc: 0.1667\n","\n","Epoch 84/100\n","----------\n","train Loss: 2.9696 Acc: 0.0435\n","val Loss: 2.2118 Acc: 0.0000\n","\n","Epoch 85/100\n","----------\n","train Loss: 2.9659 Acc: 0.0543\n","val Loss: 2.3644 Acc: 0.0000\n","\n","Epoch 86/100\n","----------\n","train Loss: 2.9631 Acc: 0.0380\n","val Loss: 2.1945 Acc: 0.0000\n","\n","Epoch 87/100\n","----------\n","train Loss: 2.9536 Acc: 0.0652\n","val Loss: 2.2788 Acc: 0.0000\n","\n","Epoch 88/100\n","----------\n","train Loss: 2.9862 Acc: 0.0326\n","val Loss: 2.2155 Acc: 0.0000\n","\n","Epoch 89/100\n","----------\n","train Loss: 2.9591 Acc: 0.0489\n","val Loss: 2.3245 Acc: 0.0000\n","\n","Epoch 90/100\n","----------\n","train Loss: 2.9759 Acc: 0.0489\n","val Loss: 2.2399 Acc: 0.0833\n","\n","Epoch 91/100\n","----------\n","train Loss: 2.9652 Acc: 0.0489\n","val Loss: 2.3539 Acc: 0.0000\n","\n","Epoch 92/100\n","----------\n","train Loss: 2.9635 Acc: 0.0489\n","val Loss: 2.2821 Acc: 0.0000\n","\n","Epoch 93/100\n","----------\n","train Loss: 2.9704 Acc: 0.0380\n","val Loss: 2.3299 Acc: 0.0000\n","\n","Epoch 94/100\n","----------\n","train Loss: 2.9792 Acc: 0.0217\n","val Loss: 2.2824 Acc: 0.0000\n","\n","Epoch 95/100\n","----------\n","train Loss: 2.9645 Acc: 0.0435\n","val Loss: 2.1838 Acc: 0.0833\n","\n","Epoch 96/100\n","----------\n","train Loss: 2.9529 Acc: 0.0761\n","val Loss: 2.3124 Acc: 0.0833\n","\n","Epoch 97/100\n","----------\n","train Loss: 2.9609 Acc: 0.0217\n","val Loss: 2.2521 Acc: 0.0000\n","\n","Epoch 98/100\n","----------\n","train Loss: 2.9786 Acc: 0.0489\n","val Loss: 2.2521 Acc: 0.0000\n","\n","Epoch 99/100\n","----------\n","train Loss: 2.9565 Acc: 0.0272\n","val Loss: 2.2083 Acc: 0.1667\n","\n","Epoch 100/100\n","----------\n","train Loss: 2.9630 Acc: 0.0489\n","val Loss: 2.2191 Acc: 0.0833\n","\n","Training complete in 20m 50s\n","Best val Acc: 0.250000\n","************************* 第 19 折 *************************\n","train: 181 val: 9\n","Epoch 1/100\n","----------\n","train Loss: 2.9548 Acc: 0.0380\n","val Loss: 2.2641 Acc: 0.0000\n","\n","Epoch 2/100\n","----------\n","train Loss: 2.9539 Acc: 0.0435\n","val Loss: 2.2766 Acc: 0.0000\n","\n","Epoch 3/100\n","----------\n","train Loss: 2.9563 Acc: 0.0761\n","val Loss: 2.1853 Acc: 0.1667\n","\n","Epoch 4/100\n","----------\n","train Loss: 2.9637 Acc: 0.0707\n","val Loss: 2.1306 Acc: 0.0833\n","\n","Epoch 5/100\n","----------\n","train Loss: 2.9916 Acc: 0.0543\n","val Loss: 2.2160 Acc: 0.0833\n","\n","Epoch 6/100\n","----------\n","train Loss: 2.9710 Acc: 0.0326\n","val Loss: 2.1589 Acc: 0.0000\n","\n","Epoch 7/100\n","----------\n","train Loss: 2.9707 Acc: 0.0652\n","val Loss: 2.2285 Acc: 0.0000\n","\n","Epoch 8/100\n","----------\n","train Loss: 2.9903 Acc: 0.0163\n","val Loss: 2.1895 Acc: 0.0833\n","\n","Epoch 9/100\n","----------\n","train Loss: 2.9548 Acc: 0.0598\n","val Loss: 2.0977 Acc: 0.0000\n","\n","Epoch 10/100\n","----------\n","train Loss: 2.9525 Acc: 0.0543\n","val Loss: 2.1621 Acc: 0.0833\n","\n","Epoch 11/100\n","----------\n","train Loss: 2.9579 Acc: 0.0707\n","val Loss: 2.2050 Acc: 0.0000\n","\n","Epoch 12/100\n","----------\n","train Loss: 2.9620 Acc: 0.0652\n","val Loss: 2.2123 Acc: 0.0833\n","\n","Epoch 13/100\n","----------\n","train Loss: 2.9629 Acc: 0.0707\n","val Loss: 2.1659 Acc: 0.0000\n","\n","Epoch 14/100\n","----------\n","train Loss: 2.9880 Acc: 0.0435\n","val Loss: 2.2458 Acc: 0.0000\n","\n","Epoch 15/100\n","----------\n","train Loss: 2.9524 Acc: 0.0598\n","val Loss: 2.1980 Acc: 0.0833\n","\n","Epoch 16/100\n","----------\n","train Loss: 2.9696 Acc: 0.0380\n","val Loss: 2.1907 Acc: 0.0000\n","\n","Epoch 17/100\n","----------\n","train Loss: 2.9508 Acc: 0.0652\n","val Loss: 2.2828 Acc: 0.0833\n","\n","Epoch 18/100\n","----------\n","train Loss: 2.9451 Acc: 0.0543\n","val Loss: 2.2864 Acc: 0.0000\n","\n","Epoch 19/100\n","----------\n","train Loss: 2.9707 Acc: 0.0380\n","val Loss: 2.1662 Acc: 0.0833\n","\n","Epoch 20/100\n","----------\n","train Loss: 2.9635 Acc: 0.0380\n","val Loss: 2.3442 Acc: 0.0000\n","\n","Epoch 21/100\n","----------\n","train Loss: 2.9785 Acc: 0.0326\n","val Loss: 2.1863 Acc: 0.0833\n","\n","Epoch 22/100\n","----------\n","train Loss: 2.9510 Acc: 0.0652\n","val Loss: 2.2973 Acc: 0.0000\n","\n","Epoch 23/100\n","----------\n","train Loss: 2.9736 Acc: 0.0815\n","val Loss: 2.2505 Acc: 0.0000\n","\n","Epoch 24/100\n","----------\n","train Loss: 2.9715 Acc: 0.0652\n","val Loss: 2.2615 Acc: 0.0000\n","\n","Epoch 25/100\n","----------\n","train Loss: 2.9864 Acc: 0.0380\n","val Loss: 2.3980 Acc: 0.0000\n","\n","Epoch 26/100\n","----------\n","train Loss: 2.9653 Acc: 0.0652\n","val Loss: 2.3046 Acc: 0.0000\n","\n","Epoch 27/100\n","----------\n","train Loss: 2.9746 Acc: 0.0435\n","val Loss: 2.3340 Acc: 0.0000\n","\n","Epoch 28/100\n","----------\n","train Loss: 2.9823 Acc: 0.0598\n","val Loss: 2.2184 Acc: 0.0833\n","\n","Epoch 29/100\n","----------\n","train Loss: 2.9681 Acc: 0.0707\n","val Loss: 2.1643 Acc: 0.0000\n","\n","Epoch 30/100\n","----------\n","train Loss: 2.9464 Acc: 0.0652\n","val Loss: 2.2785 Acc: 0.0000\n","\n","Epoch 31/100\n","----------\n","train Loss: 2.9658 Acc: 0.0380\n","val Loss: 2.2058 Acc: 0.0833\n","\n","Epoch 32/100\n","----------\n","train Loss: 2.9720 Acc: 0.0380\n","val Loss: 2.2542 Acc: 0.0000\n","\n","Epoch 33/100\n","----------\n","train Loss: 2.9757 Acc: 0.0272\n","val Loss: 2.1937 Acc: 0.0000\n","\n","Epoch 34/100\n","----------\n","train Loss: 2.9662 Acc: 0.0435\n","val Loss: 2.2042 Acc: 0.0833\n","\n","Epoch 35/100\n","----------\n","train Loss: 2.9598 Acc: 0.0435\n","val Loss: 2.2968 Acc: 0.0000\n","\n","Epoch 36/100\n","----------\n","train Loss: 2.9571 Acc: 0.0543\n","val Loss: 2.2592 Acc: 0.0000\n","\n","Epoch 37/100\n","----------\n","train Loss: 2.9692 Acc: 0.0217\n","val Loss: 2.3134 Acc: 0.0000\n","\n","Epoch 38/100\n","----------\n","train Loss: 2.9671 Acc: 0.0652\n","val Loss: 2.2719 Acc: 0.0000\n","\n","Epoch 39/100\n","----------\n","train Loss: 2.9736 Acc: 0.0489\n","val Loss: 2.2133 Acc: 0.0000\n","\n","Epoch 40/100\n","----------\n","train Loss: 2.9495 Acc: 0.0543\n","val Loss: 2.2525 Acc: 0.0000\n","\n","Epoch 41/100\n","----------\n","train Loss: 2.9784 Acc: 0.0380\n","val Loss: 2.2319 Acc: 0.1667\n","\n","Epoch 42/100\n","----------\n","train Loss: 2.9804 Acc: 0.0380\n","val Loss: 2.2701 Acc: 0.0000\n","\n","Epoch 43/100\n","----------\n","train Loss: 2.9487 Acc: 0.0652\n","val Loss: 2.1680 Acc: 0.0833\n","\n","Epoch 44/100\n","----------\n","train Loss: 2.9902 Acc: 0.0489\n","val Loss: 2.2015 Acc: 0.0000\n","\n","Epoch 45/100\n","----------\n","train Loss: 2.9557 Acc: 0.0543\n","val Loss: 2.2727 Acc: 0.0000\n","\n","Epoch 46/100\n","----------\n","train Loss: 2.9754 Acc: 0.0435\n","val Loss: 2.3040 Acc: 0.0000\n","\n","Epoch 47/100\n","----------\n","train Loss: 2.9736 Acc: 0.0380\n","val Loss: 2.1750 Acc: 0.0000\n","\n","Epoch 48/100\n","----------\n","train Loss: 2.9738 Acc: 0.0435\n","val Loss: 2.2126 Acc: 0.0833\n","\n","Epoch 49/100\n","----------\n","train Loss: 2.9740 Acc: 0.0707\n","val Loss: 2.2619 Acc: 0.0000\n","\n","Epoch 50/100\n","----------\n","train Loss: 2.9771 Acc: 0.0380\n","val Loss: 2.2472 Acc: 0.0833\n","\n","Epoch 51/100\n","----------\n","train Loss: 2.9811 Acc: 0.0272\n","val Loss: 2.2806 Acc: 0.0000\n","\n","Epoch 52/100\n","----------\n","train Loss: 2.9718 Acc: 0.0380\n","val Loss: 2.2953 Acc: 0.0000\n","\n","Epoch 53/100\n","----------\n","train Loss: 2.9907 Acc: 0.0435\n","val Loss: 2.1525 Acc: 0.0000\n","\n","Epoch 54/100\n","----------\n","train Loss: 2.9736 Acc: 0.0272\n","val Loss: 2.2437 Acc: 0.1667\n","\n","Epoch 55/100\n","----------\n","train Loss: 2.9837 Acc: 0.0380\n","val Loss: 2.2441 Acc: 0.0000\n","\n","Epoch 56/100\n","----------\n","train Loss: 2.9818 Acc: 0.0598\n","val Loss: 2.1280 Acc: 0.0000\n","\n","Epoch 57/100\n","----------\n","train Loss: 2.9705 Acc: 0.0598\n","val Loss: 2.1672 Acc: 0.1667\n","\n","Epoch 58/100\n","----------\n","train Loss: 2.9551 Acc: 0.0652\n","val Loss: 2.3092 Acc: 0.0833\n","\n","Epoch 59/100\n","----------\n","train Loss: 2.9753 Acc: 0.0217\n","val Loss: 2.1454 Acc: 0.0000\n","\n","Epoch 60/100\n","----------\n","train Loss: 2.9740 Acc: 0.0489\n","val Loss: 2.2204 Acc: 0.0833\n","\n","Epoch 61/100\n","----------\n","train Loss: 2.9734 Acc: 0.0598\n","val Loss: 2.2926 Acc: 0.0000\n","\n","Epoch 62/100\n","----------\n","train Loss: 2.9930 Acc: 0.0326\n","val Loss: 2.2670 Acc: 0.0000\n","\n","Epoch 63/100\n","----------\n","train Loss: 2.9715 Acc: 0.0489\n","val Loss: 2.2484 Acc: 0.0000\n","\n","Epoch 64/100\n","----------\n","train Loss: 2.9953 Acc: 0.0217\n","val Loss: 2.1611 Acc: 0.0833\n","\n","Epoch 65/100\n","----------\n","train Loss: 2.9663 Acc: 0.0489\n","val Loss: 2.2588 Acc: 0.0000\n","\n","Epoch 66/100\n","----------\n","train Loss: 3.0013 Acc: 0.0435\n","val Loss: 2.2469 Acc: 0.0000\n","\n","Epoch 67/100\n","----------\n","train Loss: 2.9540 Acc: 0.0652\n","val Loss: 2.2237 Acc: 0.0000\n","\n","Epoch 68/100\n","----------\n","train Loss: 2.9829 Acc: 0.0326\n","val Loss: 2.2637 Acc: 0.0000\n","\n","Epoch 69/100\n","----------\n","train Loss: 2.9572 Acc: 0.0543\n","val Loss: 2.1978 Acc: 0.0000\n","\n","Epoch 70/100\n","----------\n","train Loss: 2.9792 Acc: 0.0272\n","val Loss: 2.2194 Acc: 0.0833\n","\n","Epoch 71/100\n","----------\n","train Loss: 2.9662 Acc: 0.0435\n","val Loss: 2.2092 Acc: 0.0000\n","\n","Epoch 72/100\n","----------\n","train Loss: 2.9838 Acc: 0.0380\n","val Loss: 2.2247 Acc: 0.0000\n","\n","Epoch 73/100\n","----------\n","train Loss: 2.9782 Acc: 0.0435\n","val Loss: 2.2978 Acc: 0.0000\n","\n","Epoch 74/100\n","----------\n","train Loss: 2.9633 Acc: 0.0543\n","val Loss: 2.2349 Acc: 0.0000\n","\n","Epoch 75/100\n","----------\n","train Loss: 2.9593 Acc: 0.0761\n","val Loss: 2.1594 Acc: 0.0833\n","\n","Epoch 76/100\n","----------\n","train Loss: 3.0034 Acc: 0.0435\n","val Loss: 2.2819 Acc: 0.0000\n","\n","Epoch 77/100\n","----------\n","train Loss: 2.9872 Acc: 0.0163\n","val Loss: 2.2597 Acc: 0.0833\n","\n","Epoch 78/100\n","----------\n","train Loss: 2.9781 Acc: 0.0435\n","val Loss: 2.2028 Acc: 0.0000\n","\n","Epoch 79/100\n","----------\n","train Loss: 2.9634 Acc: 0.0543\n","val Loss: 2.3171 Acc: 0.0000\n","\n","Epoch 80/100\n","----------\n","train Loss: 2.9955 Acc: 0.0272\n","val Loss: 2.2168 Acc: 0.0000\n","\n","Epoch 81/100\n","----------\n","train Loss: 2.9578 Acc: 0.0326\n","val Loss: 2.1908 Acc: 0.0000\n","\n","Epoch 82/100\n","----------\n","train Loss: 2.9586 Acc: 0.0652\n","val Loss: 2.3383 Acc: 0.0000\n","\n","Epoch 83/100\n","----------\n","train Loss: 2.9749 Acc: 0.0326\n","val Loss: 2.1743 Acc: 0.0833\n","\n","Epoch 84/100\n","----------\n","train Loss: 2.9550 Acc: 0.0380\n","val Loss: 2.2717 Acc: 0.0833\n","\n","Epoch 85/100\n","----------\n","train Loss: 2.9789 Acc: 0.0435\n","val Loss: 2.2271 Acc: 0.0000\n","\n","Epoch 86/100\n","----------\n","train Loss: 2.9700 Acc: 0.0489\n","val Loss: 2.1956 Acc: 0.0833\n","\n","Epoch 87/100\n","----------\n","train Loss: 2.9777 Acc: 0.0380\n","val Loss: 2.2041 Acc: 0.0000\n","\n","Epoch 88/100\n","----------\n","train Loss: 2.9811 Acc: 0.0272\n","val Loss: 2.2466 Acc: 0.0000\n","\n","Epoch 89/100\n","----------\n","train Loss: 2.9546 Acc: 0.0543\n","val Loss: 2.2749 Acc: 0.0000\n","\n","Epoch 90/100\n","----------\n","train Loss: 2.9634 Acc: 0.0380\n","val Loss: 2.2373 Acc: 0.0833\n","\n","Epoch 91/100\n","----------\n","train Loss: 2.9737 Acc: 0.0435\n","val Loss: 2.1977 Acc: 0.0833\n","\n","Epoch 92/100\n","----------\n","train Loss: 2.9753 Acc: 0.0435\n","val Loss: 2.3058 Acc: 0.0000\n","\n","Epoch 93/100\n","----------\n","train Loss: 2.9550 Acc: 0.0435\n","val Loss: 2.3071 Acc: 0.0000\n","\n","Epoch 94/100\n","----------\n","train Loss: 2.9711 Acc: 0.0761\n","val Loss: 2.2665 Acc: 0.0000\n","\n","Epoch 95/100\n","----------\n","train Loss: 2.9619 Acc: 0.0380\n","val Loss: 2.2979 Acc: 0.0000\n","\n","Epoch 96/100\n","----------\n","train Loss: 2.9577 Acc: 0.0326\n","val Loss: 2.2113 Acc: 0.0000\n","\n","Epoch 97/100\n","----------\n","train Loss: 2.9759 Acc: 0.0652\n","val Loss: 2.1231 Acc: 0.2500\n","\n","Epoch 98/100\n","----------\n","train Loss: 2.9688 Acc: 0.0598\n","val Loss: 2.2640 Acc: 0.0000\n","\n","Epoch 99/100\n","----------\n","train Loss: 2.9380 Acc: 0.0598\n","val Loss: 2.2596 Acc: 0.0000\n","\n","Epoch 100/100\n","----------\n","train Loss: 2.9725 Acc: 0.0326\n","val Loss: 2.2510 Acc: 0.0000\n","\n","Training complete in 20m 47s\n","Best val Acc: 0.250000\n","************************* 第 20 折 *************************\n","train: 181 val: 9\n","Epoch 1/100\n","----------\n","train Loss: 2.9633 Acc: 0.0543\n","val Loss: 2.2768 Acc: 0.0833\n","\n","Epoch 2/100\n","----------\n","train Loss: 2.9740 Acc: 0.0435\n","val Loss: 2.2128 Acc: 0.0833\n","\n","Epoch 3/100\n","----------\n","train Loss: 2.9645 Acc: 0.0380\n","val Loss: 2.2616 Acc: 0.0000\n","\n","Epoch 4/100\n","----------\n","train Loss: 2.9640 Acc: 0.0489\n","val Loss: 2.2488 Acc: 0.0000\n","\n","Epoch 5/100\n","----------\n","train Loss: 2.9592 Acc: 0.0598\n","val Loss: 2.2823 Acc: 0.0000\n","\n","Epoch 6/100\n","----------\n","train Loss: 2.9554 Acc: 0.0489\n","val Loss: 2.3385 Acc: 0.0833\n","\n","Epoch 7/100\n","----------\n","train Loss: 2.9568 Acc: 0.0543\n","val Loss: 2.3563 Acc: 0.0000\n","\n","Epoch 8/100\n","----------\n","train Loss: 2.9683 Acc: 0.0435\n","val Loss: 2.2788 Acc: 0.0000\n","\n","Epoch 9/100\n","----------\n","train Loss: 2.9754 Acc: 0.0435\n","val Loss: 2.2984 Acc: 0.0000\n","\n","Epoch 10/100\n","----------\n","train Loss: 2.9713 Acc: 0.0652\n","val Loss: 2.4194 Acc: 0.0833\n","\n","Epoch 11/100\n","----------\n","train Loss: 2.9675 Acc: 0.0435\n","val Loss: 2.3319 Acc: 0.0000\n","\n","Epoch 12/100\n","----------\n","train Loss: 2.9804 Acc: 0.0489\n","val Loss: 2.3045 Acc: 0.0000\n","\n","Epoch 13/100\n","----------\n","train Loss: 2.9724 Acc: 0.0543\n","val Loss: 2.3688 Acc: 0.0000\n","\n","Epoch 14/100\n","----------\n","train Loss: 2.9560 Acc: 0.0380\n","val Loss: 2.3570 Acc: 0.0000\n","\n","Epoch 15/100\n","----------\n","train Loss: 2.9906 Acc: 0.0435\n","val Loss: 2.2870 Acc: 0.0000\n","\n","Epoch 16/100\n","----------\n","train Loss: 2.9510 Acc: 0.0652\n","val Loss: 2.2560 Acc: 0.0000\n","\n","Epoch 17/100\n","----------\n","train Loss: 2.9699 Acc: 0.0489\n","val Loss: 2.2799 Acc: 0.0833\n","\n","Epoch 18/100\n","----------\n","train Loss: 2.9855 Acc: 0.0163\n","val Loss: 2.2652 Acc: 0.0833\n","\n","Epoch 19/100\n","----------\n","train Loss: 2.9811 Acc: 0.0652\n","val Loss: 2.2553 Acc: 0.0000\n","\n","Epoch 20/100\n","----------\n","train Loss: 2.9643 Acc: 0.0652\n","val Loss: 2.2951 Acc: 0.0000\n","\n","Epoch 21/100\n","----------\n","train Loss: 2.9859 Acc: 0.0380\n","val Loss: 2.2791 Acc: 0.0000\n","\n","Epoch 22/100\n","----------\n","train Loss: 2.9564 Acc: 0.0543\n","val Loss: 2.1761 Acc: 0.0000\n","\n","Epoch 23/100\n","----------\n","train Loss: 2.9555 Acc: 0.0489\n","val Loss: 2.2372 Acc: 0.0000\n","\n","Epoch 24/100\n","----------\n","train Loss: 2.9448 Acc: 0.0326\n","val Loss: 2.3358 Acc: 0.0000\n","\n","Epoch 25/100\n","----------\n","train Loss: 2.9669 Acc: 0.0707\n","val Loss: 2.3529 Acc: 0.0000\n","\n","Epoch 26/100\n","----------\n","train Loss: 2.9523 Acc: 0.0598\n","val Loss: 2.3400 Acc: 0.0000\n","\n","Epoch 27/100\n","----------\n","train Loss: 2.9623 Acc: 0.0435\n","val Loss: 2.2765 Acc: 0.0000\n","\n","Epoch 28/100\n","----------\n","train Loss: 2.9556 Acc: 0.0489\n","val Loss: 2.3833 Acc: 0.0000\n","\n","Epoch 29/100\n","----------\n","train Loss: 2.9663 Acc: 0.0217\n","val Loss: 2.3826 Acc: 0.0000\n","\n","Epoch 30/100\n","----------\n","train Loss: 2.9711 Acc: 0.0489\n","val Loss: 2.3009 Acc: 0.0833\n","\n","Epoch 31/100\n","----------\n","train Loss: 2.9537 Acc: 0.0326\n","val Loss: 2.4029 Acc: 0.0000\n","\n","Epoch 32/100\n","----------\n","train Loss: 2.9445 Acc: 0.0707\n","val Loss: 2.2661 Acc: 0.0833\n","\n","Epoch 33/100\n","----------\n","train Loss: 2.9747 Acc: 0.0435\n","val Loss: 2.1616 Acc: 0.1667\n","\n","Epoch 34/100\n","----------\n","train Loss: 2.9781 Acc: 0.0435\n","val Loss: 2.2368 Acc: 0.0833\n","\n","Epoch 35/100\n","----------\n","train Loss: 2.9375 Acc: 0.0652\n","val Loss: 2.3077 Acc: 0.0000\n","\n","Epoch 36/100\n","----------\n","train Loss: 2.9513 Acc: 0.0652\n","val Loss: 2.3284 Acc: 0.0000\n","\n","Epoch 37/100\n","----------\n","train Loss: 2.9721 Acc: 0.0380\n","val Loss: 2.3975 Acc: 0.0000\n","\n","Epoch 38/100\n","----------\n","train Loss: 2.9826 Acc: 0.0380\n","val Loss: 2.3214 Acc: 0.0833\n","\n","Epoch 39/100\n","----------\n","train Loss: 2.9735 Acc: 0.0326\n","val Loss: 2.3876 Acc: 0.0000\n","\n","Epoch 40/100\n","----------\n","train Loss: 2.9612 Acc: 0.0435\n","val Loss: 2.3194 Acc: 0.0000\n","\n","Epoch 41/100\n","----------\n","train Loss: 2.9479 Acc: 0.0707\n","val Loss: 2.3397 Acc: 0.0000\n","\n","Epoch 42/100\n","----------\n","train Loss: 2.9855 Acc: 0.0489\n","val Loss: 2.2628 Acc: 0.0833\n","\n","Epoch 43/100\n","----------\n","train Loss: 2.9617 Acc: 0.0435\n","val Loss: 2.3105 Acc: 0.0833\n","\n","Epoch 44/100\n","----------\n","train Loss: 2.9678 Acc: 0.0435\n","val Loss: 2.3475 Acc: 0.0000\n","\n","Epoch 45/100\n","----------\n","train Loss: 2.9978 Acc: 0.0326\n","val Loss: 2.3121 Acc: 0.0833\n","\n","Epoch 46/100\n","----------\n","train Loss: 2.9631 Acc: 0.0652\n","val Loss: 2.2540 Acc: 0.1667\n","\n","Epoch 47/100\n","----------\n","train Loss: 2.9613 Acc: 0.0272\n","val Loss: 2.2788 Acc: 0.0833\n","\n","Epoch 48/100\n","----------\n","train Loss: 2.9713 Acc: 0.0272\n","val Loss: 2.2627 Acc: 0.0000\n","\n","Epoch 49/100\n","----------\n","train Loss: 2.9681 Acc: 0.0435\n","val Loss: 2.4135 Acc: 0.0000\n","\n","Epoch 50/100\n","----------\n","train Loss: 2.9493 Acc: 0.0543\n","val Loss: 2.2882 Acc: 0.0000\n","\n","Epoch 51/100\n","----------\n","train Loss: 2.9806 Acc: 0.0272\n","val Loss: 2.3411 Acc: 0.0000\n","\n","Epoch 52/100\n","----------\n","train Loss: 2.9612 Acc: 0.0272\n","val Loss: 2.3398 Acc: 0.0000\n","\n","Epoch 53/100\n","----------\n","train Loss: 2.9794 Acc: 0.0435\n","val Loss: 2.3660 Acc: 0.0000\n","\n","Epoch 54/100\n","----------\n","train Loss: 2.9578 Acc: 0.0761\n","val Loss: 2.2724 Acc: 0.0833\n","\n","Epoch 55/100\n","----------\n","train Loss: 2.9556 Acc: 0.0543\n","val Loss: 2.2508 Acc: 0.0000\n","\n","Epoch 56/100\n","----------\n","train Loss: 2.9648 Acc: 0.0380\n","val Loss: 2.2830 Acc: 0.0833\n","\n","Epoch 57/100\n","----------\n","train Loss: 2.9801 Acc: 0.0326\n","val Loss: 2.3244 Acc: 0.0000\n","\n","Epoch 58/100\n","----------\n","train Loss: 2.9574 Acc: 0.0543\n","val Loss: 2.3250 Acc: 0.0000\n","\n","Epoch 59/100\n","----------\n","train Loss: 2.9822 Acc: 0.0380\n","val Loss: 2.3394 Acc: 0.0833\n","\n","Epoch 60/100\n","----------\n","train Loss: 2.9928 Acc: 0.0272\n","val Loss: 2.3060 Acc: 0.0833\n","\n","Epoch 61/100\n","----------\n","train Loss: 2.9746 Acc: 0.0326\n","val Loss: 2.2304 Acc: 0.0000\n","\n","Epoch 62/100\n","----------\n","train Loss: 2.9595 Acc: 0.0543\n","val Loss: 2.3114 Acc: 0.0000\n","\n","Epoch 63/100\n","----------\n","train Loss: 2.9732 Acc: 0.0543\n","val Loss: 2.2681 Acc: 0.0000\n","\n","Epoch 64/100\n","----------\n","train Loss: 2.9688 Acc: 0.0707\n","val Loss: 2.2953 Acc: 0.0000\n","\n","Epoch 65/100\n","----------\n","train Loss: 2.9638 Acc: 0.0543\n","val Loss: 2.3149 Acc: 0.0000\n","\n","Epoch 66/100\n","----------\n","train Loss: 2.9749 Acc: 0.0272\n","val Loss: 2.2717 Acc: 0.0833\n","\n","Epoch 67/100\n","----------\n","train Loss: 2.9705 Acc: 0.0380\n","val Loss: 2.3136 Acc: 0.0000\n","\n","Epoch 68/100\n","----------\n","train Loss: 2.9669 Acc: 0.0598\n","val Loss: 2.3525 Acc: 0.0833\n","\n","Epoch 69/100\n","----------\n","train Loss: 2.9736 Acc: 0.0380\n","val Loss: 2.3059 Acc: 0.0000\n","\n","Epoch 70/100\n","----------\n","train Loss: 2.9509 Acc: 0.0380\n","val Loss: 2.3224 Acc: 0.0000\n","\n","Epoch 71/100\n","----------\n","train Loss: 2.9670 Acc: 0.0435\n","val Loss: 2.3885 Acc: 0.0000\n","\n","Epoch 72/100\n","----------\n","train Loss: 2.9883 Acc: 0.0109\n","val Loss: 2.3839 Acc: 0.0000\n","\n","Epoch 73/100\n","----------\n","train Loss: 2.9600 Acc: 0.0489\n","val Loss: 2.2624 Acc: 0.0000\n","\n","Epoch 74/100\n","----------\n","train Loss: 2.9622 Acc: 0.0489\n","val Loss: 2.2768 Acc: 0.0000\n","\n","Epoch 75/100\n","----------\n","train Loss: 2.9699 Acc: 0.0326\n","val Loss: 2.2774 Acc: 0.0000\n","\n","Epoch 76/100\n","----------\n","train Loss: 2.9763 Acc: 0.0489\n","val Loss: 2.3497 Acc: 0.0000\n","\n","Epoch 77/100\n","----------\n","train Loss: 2.9438 Acc: 0.0380\n","val Loss: 2.2421 Acc: 0.0000\n","\n","Epoch 78/100\n","----------\n","train Loss: 2.9784 Acc: 0.0598\n","val Loss: 2.2838 Acc: 0.0000\n","\n","Epoch 79/100\n","----------\n","train Loss: 2.9705 Acc: 0.0326\n","val Loss: 2.3115 Acc: 0.0000\n","\n","Epoch 80/100\n","----------\n","train Loss: 2.9812 Acc: 0.0326\n","val Loss: 2.3237 Acc: 0.0000\n","\n","Epoch 81/100\n","----------\n","train Loss: 2.9534 Acc: 0.0435\n","val Loss: 2.2675 Acc: 0.0833\n","\n","Epoch 82/100\n","----------\n","train Loss: 2.9821 Acc: 0.0489\n","val Loss: 2.3357 Acc: 0.0000\n","\n","Epoch 83/100\n","----------\n","train Loss: 2.9728 Acc: 0.0217\n","val Loss: 2.3066 Acc: 0.0000\n","\n","Epoch 84/100\n","----------\n","train Loss: 2.9879 Acc: 0.0326\n","val Loss: 2.2450 Acc: 0.0833\n","\n","Epoch 85/100\n","----------\n","train Loss: 2.9540 Acc: 0.0652\n","val Loss: 2.2688 Acc: 0.0000\n","\n","Epoch 86/100\n","----------\n","train Loss: 2.9717 Acc: 0.0272\n","val Loss: 2.3908 Acc: 0.0000\n","\n","Epoch 87/100\n","----------\n","train Loss: 2.9610 Acc: 0.0489\n","val Loss: 2.2722 Acc: 0.0000\n","\n","Epoch 88/100\n","----------\n","train Loss: 2.9408 Acc: 0.0652\n","val Loss: 2.2503 Acc: 0.0833\n","\n","Epoch 89/100\n","----------\n","train Loss: 2.9878 Acc: 0.0435\n","val Loss: 2.3507 Acc: 0.0833\n","\n","Epoch 90/100\n","----------\n","train Loss: 2.9419 Acc: 0.0707\n","val Loss: 2.2950 Acc: 0.0000\n","\n","Epoch 91/100\n","----------\n","train Loss: 2.9584 Acc: 0.0380\n","val Loss: 2.4055 Acc: 0.0000\n","\n","Epoch 92/100\n","----------\n","train Loss: 2.9534 Acc: 0.0380\n","val Loss: 2.2548 Acc: 0.0000\n","\n","Epoch 93/100\n","----------\n","train Loss: 2.9785 Acc: 0.0380\n","val Loss: 2.3115 Acc: 0.0833\n","\n","Epoch 94/100\n","----------\n","train Loss: 2.9588 Acc: 0.0815\n","val Loss: 2.2968 Acc: 0.0000\n","\n","Epoch 95/100\n","----------\n","train Loss: 2.9861 Acc: 0.0543\n","val Loss: 2.2684 Acc: 0.0833\n","\n","Epoch 96/100\n","----------\n","train Loss: 2.9548 Acc: 0.0435\n","val Loss: 2.3082 Acc: 0.0000\n","\n","Epoch 97/100\n","----------\n","train Loss: 2.9834 Acc: 0.0217\n","val Loss: 2.2860 Acc: 0.0000\n","\n","Epoch 98/100\n","----------\n","train Loss: 2.9610 Acc: 0.0435\n","val Loss: 2.3605 Acc: 0.0000\n","\n","Epoch 99/100\n","----------\n","train Loss: 2.9666 Acc: 0.0598\n","val Loss: 2.2922 Acc: 0.0000\n","\n","Epoch 100/100\n","----------\n","train Loss: 2.9891 Acc: 0.0272\n","val Loss: 2.3154 Acc: 0.0000\n","\n","Training complete in 20m 49s\n","Best val Acc: 0.166667"]}]}